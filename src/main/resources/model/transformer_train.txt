2022-05-28 11:44:13 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-05-28 11:44:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/5_25_tokenized/transformer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/5_25.tokenized.gu-bai', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0007], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/5_25_tokenized/transformer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/5_25.tokenized.gu-bai', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0007]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-05-28 11:44:15 | INFO | fairseq.tasks.translation | [gu] dictionary: 50000 types
2022-05-28 11:44:15 | INFO | fairseq.tasks.translation | [bai] dictionary: 50000 types
2022-05-28 11:44:17 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=50000, bias=False)
  )
)
2022-05-28 11:44:17 | INFO | fairseq_cli.train | task: TranslationTask
2022-05-28 11:44:17 | INFO | fairseq_cli.train | model: TransformerModel
2022-05-28 11:44:17 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-05-28 11:44:17 | INFO | fairseq_cli.train | num. shared model params: 120,938,496 (num. trained: 120,938,496)
2022-05-28 11:44:17 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-05-28 11:44:17 | INFO | fairseq.data.data_utils | loaded 96,496 examples from: data-bin/5_25.tokenized.gu-bai/valid.gu-bai.gu
2022-05-28 11:44:17 | INFO | fairseq.data.data_utils | loaded 96,496 examples from: data-bin/5_25.tokenized.gu-bai/valid.gu-bai.bai
2022-05-28 11:44:17 | INFO | fairseq.tasks.translation | data-bin/5_25.tokenized.gu-bai valid gu-bai 96496 examples
2022-05-28 11:44:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-28 11:44:21 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-05-28 11:44:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-05-28 11:44:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-05-28 11:44:21 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 128
2022-05-28 11:44:21 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/5_25_tokenized/transformer/checkpoint_last.pt
2022-05-28 11:44:21 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/5_25_tokenized/transformer/checkpoint_last.pt
2022-05-28 11:44:21 | INFO | fairseq.trainer | loading train data for epoch 1
2022-05-28 11:44:21 | INFO | fairseq.data.data_utils | loaded 771,964 examples from: data-bin/5_25.tokenized.gu-bai/train.gu-bai.gu
2022-05-28 11:44:21 | INFO | fairseq.data.data_utils | loaded 771,964 examples from: data-bin/5_25.tokenized.gu-bai/train.gu-bai.bai
2022-05-28 11:44:21 | INFO | fairseq.tasks.translation | data-bin/5_25.tokenized.gu-bai train gu-bai 771964 examples
2022-05-28 11:44:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 11:44:21 | INFO | fairseq.trainer | begin training epoch 1
2022-05-28 11:44:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 11:44:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-05-28 11:44:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-05-28 11:44:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-05-28 11:44:30 | INFO | train_inner | epoch 001:    103 / 6032 loss=13.965, nll_loss=13.682, ppl=13143.5, wps=29703.3, ups=12.46, wpb=2368.6, bsz=128, num_updates=100, lr=1.75e-05, gnorm=5.785, loss_scale=16, train_wall=8, gb_free=20.4, wall=9
2022-05-28 11:44:38 | INFO | train_inner | epoch 001:    203 / 6032 loss=12.162, nll_loss=11.669, ppl=3256.27, wps=37719.6, ups=12.78, wpb=2951.6, bsz=128, num_updates=200, lr=3.5e-05, gnorm=2.883, loss_scale=16, train_wall=8, gb_free=16.8, wall=17
2022-05-28 11:44:45 | INFO | train_inner | epoch 001:    303 / 6032 loss=11.248, nll_loss=10.629, ppl=1583.78, wps=36129.9, ups=13.27, wpb=2723.6, bsz=128, num_updates=300, lr=5.25e-05, gnorm=2.948, loss_scale=16, train_wall=7, gb_free=19.3, wall=25
2022-05-28 11:44:53 | INFO | train_inner | epoch 001:    403 / 6032 loss=10.786, nll_loss=10.075, ppl=1078.4, wps=36278.2, ups=13.09, wpb=2770.7, bsz=128, num_updates=400, lr=7e-05, gnorm=2.682, loss_scale=16, train_wall=7, gb_free=19.7, wall=32
2022-05-28 11:45:01 | INFO | train_inner | epoch 001:    503 / 6032 loss=10.62, nll_loss=9.86, ppl=929.56, wps=37144.6, ups=13.04, wpb=2848.4, bsz=128, num_updates=500, lr=8.75e-05, gnorm=2.278, loss_scale=16, train_wall=7, gb_free=19.2, wall=40
2022-05-28 11:45:08 | INFO | train_inner | epoch 001:    603 / 6032 loss=10.419, nll_loss=9.622, ppl=788.19, wps=32065.4, ups=14.05, wpb=2283, bsz=128, num_updates=600, lr=0.000105, gnorm=2.326, loss_scale=16, train_wall=7, gb_free=19.8, wall=47
2022-05-28 11:45:15 | INFO | train_inner | epoch 001:    703 / 6032 loss=10.352, nll_loss=9.541, ppl=745.15, wps=33812.7, ups=13.46, wpb=2512.4, bsz=128, num_updates=700, lr=0.0001225, gnorm=1.984, loss_scale=16, train_wall=7, gb_free=20.5, wall=54
2022-05-28 11:45:23 | INFO | train_inner | epoch 001:    803 / 6032 loss=10.342, nll_loss=9.526, ppl=737.19, wps=33577.4, ups=13.09, wpb=2565.7, bsz=128, num_updates=800, lr=0.00014, gnorm=2.196, loss_scale=16, train_wall=7, gb_free=20.2, wall=62
2022-05-28 11:45:31 | INFO | train_inner | epoch 001:    903 / 6032 loss=10.264, nll_loss=9.438, ppl=693.74, wps=36217, ups=12.93, wpb=2800, bsz=128, num_updates=900, lr=0.0001575, gnorm=1.758, loss_scale=16, train_wall=8, gb_free=19.3, wall=70
2022-05-28 11:45:38 | INFO | train_inner | epoch 001:   1003 / 6032 loss=10.155, nll_loss=9.316, ppl=637.35, wps=36041.8, ups=12.97, wpb=2778.8, bsz=128, num_updates=1000, lr=0.000175, gnorm=1.798, loss_scale=16, train_wall=8, gb_free=20, wall=77
2022-05-28 11:45:46 | INFO | train_inner | epoch 001:   1103 / 6032 loss=10.08, nll_loss=9.226, ppl=598.65, wps=35475.3, ups=13.02, wpb=2724, bsz=128, num_updates=1100, lr=0.0001925, gnorm=1.761, loss_scale=16, train_wall=7, gb_free=20.1, wall=85
2022-05-28 11:45:53 | INFO | train_inner | epoch 001:   1203 / 6032 loss=9.897, nll_loss=9.019, ppl=518.92, wps=34542.6, ups=13.56, wpb=2546.8, bsz=128, num_updates=1200, lr=0.00021, gnorm=1.675, loss_scale=16, train_wall=7, gb_free=18.1, wall=93
2022-05-28 11:45:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 2.05 GiB free; 19.98 GiB reserved in total by PyTorch)
2022-05-28 11:45:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13955 MB |   18277 MB |    7100 GB |    7087 GB |
|       from large pool |   13917 MB |   18239 MB |    6940 GB |    6926 GB |
|       from small pool |      38 MB |      72 MB |     160 GB |     160 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13955 MB |   18277 MB |    7100 GB |    7087 GB |
|       from large pool |   13917 MB |   18239 MB |    6940 GB |    6926 GB |
|       from small pool |      38 MB |      72 MB |     160 GB |     160 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20462 MB |   21810 MB |   36392 MB |   15930 MB |
|       from large pool |   20422 MB |   21608 MB |   36040 MB |   15618 MB |
|       from small pool |      40 MB |     202 MB |     352 MB |     312 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2184 MB |    2184 MB |    6946 GB |    6944 GB |
|       from large pool |    2182 MB |    2182 MB |    6756 GB |    6754 GB |
|       from small pool |       1 MB |      25 MB |     189 GB |     189 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    1884 K  |    1883 K  |
|       from large pool |     271    |     275    |     885 K  |     885 K  |
|       from small pool |     317    |     462    |     998 K  |     998 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    1884 K  |    1883 K  |
|       from large pool |     271    |     275    |     885 K  |     885 K  |
|       from small pool |     317    |     462    |     998 K  |     998 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     129    |     212    |     165    |
|       from large pool |      27    |      28    |      36    |       9    |
|       from small pool |      20    |     101    |     176    |     156    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      54    |     946 K  |     946 K  |
|       from large pool |      21    |      21    |     494 K  |     494 K  |
|       from small pool |       8    |      43    |     452 K  |     452 K  |
|===========================================================================|

2022-05-28 11:45:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:46:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 19.31 GiB already allocated; 1.57 GiB free; 20.46 GiB reserved in total by PyTorch)
2022-05-28 11:46:01 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15205 MB |   19771 MB |    7597 GB |    7582 GB |
|       from large pool |   15166 MB |   19732 MB |    7425 GB |    7410 GB |
|       from small pool |      38 MB |      72 MB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15205 MB |   19771 MB |    7597 GB |    7582 GB |
|       from large pool |   15166 MB |   19732 MB |    7425 GB |    7410 GB |
|       from small pool |      38 MB |      72 MB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20950 MB |   20950 MB |   45686 MB |   24736 MB |
|       from large pool |   20910 MB |   20910 MB |   45172 MB |   24262 MB |
|       from small pool |      40 MB |     202 MB |     514 MB |     474 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1178 MB |    4049 MB |    7431 GB |    7429 GB |
|       from large pool |    1177 MB |    4047 MB |    7227 GB |    7226 GB |
|       from small pool |       1 MB |      40 MB |     203 GB |     203 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    2015 K  |    2014 K  |
|       from large pool |     271    |     275    |     947 K  |     946 K  |
|       from small pool |     317    |     462    |    1068 K  |    1068 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    2015 K  |    2014 K  |
|       from large pool |     271    |     275    |     947 K  |     946 K  |
|       from small pool |     317    |     462    |    1068 K  |    1068 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     128    |     295    |     248    |
|       from large pool |      27    |      27    |      38    |      11    |
|       from small pool |      20    |     101    |     257    |     237    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      79    |    1012 K  |    1012 K  |
|       from large pool |      19    |      19    |     527 K  |     527 K  |
|       from small pool |       9    |      70    |     484 K  |     484 K  |
|===========================================================================|

2022-05-28 11:46:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:46:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 3.07 GiB free; 18.96 GiB reserved in total by PyTorch)
2022-05-28 11:46:01 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12703 MB |   16634 MB |    7635 GB |    7622 GB |
|       from large pool |   12664 MB |   16595 MB |    7462 GB |    7450 GB |
|       from small pool |      38 MB |      72 MB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12703 MB |   16634 MB |    7635 GB |    7622 GB |
|       from large pool |   12664 MB |   16595 MB |    7462 GB |    7450 GB |
|       from small pool |      38 MB |      72 MB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19412 MB |   20370 MB |   49672 MB |   30260 MB |
|       from large pool |   19372 MB |   20276 MB |   49104 MB |   29732 MB |
|       from small pool |      40 MB |      94 MB |     568 MB |     528 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2776 MB |    2778 MB |    7469 GB |    7466 GB |
|       from large pool |    2775 MB |    2776 MB |    7265 GB |    7262 GB |
|       from small pool |       1 MB |       3 MB |     204 GB |     204 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    2022 K  |    2021 K  |
|       from large pool |     271    |     275    |     950 K  |     950 K  |
|       from small pool |     317    |     462    |    1071 K  |    1071 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    2022 K  |    2021 K  |
|       from large pool |     271    |     275    |     950 K  |     950 K  |
|       from small pool |     317    |     462    |    1071 K  |    1071 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |      74    |     323    |     277    |
|       from large pool |      26    |      27    |      39    |      13    |
|       from small pool |      20    |      47    |     284    |     264    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      31    |    1015 K  |    1015 K  |
|       from large pool |      16    |      17    |     529 K  |     529 K  |
|       from small pool |       6    |      17    |     486 K  |     486 K  |
|===========================================================================|

2022-05-28 11:46:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:46:02 | INFO | train_inner | epoch 001:   1306 / 6032 loss=9.776, nll_loss=8.879, ppl=470.82, wps=30855, ups=12.08, wpb=2553.3, bsz=128, num_updates=1300, lr=0.0002275, gnorm=1.639, loss_scale=16, train_wall=7, gb_free=20.2, wall=101
2022-05-28 11:46:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.46 GiB (GPU 0; 23.70 GiB total capacity; 15.28 GiB already allocated; 3.45 GiB free; 18.58 GiB reserved in total by PyTorch)
2022-05-28 11:46:03 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12109 MB |   15649 MB |    7743 GB |    7731 GB |
|       from large pool |   12070 MB |   15611 MB |    7568 GB |    7556 GB |
|       from small pool |      38 MB |      72 MB |     174 GB |     174 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12109 MB |   15649 MB |    7743 GB |    7731 GB |
|       from large pool |   12070 MB |   15611 MB |    7568 GB |    7556 GB |
|       from small pool |      38 MB |      72 MB |     174 GB |     174 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19028 MB |   19112 MB |   53304 MB |   34276 MB |
|       from large pool |   18982 MB |   18982 MB |   52646 MB |   33664 MB |
|       from small pool |      46 MB |     130 MB |     658 MB |     612 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3376 MB |    3378 MB |    7586 GB |    7582 GB |
|       from large pool |    3369 MB |    3370 MB |    7379 GB |    7376 GB |
|       from small pool |       7 MB |      16 MB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    2047 K  |    2047 K  |
|       from large pool |     271    |     275    |     962 K  |     962 K  |
|       from small pool |     317    |     462    |    1084 K  |    1084 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    2047 K  |    2047 K  |
|       from large pool |     271    |     275    |     962 K  |     962 K  |
|       from small pool |     317    |     462    |    1084 K  |    1084 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |      91    |     369    |     320    |
|       from large pool |      26    |      26    |      40    |      14    |
|       from small pool |      23    |      65    |     329    |     306    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      54    |    1028 K  |    1028 K  |
|       from large pool |      15    |      17    |     536 K  |     536 K  |
|       from small pool |      10    |      38    |     492 K  |     492 K  |
|===========================================================================|

2022-05-28 11:46:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:46:09 | INFO | train_inner | epoch 001:   1407 / 6032 loss=9.685, nll_loss=8.774, ppl=437.85, wps=34474.1, ups=13, wpb=2652, bsz=128, num_updates=1400, lr=0.000245, gnorm=1.421, loss_scale=16, train_wall=7, gb_free=17.5, wall=109
2022-05-28 11:46:17 | INFO | train_inner | epoch 001:   1507 / 6032 loss=9.603, nll_loss=8.678, ppl=409.47, wps=34937.2, ups=12.95, wpb=2698.6, bsz=128, num_updates=1500, lr=0.0002625, gnorm=1.685, loss_scale=16, train_wall=8, gb_free=20.2, wall=116
2022-05-28 11:46:25 | INFO | train_inner | epoch 001:   1607 / 6032 loss=9.491, nll_loss=8.547, ppl=374.16, wps=33195.5, ups=13.21, wpb=2513.8, bsz=128, num_updates=1600, lr=0.00028, gnorm=1.638, loss_scale=16, train_wall=7, gb_free=17.7, wall=124
2022-05-28 11:46:32 | INFO | train_inner | epoch 001:   1707 / 6032 loss=9.397, nll_loss=8.44, ppl=347.27, wps=34922.8, ups=13.09, wpb=2668, bsz=128, num_updates=1700, lr=0.0002975, gnorm=1.491, loss_scale=16, train_wall=7, gb_free=21, wall=131
2022-05-28 11:46:40 | INFO | train_inner | epoch 001:   1807 / 6032 loss=9.342, nll_loss=8.374, ppl=331.78, wps=37127.6, ups=12.6, wpb=2947, bsz=128, num_updates=1800, lr=0.000315, gnorm=1.425, loss_scale=16, train_wall=8, gb_free=18.7, wall=139
2022-05-28 11:46:48 | INFO | train_inner | epoch 001:   1907 / 6032 loss=9.288, nll_loss=8.308, ppl=317, wps=36902.8, ups=12.97, wpb=2845.9, bsz=128, num_updates=1900, lr=0.0003325, gnorm=1.493, loss_scale=16, train_wall=8, gb_free=20.2, wall=147
2022-05-28 11:46:55 | INFO | train_inner | epoch 001:   2007 / 6032 loss=9.119, nll_loss=8.114, ppl=277.09, wps=35796.7, ups=13.35, wpb=2681.8, bsz=128, num_updates=2000, lr=0.00035, gnorm=1.445, loss_scale=16, train_wall=7, gb_free=17.8, wall=155
2022-05-28 11:47:03 | INFO | train_inner | epoch 001:   2107 / 6032 loss=8.973, nll_loss=7.945, ppl=246.49, wps=34581.2, ups=13.02, wpb=2655.2, bsz=128, num_updates=2100, lr=0.0003675, gnorm=1.417, loss_scale=16, train_wall=7, gb_free=20.5, wall=162
2022-05-28 11:47:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 473.69 MiB free; 21.57 GiB reserved in total by PyTorch)
2022-05-28 11:47:04 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21958 MB |   22258 MB |   12548 GB |   12526 GB |
|       from large pool |   21915 MB |   22215 MB |   12268 GB |   12246 GB |
|       from small pool |      42 MB |      72 MB |     280 GB |     280 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21958 MB |   22258 MB |   12548 GB |   12526 GB |
|       from large pool |   21915 MB |   22215 MB |   12268 GB |   12246 GB |
|       from small pool |      42 MB |      72 MB |     280 GB |     280 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22086 MB |   22544 MB |   67376 MB |   45290 MB |
|       from large pool |   22042 MB |   22342 MB |   66240 MB |   44198 MB |
|       from small pool |      44 MB |     202 MB |    1136 MB |    1092 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  130996 KB |    4282 MB |   12785 GB |   12785 GB |
|       from large pool |  129577 KB |    4282 MB |   12454 GB |   12454 GB |
|       from small pool |    1419 KB |      13 MB |     330 GB |     330 GB |
|---------------------------------------------------------------------------|
| Allocations           |     512    |     522    |    3292 K  |    3291 K  |
|       from large pool |     214    |     215    |    1549 K  |    1548 K  |
|       from small pool |     298    |     462    |    1743 K  |    1742 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     512    |     522    |    3292 K  |    3291 K  |
|       from large pool |     214    |     215    |    1549 K  |    1548 K  |
|       from small pool |     298    |     462    |    1743 K  |    1742 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     135    |     620    |     565    |
|       from large pool |      33    |      34    |      52    |      19    |
|       from small pool |      22    |     101    |     568    |     546    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      59    |    1653 K  |    1653 K  |
|       from large pool |      20    |      20    |     862 K  |     862 K  |
|       from small pool |      14    |      46    |     790 K  |     790 K  |
|===========================================================================|

2022-05-28 11:47:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:47:11 | INFO | train_inner | epoch 001:   2208 / 6032 loss=8.944, nll_loss=7.907, ppl=240.06, wps=34459.2, ups=12.69, wpb=2715.2, bsz=128, num_updates=2200, lr=0.000385, gnorm=1.395, loss_scale=16, train_wall=7, gb_free=20.4, wall=170
2022-05-28 11:47:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 3.42 GiB free; 18.61 GiB reserved in total by PyTorch)
2022-05-28 11:47:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14034 MB |   18404 MB |   13278 GB |   13264 GB |
|       from large pool |   13995 MB |   18365 MB |   12982 GB |   12968 GB |
|       from small pool |      39 MB |      73 MB |     295 GB |     295 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14034 MB |   18404 MB |   13278 GB |   13264 GB |
|       from large pool |   13995 MB |   18365 MB |   12982 GB |   12968 GB |
|       from small pool |      39 MB |      73 MB |     295 GB |     295 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19060 MB |   22240 MB |   71902 MB |   52842 MB |
|       from large pool |   19014 MB |   22042 MB |   70612 MB |   51598 MB |
|       from small pool |      46 MB |     198 MB |    1290 MB |    1244 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  669214 KB |    2872 MB |   13535 GB |   13535 GB |
|       from large pool |  662372 KB |    2865 MB |   13186 GB |   13185 GB |
|       from small pool |    6842 KB |       9 MB |     349 GB |     349 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    3477 K  |    3476 K  |
|       from large pool |     271    |     275    |    1636 K  |    1636 K  |
|       from small pool |     317    |     462    |    1841 K  |    1840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    3477 K  |    3476 K  |
|       from large pool |     271    |     275    |    1636 K  |    1636 K  |
|       from small pool |     317    |     462    |    1841 K  |    1840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     132    |     698    |     643    |
|       from large pool |      32    |      33    |      53    |      21    |
|       from small pool |      23    |      99    |     645    |     622    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      34    |    1746 K  |    1746 K  |
|       from large pool |      23    |      24    |     911 K  |     911 K  |
|       from small pool |       9    |      16    |     834 K  |     834 K  |
|===========================================================================|

2022-05-28 11:47:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:47:19 | INFO | train_inner | epoch 001:   2309 / 6032 loss=8.883, nll_loss=7.836, ppl=228.42, wps=34141.5, ups=12.45, wpb=2742.7, bsz=126.8, num_updates=2300, lr=0.0004025, gnorm=1.505, loss_scale=16, train_wall=8, gb_free=20.5, wall=178
2022-05-28 11:47:27 | INFO | train_inner | epoch 001:   2409 / 6032 loss=8.799, nll_loss=7.735, ppl=212.98, wps=34231.5, ups=12.58, wpb=2720.8, bsz=128, num_updates=2400, lr=0.00042, gnorm=1.461, loss_scale=16, train_wall=8, gb_free=19.3, wall=186
2022-05-28 11:47:34 | INFO | train_inner | epoch 001:   2509 / 6032 loss=8.601, nll_loss=7.508, ppl=182.07, wps=33444.2, ups=13.59, wpb=2460.2, bsz=128, num_updates=2500, lr=0.0004375, gnorm=1.419, loss_scale=16, train_wall=7, gb_free=19, wall=193
2022-05-28 11:47:42 | INFO | train_inner | epoch 001:   2609 / 6032 loss=8.618, nll_loss=7.524, ppl=184, wps=35484.8, ups=13.01, wpb=2727.4, bsz=128, num_updates=2600, lr=0.000455, gnorm=1.415, loss_scale=16, train_wall=8, gb_free=11.4, wall=201
2022-05-28 11:47:49 | INFO | train_inner | epoch 001:   2709 / 6032 loss=8.419, nll_loss=7.295, ppl=157, wps=32655.3, ups=13.7, wpb=2383.9, bsz=128, num_updates=2700, lr=0.0004725, gnorm=1.442, loss_scale=16, train_wall=7, gb_free=20.5, wall=208
2022-05-28 11:47:57 | INFO | train_inner | epoch 001:   2809 / 6032 loss=8.45, nll_loss=7.326, ppl=160.47, wps=33133.4, ups=12.89, wpb=2569.5, bsz=128, num_updates=2800, lr=0.00049, gnorm=1.424, loss_scale=16, train_wall=8, gb_free=20.4, wall=216
2022-05-28 11:48:05 | INFO | train_inner | epoch 001:   2909 / 6032 loss=8.431, nll_loss=7.3, ppl=157.58, wps=35033.1, ups=13.06, wpb=2683.2, bsz=128, num_updates=2900, lr=0.0005075, gnorm=1.418, loss_scale=16, train_wall=7, gb_free=17.8, wall=224
2022-05-28 11:48:12 | INFO | train_inner | epoch 001:   3009 / 6032 loss=8.348, nll_loss=7.203, ppl=147.3, wps=34709.7, ups=13.52, wpb=2567.1, bsz=128, num_updates=3000, lr=0.000525, gnorm=1.508, loss_scale=16, train_wall=7, gb_free=20.5, wall=231
2022-05-28 11:48:20 | INFO | train_inner | epoch 001:   3109 / 6032 loss=8.297, nll_loss=7.143, ppl=141.32, wps=35743.2, ups=13.36, wpb=2676, bsz=128, num_updates=3100, lr=0.0005425, gnorm=1.427, loss_scale=16, train_wall=7, gb_free=20.1, wall=239
2022-05-28 11:48:27 | INFO | train_inner | epoch 001:   3209 / 6032 loss=8.184, nll_loss=7.013, ppl=129.15, wps=34367.4, ups=13.35, wpb=2574.6, bsz=128, num_updates=3200, lr=0.00056, gnorm=1.446, loss_scale=16, train_wall=7, gb_free=19.7, wall=246
2022-05-28 11:48:35 | INFO | train_inner | epoch 001:   3309 / 6032 loss=8.305, nll_loss=7.145, ppl=141.54, wps=36630.8, ups=12.74, wpb=2875.8, bsz=128, num_updates=3300, lr=0.0005775, gnorm=1.437, loss_scale=16, train_wall=8, gb_free=19.7, wall=254
2022-05-28 11:48:43 | INFO | train_inner | epoch 001:   3409 / 6032 loss=8.167, nll_loss=6.986, ppl=126.77, wps=35879.7, ups=13.28, wpb=2702.3, bsz=128, num_updates=3400, lr=0.000595, gnorm=1.407, loss_scale=16, train_wall=7, gb_free=19.2, wall=262
2022-05-28 11:48:50 | INFO | train_inner | epoch 001:   3509 / 6032 loss=8.033, nll_loss=6.832, ppl=113.92, wps=34152.4, ups=13.34, wpb=2559.9, bsz=128, num_updates=3500, lr=0.0006125, gnorm=1.413, loss_scale=16, train_wall=7, gb_free=19.8, wall=269
2022-05-28 11:48:57 | INFO | train_inner | epoch 001:   3609 / 6032 loss=8.024, nll_loss=6.821, ppl=113.04, wps=34173.3, ups=13.37, wpb=2555.2, bsz=128, num_updates=3600, lr=0.00063, gnorm=1.474, loss_scale=16, train_wall=7, gb_free=20.4, wall=277
2022-05-28 11:49:05 | INFO | train_inner | epoch 001:   3709 / 6032 loss=8.207, nll_loss=7.026, ppl=130.29, wps=39116.5, ups=12.76, wpb=3065.2, bsz=128, num_updates=3700, lr=0.0006475, gnorm=1.396, loss_scale=16, train_wall=8, gb_free=19.9, wall=284
2022-05-28 11:49:13 | INFO | train_inner | epoch 001:   3809 / 6032 loss=8.107, nll_loss=6.911, ppl=120.36, wps=36845.2, ups=12.9, wpb=2856.9, bsz=128, num_updates=3800, lr=0.000665, gnorm=1.442, loss_scale=16, train_wall=8, gb_free=14.6, wall=292
2022-05-28 11:49:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 1.68 GiB free; 20.35 GiB reserved in total by PyTorch)
2022-05-28 11:49:17 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13822 MB |   17924 MB |   22785 GB |   22772 GB |
|       from large pool |   13783 MB |   17885 MB |   22274 GB |   22261 GB |
|       from small pool |      39 MB |      73 MB |     510 GB |     510 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13822 MB |   17924 MB |   22785 GB |   22772 GB |
|       from large pool |   13783 MB |   17885 MB |   22274 GB |   22261 GB |
|       from small pool |      39 MB |      73 MB |     510 GB |     510 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20838 MB |   20838 MB |  112520 MB |   91682 MB |
|       from large pool |   20796 MB |   20796 MB |  110762 MB |   89966 MB |
|       from small pool |      42 MB |     202 MB |    1758 MB |    1716 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2913 MB |    2913 MB |   22600 GB |   22597 GB |
|       from large pool |    2910 MB |    2910 MB |   21997 GB |   21994 GB |
|       from small pool |       2 MB |       7 MB |     602 GB |     602 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    6001 K  |    6000 K  |
|       from large pool |     271    |     275    |    2823 K  |    2823 K  |
|       from small pool |     317    |     462    |    3177 K  |    3177 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    6001 K  |    6000 K  |
|       from large pool |     271    |     275    |    2823 K  |    2823 K  |
|       from small pool |     317    |     462    |    3177 K  |    3177 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      54    |     134    |     946    |     892    |
|       from large pool |      33    |      33    |      67    |      34    |
|       from small pool |      21    |     101    |     879    |     858    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      35    |    3028 K  |    3028 K  |
|       from large pool |      24    |      24    |    1588 K  |    1588 K  |
|       from small pool |      10    |      19    |    1439 K  |    1439 K  |
|===========================================================================|

2022-05-28 11:49:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:49:21 | INFO | train_inner | epoch 001:   3910 / 6032 loss=8.101, nll_loss=6.902, ppl=119.6, wps=35902.9, ups=11.89, wpb=3020.3, bsz=128, num_updates=3900, lr=0.0006825, gnorm=1.546, loss_scale=16, train_wall=8, gb_free=16, wall=301
2022-05-28 11:49:29 | INFO | train_inner | epoch 001:   4010 / 6032 loss=7.951, nll_loss=6.731, ppl=106.24, wps=35470.5, ups=13.2, wpb=2688.1, bsz=128, num_updates=4000, lr=0.0007, gnorm=1.505, loss_scale=16, train_wall=7, gb_free=17.8, wall=308
2022-05-28 11:49:37 | INFO | train_inner | epoch 001:   4110 / 6032 loss=7.744, nll_loss=6.495, ppl=90.23, wps=33011.3, ups=13.41, wpb=2461.4, bsz=128, num_updates=4100, lr=0.000691411, gnorm=1.558, loss_scale=16, train_wall=7, gb_free=20.1, wall=316
2022-05-28 11:49:44 | INFO | train_inner | epoch 001:   4210 / 6032 loss=7.773, nll_loss=6.525, ppl=92.08, wps=35214.2, ups=13.23, wpb=2661.1, bsz=128, num_updates=4200, lr=0.00068313, gnorm=1.577, loss_scale=16, train_wall=7, gb_free=19.4, wall=323
2022-05-28 11:49:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.68 GiB already allocated; 4.49 GiB free; 17.54 GiB reserved in total by PyTorch)
2022-05-28 11:49:51 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17084 MB |   17142 MB |   25390 GB |   25373 GB |
|       from large pool |   17044 MB |   17102 MB |   24822 GB |   24806 GB |
|       from small pool |      39 MB |      73 MB |     567 GB |     567 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17084 MB |   17142 MB |   25390 GB |   25373 GB |
|       from large pool |   17044 MB |   17102 MB |   24822 GB |   24806 GB |
|       from small pool |      39 MB |      73 MB |     567 GB |     567 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   17958 MB |   20682 MB |  125068 MB |  107110 MB |
|       from large pool |   17916 MB |   20480 MB |  122998 MB |  105082 MB |
|       from small pool |      42 MB |     202 MB |    2070 MB |    2028 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     871 MB |    3643 MB |   25085 GB |   25085 GB |
|       from large pool |     871 MB |    3642 MB |   24415 GB |   24414 GB |
|       from small pool |       0 MB |      35 MB |     670 GB |     670 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    6686 K  |    6685 K  |
|       from large pool |     271    |     275    |    3147 K  |    3147 K  |
|       from small pool |     317    |     462    |    3539 K  |    3538 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    6686 K  |    6685 K  |
|       from large pool |     271    |     275    |    3147 K  |    3147 K  |
|       from small pool |     317    |     462    |    3539 K  |    3538 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      53    |     134    |    1105    |    1052    |
|       from large pool |      32    |      33    |      70    |      38    |
|       from small pool |      21    |     101    |    1035    |    1014    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      82    |    3376 K  |    3376 K  |
|       from large pool |      23    |      23    |    1772 K  |    1772 K  |
|       from small pool |       7    |      76    |    1603 K  |    1603 K  |
|===========================================================================|

2022-05-28 11:49:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:49:52 | INFO | train_inner | epoch 001:   4311 / 6032 loss=7.837, nll_loss=6.596, ppl=96.73, wps=34044.1, ups=12.6, wpb=2702.3, bsz=128, num_updates=4300, lr=0.00067514, gnorm=1.545, loss_scale=16, train_wall=7, gb_free=19.7, wall=331
2022-05-28 11:50:00 | INFO | train_inner | epoch 001:   4411 / 6032 loss=7.72, nll_loss=6.463, ppl=88.24, wps=33909.4, ups=13.31, wpb=2548.2, bsz=128, num_updates=4400, lr=0.000667424, gnorm=1.535, loss_scale=16, train_wall=7, gb_free=18.2, wall=339
2022-05-28 11:50:07 | INFO | train_inner | epoch 001:   4511 / 6032 loss=7.911, nll_loss=6.68, ppl=102.55, wps=36256.9, ups=13.15, wpb=2757.8, bsz=128, num_updates=4500, lr=0.000659966, gnorm=1.82, loss_scale=16, train_wall=7, gb_free=19.3, wall=346
2022-05-28 11:50:15 | INFO | train_inner | epoch 001:   4611 / 6032 loss=7.717, nll_loss=6.458, ppl=87.93, wps=36002.8, ups=13.35, wpb=2696.9, bsz=128, num_updates=4600, lr=0.000652753, gnorm=1.512, loss_scale=16, train_wall=7, gb_free=19.5, wall=354
2022-05-28 11:50:22 | INFO | train_inner | epoch 001:   4711 / 6032 loss=7.625, nll_loss=6.352, ppl=81.68, wps=34685.4, ups=13.5, wpb=2569.7, bsz=128, num_updates=4700, lr=0.000645772, gnorm=1.591, loss_scale=16, train_wall=7, gb_free=19.5, wall=361
2022-05-28 11:50:30 | INFO | train_inner | epoch 001:   4811 / 6032 loss=7.596, nll_loss=6.32, ppl=79.9, wps=34590.3, ups=13.3, wpb=2601.2, bsz=128, num_updates=4800, lr=0.00063901, gnorm=1.612, loss_scale=16, train_wall=7, gb_free=21, wall=369
2022-05-28 11:50:37 | INFO | train_inner | epoch 001:   4911 / 6032 loss=7.715, nll_loss=6.455, ppl=87.73, wps=36629.5, ups=13.03, wpb=2810.1, bsz=128, num_updates=4900, lr=0.000632456, gnorm=1.544, loss_scale=16, train_wall=7, gb_free=18.9, wall=376
2022-05-28 11:50:45 | INFO | train_inner | epoch 001:   5011 / 6032 loss=7.549, nll_loss=6.266, ppl=76.96, wps=35225, ups=13.28, wpb=2652.5, bsz=128, num_updates=5000, lr=0.000626099, gnorm=1.462, loss_scale=16, train_wall=7, gb_free=19.9, wall=384
2022-05-28 11:50:52 | INFO | train_inner | epoch 001:   5111 / 6032 loss=7.492, nll_loss=6.2, ppl=73.49, wps=35515.2, ups=13.1, wpb=2711.1, bsz=128, num_updates=5100, lr=0.00061993, gnorm=1.447, loss_scale=16, train_wall=7, gb_free=19.2, wall=392
2022-05-28 11:50:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 1.46 GiB free; 20.57 GiB reserved in total by PyTorch)
2022-05-28 11:50:59 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18611 MB |   18674 MB |   30580 GB |   30562 GB |
|       from large pool |   18571 MB |   18634 MB |   29892 GB |   29874 GB |
|       from small pool |      40 MB |      73 MB |     687 GB |     687 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18611 MB |   18674 MB |   30580 GB |   30562 GB |
|       from large pool |   18571 MB |   18634 MB |   29892 GB |   29874 GB |
|       from small pool |      40 MB |      73 MB |     687 GB |     687 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21064 MB |   21064 MB |  133458 MB |  112394 MB |
|       from large pool |   21020 MB |   21020 MB |  131226 MB |  110206 MB |
|       from small pool |      44 MB |     202 MB |    2232 MB |    2188 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2452 MB |    2931 MB |   30029 GB |   30026 GB |
|       from large pool |    2448 MB |    2926 MB |   29217 GB |   29214 GB |
|       from small pool |       3 MB |      16 MB |     811 GB |     811 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    8072 K  |    8071 K  |
|       from large pool |     271    |     275    |    3797 K  |    3796 K  |
|       from small pool |     317    |     462    |    4275 K  |    4274 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    8072 K  |    8071 K  |
|       from large pool |     271    |     275    |    3797 K  |    3796 K  |
|       from small pool |     317    |     462    |    4275 K  |    4274 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      54    |     134    |    1188    |    1134    |
|       from large pool |      32    |      33    |      72    |      40    |
|       from small pool |      22    |     101    |    1116    |    1094    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      38    |      62    |    4081 K  |    4081 K  |
|       from large pool |      27    |      27    |    2143 K  |    2143 K  |
|       from small pool |      11    |      49    |    1937 K  |    1937 K  |
|===========================================================================|

2022-05-28 11:50:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:51:01 | INFO | train_inner | epoch 001:   5212 / 6032 loss=7.546, nll_loss=6.261, ppl=76.72, wps=32578, ups=12.13, wpb=2684.9, bsz=128, num_updates=5200, lr=0.000613941, gnorm=1.415, loss_scale=16, train_wall=8, gb_free=18.6, wall=400
2022-05-28 11:51:08 | INFO | train_inner | epoch 001:   5312 / 6032 loss=7.433, nll_loss=6.132, ppl=70.15, wps=35032.3, ups=13.24, wpb=2645.1, bsz=128, num_updates=5300, lr=0.000608121, gnorm=1.42, loss_scale=16, train_wall=7, gb_free=18.2, wall=407
2022-05-28 11:51:16 | INFO | train_inner | epoch 001:   5412 / 6032 loss=7.349, nll_loss=6.034, ppl=65.54, wps=35942.5, ups=12.98, wpb=2768.4, bsz=128, num_updates=5400, lr=0.000602464, gnorm=1.386, loss_scale=16, train_wall=8, gb_free=20.3, wall=415
2022-05-28 11:51:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 2.11 GiB free; 19.92 GiB reserved in total by PyTorch)
2022-05-28 11:51:18 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12953 MB |   16933 MB |   32071 GB |   32058 GB |
|       from large pool |   12914 MB |   16894 MB |   31354 GB |   31341 GB |
|       from small pool |      39 MB |      73 MB |     717 GB |     717 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12953 MB |   16933 MB |   32071 GB |   32058 GB |
|       from large pool |   12914 MB |   16894 MB |   31354 GB |   31341 GB |
|       from small pool |      39 MB |      73 MB |     717 GB |     717 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20400 MB |   22272 MB |  145120 MB |  124720 MB |
|       from large pool |   20360 MB |   22070 MB |  142630 MB |  122270 MB |
|       from small pool |      40 MB |     202 MB |    2490 MB |    2450 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3466 MB |    3466 MB |   31476 GB |   31473 GB |
|       from large pool |    3465 MB |    3465 MB |   30629 GB |   30625 GB |
|       from small pool |       0 MB |       3 MB |     847 GB |     847 GB |
|---------------------------------------------------------------------------|
| Allocations           |     588    |     593    |    8454 K  |    8454 K  |
|       from large pool |     271    |     275    |    3980 K  |    3979 K  |
|       from small pool |     317    |     462    |    4474 K  |    4474 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     588    |     593    |    8454 K  |    8454 K  |
|       from large pool |     271    |     275    |    3980 K  |    3979 K  |
|       from small pool |     317    |     462    |    4474 K  |    4474 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     133    |    1320    |    1269    |
|       from large pool |      31    |      32    |      75    |      44    |
|       from small pool |      20    |     101    |    1245    |    1225    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      32    |    4275 K  |    4275 K  |
|       from large pool |      23    |      23    |    2247 K  |    2247 K  |
|       from small pool |       8    |      15    |    2027 K  |    2027 K  |
|===========================================================================|

2022-05-28 11:51:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:51:24 | INFO | train_inner | epoch 001:   5513 / 6032 loss=7.483, nll_loss=6.189, ppl=72.98, wps=35184.3, ups=12.51, wpb=2813.5, bsz=128, num_updates=5500, lr=0.000596962, gnorm=1.53, loss_scale=16, train_wall=8, gb_free=19.5, wall=423
2022-05-28 11:51:32 | INFO | train_inner | epoch 001:   5613 / 6032 loss=7.281, nll_loss=5.958, ppl=62.15, wps=33036.3, ups=12.94, wpb=2553.9, bsz=128, num_updates=5600, lr=0.000591608, gnorm=1.463, loss_scale=16, train_wall=8, gb_free=19.7, wall=431
2022-05-28 11:51:39 | INFO | train_inner | epoch 001:   5713 / 6032 loss=7.195, nll_loss=5.858, ppl=58.01, wps=33616.7, ups=13.08, wpb=2571, bsz=128, num_updates=5700, lr=0.000586395, gnorm=1.405, loss_scale=16, train_wall=7, gb_free=20.6, wall=438
2022-05-28 11:51:47 | INFO | train_inner | epoch 001:   5813 / 6032 loss=7.068, nll_loss=5.713, ppl=52.46, wps=32846.4, ups=13.81, wpb=2378.6, bsz=128, num_updates=5800, lr=0.000581318, gnorm=1.464, loss_scale=16, train_wall=7, gb_free=19.5, wall=446
2022-05-28 11:51:54 | INFO | train_inner | epoch 001:   5913 / 6032 loss=7.258, nll_loss=5.929, ppl=60.94, wps=35738.2, ups=13.01, wpb=2747.4, bsz=128, num_updates=5900, lr=0.000576371, gnorm=1.37, loss_scale=16, train_wall=8, gb_free=20.1, wall=453
2022-05-28 11:52:02 | INFO | train_inner | epoch 001:   6013 / 6032 loss=7.26, nll_loss=5.93, ppl=60.99, wps=36971.9, ups=12.45, wpb=2970.6, bsz=128, num_updates=6000, lr=0.000571548, gnorm=1.322, loss_scale=16, train_wall=8, gb_free=18.5, wall=461
2022-05-28 11:52:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 11:52:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.079 | nll_loss 5.673 | ppl 51.03 | wps 109704 | wpb 2687.4 | bsz 128 | num_updates 6019
2022-05-28 11:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 6019 updates
2022-05-28 11:52:22 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint1.pt
2022-05-28 11:52:26 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint1.pt
2022-05-28 11:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint1.pt (epoch 1 @ 6019 updates, score 7.079) (writing took 8.982732375152409 seconds)
2022-05-28 11:52:31 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-05-28 11:52:31 | INFO | train | epoch 001 | loss 8.73 | nll_loss 7.649 | ppl 200.72 | wps 32927.4 | ups 12.3 | wpb 2676.3 | bsz 128 | num_updates 6019 | lr 0.000570645 | gnorm 1.686 | loss_scale 16 | train_wall 448 | gb_free 19.8 | wall 490
2022-05-28 11:52:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 11:52:31 | INFO | fairseq.trainer | begin training epoch 2
2022-05-28 11:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 11:52:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 2.58 GiB free; 19.46 GiB reserved in total by PyTorch)
2022-05-28 11:52:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13821 MB |   17923 MB |   37140 GB |   37127 GB |
|       from large pool |   13781 MB |   17883 MB |   36315 GB |   36302 GB |
|       from small pool |      39 MB |      73 MB |     825 GB |     825 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13821 MB |   17923 MB |   37140 GB |   37127 GB |
|       from large pool |   13781 MB |   17883 MB |   36315 GB |   36302 GB |
|       from small pool |      39 MB |      73 MB |     825 GB |     825 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19922 MB |   20038 MB |  170786 MB |  150864 MB |
|       from large pool |   19878 MB |   19878 MB |  167596 MB |  147718 MB |
|       from small pool |      44 MB |     160 MB |    3190 MB |    3146 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1998 MB |    6054 MB |   37015 GB |   37013 GB |
|       from large pool |    1994 MB |    6049 MB |   36043 GB |   36041 GB |
|       from small pool |       4 MB |      31 MB |     972 GB |     972 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |    9757 K  |    9756 K  |
|       from large pool |     271    |     275    |    4616 K  |    4616 K  |
|       from small pool |     321    |     466    |    5140 K  |    5140 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |    9757 K  |    9756 K  |
|       from large pool |     271    |     275    |    4616 K  |    4616 K  |
|       from small pool |     321    |     466    |    5140 K  |    5140 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      44    |     102    |    1676    |    1632    |
|       from large pool |      22    |      22    |      81    |      59    |
|       from small pool |      22    |      80    |    1595    |    1573    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      46    |    4929 K  |    4929 K  |
|       from large pool |      16    |      16    |    2606 K  |    2606 K  |
|       from small pool |       8    |      34    |    2322 K  |    2322 K  |
|===========================================================================|

2022-05-28 11:52:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:52:38 | INFO | train_inner | epoch 002:     82 / 6032 loss=7.091, nll_loss=5.734, ppl=53.21, wps=7815.6, ups=2.81, wpb=2785.4, bsz=128, num_updates=6100, lr=0.000566843, gnorm=1.38, loss_scale=16, train_wall=8, gb_free=19.5, wall=497
2022-05-28 11:52:45 | INFO | train_inner | epoch 002:    182 / 6032 loss=6.927, nll_loss=5.546, ppl=46.71, wps=35224.5, ups=13.38, wpb=2632.9, bsz=128, num_updates=6200, lr=0.000562254, gnorm=1.339, loss_scale=16, train_wall=7, gb_free=19.7, wall=504
2022-05-28 11:52:53 | INFO | train_inner | epoch 002:    282 / 6032 loss=7.046, nll_loss=5.682, ppl=51.34, wps=35333.7, ups=13.33, wpb=2649.7, bsz=128, num_updates=6300, lr=0.000557773, gnorm=1.29, loss_scale=16, train_wall=7, gb_free=20.4, wall=512
2022-05-28 11:53:00 | INFO | train_inner | epoch 002:    382 / 6032 loss=6.935, nll_loss=5.551, ppl=46.89, wps=36981.1, ups=13.08, wpb=2828, bsz=128, num_updates=6400, lr=0.000553399, gnorm=1.27, loss_scale=16, train_wall=7, gb_free=17.7, wall=520
2022-05-28 11:53:08 | INFO | train_inner | epoch 002:    482 / 6032 loss=6.895, nll_loss=5.505, ppl=45.43, wps=36815, ups=13.07, wpb=2816.3, bsz=128, num_updates=6500, lr=0.000549125, gnorm=1.288, loss_scale=16, train_wall=7, gb_free=19.6, wall=527
2022-05-28 11:53:16 | INFO | train_inner | epoch 002:    582 / 6032 loss=6.803, nll_loss=5.401, ppl=42.26, wps=34436.9, ups=13.56, wpb=2539.9, bsz=128, num_updates=6600, lr=0.000544949, gnorm=1.353, loss_scale=16, train_wall=7, gb_free=19.7, wall=535
2022-05-28 11:53:23 | INFO | train_inner | epoch 002:    682 / 6032 loss=6.946, nll_loss=5.564, ppl=47.3, wps=34605.6, ups=13.35, wpb=2592.2, bsz=128, num_updates=6700, lr=0.000540867, gnorm=1.268, loss_scale=16, train_wall=7, gb_free=20.4, wall=542
2022-05-28 11:53:31 | INFO | train_inner | epoch 002:    782 / 6032 loss=6.785, nll_loss=5.379, ppl=41.6, wps=35407.6, ups=13.25, wpb=2673, bsz=128, num_updates=6800, lr=0.000536875, gnorm=1.299, loss_scale=16, train_wall=7, gb_free=17.6, wall=550
2022-05-28 11:53:38 | INFO | train_inner | epoch 002:    882 / 6032 loss=6.733, nll_loss=5.32, ppl=39.96, wps=32989.5, ups=13.78, wpb=2394.8, bsz=128, num_updates=6900, lr=0.000532971, gnorm=1.363, loss_scale=16, train_wall=7, gb_free=19.9, wall=557
2022-05-28 11:53:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 2.75 GiB free; 19.29 GiB reserved in total by PyTorch)
2022-05-28 11:53:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12703 MB |   16634 MB |   42603 GB |   42591 GB |
|       from large pool |   12664 MB |   16595 MB |   41658 GB |   41646 GB |
|       from small pool |      39 MB |      73 MB |     944 GB |     944 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12703 MB |   16634 MB |   42603 GB |   42591 GB |
|       from large pool |   12664 MB |   16595 MB |   41658 GB |   41646 GB |
|       from small pool |      39 MB |      73 MB |     944 GB |     944 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19748 MB |   19910 MB |  174876 MB |  155128 MB |
|       from large pool |   19708 MB |   19708 MB |  171528 MB |  151820 MB |
|       from small pool |      40 MB |     202 MB |    3348 MB |    3308 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3112 MB |    7000 MB |   50844 GB |   50841 GB |
|       from large pool |    3111 MB |    6999 MB |   49730 GB |   49727 GB |
|       from small pool |       0 MB |       3 MB |    1113 GB |    1113 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   11215 K  |   11214 K  |
|       from large pool |     271    |     275    |    5308 K  |    5308 K  |
|       from small pool |     321    |     466    |    5906 K  |    5906 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   11215 K  |   11214 K  |
|       from large pool |     271    |     275    |    5308 K  |    5308 K  |
|       from small pool |     321    |     466    |    5906 K  |    5906 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      42    |     123    |    1756    |    1714    |
|       from large pool |      22    |      22    |      82    |      60    |
|       from small pool |      20    |     101    |    1674    |    1654    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      31    |    5670 K  |    5670 K  |
|       from large pool |      12    |      18    |    2996 K  |    2996 K  |
|       from small pool |       8    |      15    |    2674 K  |    2674 K  |
|===========================================================================|

2022-05-28 11:53:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:53:46 | INFO | train_inner | epoch 002:    983 / 6032 loss=6.818, nll_loss=5.417, ppl=42.73, wps=34589.8, ups=12.82, wpb=2697.6, bsz=128, num_updates=7000, lr=0.00052915, gnorm=1.327, loss_scale=16, train_wall=7, gb_free=19.5, wall=565
2022-05-28 11:53:53 | INFO | train_inner | epoch 002:   1083 / 6032 loss=6.884, nll_loss=5.493, ppl=45.05, wps=34510.5, ups=13.2, wpb=2614.6, bsz=128, num_updates=7100, lr=0.000525411, gnorm=1.347, loss_scale=16, train_wall=7, gb_free=19.6, wall=572
2022-05-28 11:54:01 | INFO | train_inner | epoch 002:   1183 / 6032 loss=6.96, nll_loss=5.58, ppl=47.82, wps=34477.1, ups=12.53, wpb=2752.4, bsz=128, num_updates=7200, lr=0.000521749, gnorm=1.257, loss_scale=16, train_wall=8, gb_free=18.7, wall=580
2022-05-28 11:54:09 | INFO | train_inner | epoch 002:   1283 / 6032 loss=6.892, nll_loss=5.499, ppl=45.23, wps=37695.2, ups=12.84, wpb=2934.6, bsz=128, num_updates=7300, lr=0.000518163, gnorm=1.252, loss_scale=16, train_wall=8, gb_free=20.5, wall=588
2022-05-28 11:54:17 | INFO | train_inner | epoch 002:   1383 / 6032 loss=6.768, nll_loss=5.359, ppl=41.04, wps=34575.8, ups=13, wpb=2660.5, bsz=128, num_updates=7400, lr=0.00051465, gnorm=1.22, loss_scale=16, train_wall=8, gb_free=20.6, wall=596
2022-05-28 11:54:24 | INFO | train_inner | epoch 002:   1483 / 6032 loss=6.925, nll_loss=5.54, ppl=46.52, wps=37037.4, ups=12.98, wpb=2852.9, bsz=128, num_updates=7500, lr=0.000511208, gnorm=1.204, loss_scale=16, train_wall=8, gb_free=19.1, wall=604
2022-05-28 11:54:32 | INFO | train_inner | epoch 002:   1583 / 6032 loss=6.698, nll_loss=5.279, ppl=38.84, wps=33549.5, ups=13.47, wpb=2490.9, bsz=128, num_updates=7600, lr=0.000507833, gnorm=1.24, loss_scale=16, train_wall=7, gb_free=19.3, wall=611
2022-05-28 11:54:39 | INFO | train_inner | epoch 002:   1683 / 6032 loss=6.765, nll_loss=5.355, ppl=40.94, wps=36023.3, ups=13.11, wpb=2747.8, bsz=128, num_updates=7700, lr=0.000504525, gnorm=1.22, loss_scale=16, train_wall=7, gb_free=19.8, wall=619
2022-05-28 11:54:47 | INFO | train_inner | epoch 002:   1783 / 6032 loss=6.615, nll_loss=5.184, ppl=36.35, wps=32172.6, ups=13, wpb=2474.2, bsz=128, num_updates=7800, lr=0.00050128, gnorm=1.283, loss_scale=16, train_wall=8, gb_free=19.6, wall=626
2022-05-28 11:54:55 | INFO | train_inner | epoch 002:   1883 / 6032 loss=6.634, nll_loss=5.208, ppl=36.96, wps=35271.8, ups=13.37, wpb=2637.8, bsz=128, num_updates=7900, lr=0.000498098, gnorm=1.228, loss_scale=16, train_wall=7, gb_free=20.2, wall=634
2022-05-28 11:55:02 | INFO | train_inner | epoch 002:   1983 / 6032 loss=6.623, nll_loss=5.193, ppl=36.58, wps=35067.7, ups=13.28, wpb=2640.4, bsz=128, num_updates=8000, lr=0.000494975, gnorm=1.176, loss_scale=16, train_wall=7, gb_free=19.3, wall=641
2022-05-28 11:55:10 | INFO | train_inner | epoch 002:   2083 / 6032 loss=6.661, nll_loss=5.238, ppl=37.74, wps=36206.7, ups=13.07, wpb=2770, bsz=128, num_updates=8100, lr=0.00049191, gnorm=1.175, loss_scale=16, train_wall=7, gb_free=15.2, wall=649
2022-05-28 11:55:17 | INFO | train_inner | epoch 002:   2183 / 6032 loss=6.535, nll_loss=5.094, ppl=34.15, wps=34467.6, ups=13.32, wpb=2588.6, bsz=128, num_updates=8200, lr=0.000488901, gnorm=1.162, loss_scale=16, train_wall=7, gb_free=14.9, wall=656
2022-05-28 11:55:25 | INFO | train_inner | epoch 002:   2283 / 6032 loss=6.701, nll_loss=5.283, ppl=38.92, wps=36217.1, ups=12.79, wpb=2831.1, bsz=128, num_updates=8300, lr=0.000485947, gnorm=1.173, loss_scale=16, train_wall=8, gb_free=18.6, wall=664
2022-05-28 11:55:33 | INFO | train_inner | epoch 002:   2383 / 6032 loss=6.521, nll_loss=5.078, ppl=33.77, wps=36430.3, ups=12.96, wpb=2810.6, bsz=128, num_updates=8400, lr=0.000483046, gnorm=1.173, loss_scale=16, train_wall=8, gb_free=17.5, wall=672
2022-05-28 11:55:40 | INFO | train_inner | epoch 002:   2483 / 6032 loss=6.59, nll_loss=5.156, ppl=35.65, wps=35905.2, ups=13.17, wpb=2725.4, bsz=128, num_updates=8500, lr=0.000480196, gnorm=1.235, loss_scale=16, train_wall=7, gb_free=20.6, wall=680
2022-05-28 11:55:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 613.69 MiB free; 21.43 GiB reserved in total by PyTorch)
2022-05-28 11:55:41 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18611 MB |   18674 MB |   51599 GB |   51581 GB |
|       from large pool |   18571 MB |   18634 MB |   50456 GB |   50438 GB |
|       from small pool |      40 MB |      73 MB |    1142 GB |    1142 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18611 MB |   18674 MB |   51599 GB |   51581 GB |
|       from large pool |   18571 MB |   18634 MB |   50456 GB |   50438 GB |
|       from small pool |      40 MB |      73 MB |    1142 GB |    1142 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21946 MB |   22124 MB |  202924 MB |  180978 MB |
|       from large pool |   21902 MB |   21922 MB |  198928 MB |  177026 MB |
|       from small pool |      44 MB |     202 MB |    3996 MB |    3952 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3334 MB |    3691 MB |   67776 GB |   67772 GB |
|       from large pool |    3330 MB |    3686 MB |   66427 GB |   66424 GB |
|       from small pool |       3 MB |      10 MB |    1348 GB |    1348 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   13581 K  |   13581 K  |
|       from large pool |     271    |     275    |    6426 K  |    6425 K  |
|       from small pool |     321    |     466    |    7155 K  |    7155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   13581 K  |   13581 K  |
|       from large pool |     271    |     275    |    6426 K  |    6425 K  |
|       from small pool |     321    |     466    |    7155 K  |    7155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     125    |    2088    |    2043    |
|       from large pool |      23    |      24    |      90    |      67    |
|       from small pool |      22    |     101    |    1998    |    1976    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      36    |    6875 K  |    6875 K  |
|       from large pool |      16    |      16    |    3625 K  |    3625 K  |
|       from small pool |       9    |      23    |    3249 K  |    3249 K  |
|===========================================================================|

2022-05-28 11:55:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:55:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 4.08 GiB free; 17.95 GiB reserved in total by PyTorch)
2022-05-28 11:55:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15208 MB |   15259 MB |   51706 GB |   51691 GB |
|       from large pool |   15168 MB |   15219 MB |   50562 GB |   50547 GB |
|       from small pool |      39 MB |      73 MB |    1144 GB |    1144 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15208 MB |   15259 MB |   51706 GB |   51691 GB |
|       from large pool |   15168 MB |   15219 MB |   50562 GB |   50547 GB |
|       from small pool |      39 MB |      73 MB |    1144 GB |    1144 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18380 MB |   22030 MB |  203008 MB |  184628 MB |
|       from large pool |   18336 MB |   21902 MB |  198928 MB |  180592 MB |
|       from small pool |      44 MB |     128 MB |    4080 MB |    4036 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3171 MB |    3307 MB |   67961 GB |   67958 GB |
|       from large pool |    3167 MB |    3302 MB |   66610 GB |   66607 GB |
|       from small pool |       4 MB |      17 MB |    1350 GB |    1350 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   13605 K  |   13605 K  |
|       from large pool |     271    |     275    |    6438 K  |    6437 K  |
|       from small pool |     321    |     466    |    7167 K  |    7167 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   13605 K  |   13605 K  |
|       from large pool |     271    |     275    |    6438 K  |    6437 K  |
|       from small pool |     321    |     466    |    7167 K  |    7167 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      44    |      87    |    2130    |    2086    |
|       from large pool |      22    |      23    |      90    |      68    |
|       from small pool |      22    |      64    |    2040    |    2018    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      53    |    6887 K  |    6887 K  |
|       from large pool |      12    |      13    |    3631 K  |    3631 K  |
|       from small pool |      10    |      42    |    3255 K  |    3255 K  |
|===========================================================================|

2022-05-28 11:55:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:55:48 | INFO | train_inner | epoch 002:   2585 / 6032 loss=6.623, nll_loss=5.195, ppl=36.64, wps=33384.1, ups=12.43, wpb=2686.2, bsz=128, num_updates=8600, lr=0.000477396, gnorm=1.211, loss_scale=16, train_wall=7, gb_free=19.2, wall=688
2022-05-28 11:55:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 809.69 MiB free; 21.24 GiB reserved in total by PyTorch)
2022-05-28 11:55:51 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13631 MB |   17562 MB |   52341 GB |   52328 GB |
|       from large pool |   13592 MB |   17522 MB |   51182 GB |   51169 GB |
|       from small pool |      39 MB |      73 MB |    1158 GB |    1158 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13631 MB |   17562 MB |   52341 GB |   52328 GB |
|       from large pool |   13592 MB |   17522 MB |   51182 GB |   51169 GB |
|       from small pool |      39 MB |      73 MB |    1158 GB |    1158 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21750 MB |   21750 MB |  210086 MB |  188336 MB |
|       from large pool |   21706 MB |   21706 MB |  205864 MB |  184158 MB |
|       from small pool |      44 MB |     186 MB |    4222 MB |    4178 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4186 MB |    4187 MB |   69131 GB |   69127 GB |
|       from large pool |    4181 MB |    4183 MB |   67763 GB |   67759 GB |
|       from small pool |       4 MB |       8 MB |    1367 GB |    1367 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   13770 K  |   13770 K  |
|       from large pool |     271    |     275    |    6514 K  |    6514 K  |
|       from small pool |     321    |     466    |    7255 K  |    7255 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   13770 K  |   13770 K  |
|       from large pool |     271    |     275    |    6514 K  |    6514 K  |
|       from small pool |     321    |     466    |    7255 K  |    7255 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     116    |    2203    |    2158    |
|       from large pool |      23    |      23    |      92    |      69    |
|       from small pool |      22    |      93    |    2111    |    2089    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      33    |    6970 K  |    6970 K  |
|       from large pool |      19    |      20    |    3674 K  |    3674 K  |
|       from small pool |       9    |      19    |    3295 K  |    3295 K  |
|===========================================================================|

2022-05-28 11:55:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:55:56 | INFO | train_inner | epoch 002:   2686 / 6032 loss=6.38, nll_loss=4.919, ppl=30.24, wps=31833.1, ups=13.14, wpb=2421.8, bsz=128, num_updates=8700, lr=0.000474644, gnorm=1.193, loss_scale=16, train_wall=7, gb_free=19.6, wall=695
2022-05-28 11:56:03 | INFO | train_inner | epoch 002:   2786 / 6032 loss=6.568, nll_loss=5.133, ppl=35.1, wps=34165.9, ups=13.52, wpb=2527.5, bsz=128, num_updates=8800, lr=0.00047194, gnorm=1.185, loss_scale=16, train_wall=7, gb_free=19.2, wall=703
2022-05-28 11:56:11 | INFO | train_inner | epoch 002:   2886 / 6032 loss=6.494, nll_loss=5.047, ppl=33.06, wps=34005.3, ups=13, wpb=2616.6, bsz=128, num_updates=8900, lr=0.000469281, gnorm=1.144, loss_scale=16, train_wall=8, gb_free=18.3, wall=710
2022-05-28 11:56:19 | INFO | train_inner | epoch 002:   2986 / 6032 loss=6.442, nll_loss=4.989, ppl=31.77, wps=32912.8, ups=12.44, wpb=2646.2, bsz=128, num_updates=9000, lr=0.000466667, gnorm=1.202, loss_scale=16, train_wall=8, gb_free=15.6, wall=718
2022-05-28 11:56:27 | INFO | train_inner | epoch 002:   3086 / 6032 loss=6.314, nll_loss=4.839, ppl=28.63, wps=35595.8, ups=13.34, wpb=2667.6, bsz=128, num_updates=9100, lr=0.000464095, gnorm=1.172, loss_scale=16, train_wall=7, gb_free=20.6, wall=726
2022-05-28 11:56:35 | INFO | train_inner | epoch 002:   3186 / 6032 loss=6.65, nll_loss=5.227, ppl=37.46, wps=35543.2, ups=12.49, wpb=2845.5, bsz=128, num_updates=9200, lr=0.000461566, gnorm=1.134, loss_scale=16, train_wall=8, gb_free=20, wall=734
2022-05-28 11:56:42 | INFO | train_inner | epoch 002:   3286 / 6032 loss=6.535, nll_loss=5.092, ppl=34.11, wps=36736.7, ups=13.08, wpb=2809.6, bsz=128, num_updates=9300, lr=0.000459078, gnorm=1.126, loss_scale=16, train_wall=7, gb_free=18.9, wall=741
2022-05-28 11:56:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 3.40 GiB free; 18.63 GiB reserved in total by PyTorch)
2022-05-28 11:56:47 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   17220 MB |   56524 GB |   56511 GB |
|       from large pool |   13250 MB |   17180 MB |   55268 GB |   55255 GB |
|       from small pool |      39 MB |      73 MB |    1256 GB |    1256 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   17220 MB |   56524 GB |   56511 GB |
|       from large pool |   13250 MB |   17180 MB |   55268 GB |   55255 GB |
|       from small pool |      39 MB |      73 MB |    1256 GB |    1256 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19074 MB |   21384 MB |  267538 MB |  248464 MB |
|       from large pool |   19032 MB |   21182 MB |  262678 MB |  243646 MB |
|       from small pool |      42 MB |     202 MB |    4860 MB |    4818 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1852 MB |    2205 MB |   74510 GB |   74508 GB |
|       from large pool |    1849 MB |    2202 MB |   73027 GB |   73025 GB |
|       from small pool |       2 MB |      17 MB |    1482 GB |    1482 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   14905 K  |   14904 K  |
|       from large pool |     271    |     275    |    7046 K  |    7046 K  |
|       from small pool |     321    |     466    |    7858 K  |    7857 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   14905 K  |   14904 K  |
|       from large pool |     271    |     275    |    7046 K  |    7046 K  |
|       from small pool |     321    |     466    |    7858 K  |    7857 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     128    |    2564    |    2517    |
|       from large pool |      26    |      27    |     134    |     108    |
|       from small pool |      21    |     101    |    2430    |    2409    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      69    |    7546 K  |    7546 K  |
|       from large pool |      18    |      19    |    3972 K  |    3972 K  |
|       from small pool |       9    |      55    |    3574 K  |    3574 K  |
|===========================================================================|

2022-05-28 11:56:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:56:50 | INFO | train_inner | epoch 002:   3387 / 6032 loss=6.288, nll_loss=4.813, ppl=28.11, wps=32867.2, ups=13.14, wpb=2502, bsz=128, num_updates=9400, lr=0.00045663, gnorm=1.16, loss_scale=16, train_wall=7, gb_free=19.4, wall=749
2022-05-28 11:56:58 | INFO | train_inner | epoch 002:   3487 / 6032 loss=6.492, nll_loss=5.046, ppl=33.03, wps=35623.9, ups=13.1, wpb=2719.9, bsz=128, num_updates=9500, lr=0.00045422, gnorm=1.151, loss_scale=16, train_wall=7, gb_free=19.6, wall=757
2022-05-28 11:57:05 | INFO | train_inner | epoch 002:   3587 / 6032 loss=6.351, nll_loss=4.885, ppl=29.54, wps=34331.8, ups=13.36, wpb=2570.4, bsz=128, num_updates=9600, lr=0.000451848, gnorm=1.113, loss_scale=16, train_wall=7, gb_free=19.8, wall=764
2022-05-28 11:57:13 | INFO | train_inner | epoch 002:   3687 / 6032 loss=6.348, nll_loss=4.88, ppl=29.44, wps=36653.5, ups=13.03, wpb=2813.7, bsz=128, num_updates=9700, lr=0.000449513, gnorm=1.105, loss_scale=16, train_wall=7, gb_free=19, wall=772
2022-05-28 11:57:20 | INFO | train_inner | epoch 002:   3787 / 6032 loss=6.291, nll_loss=4.817, ppl=28.19, wps=33436.8, ups=13.65, wpb=2448.8, bsz=128, num_updates=9800, lr=0.000447214, gnorm=1.138, loss_scale=16, train_wall=7, gb_free=18.5, wall=779
2022-05-28 11:57:28 | INFO | train_inner | epoch 002:   3887 / 6032 loss=6.531, nll_loss=5.089, ppl=34.04, wps=35558.1, ups=12.21, wpb=2913.3, bsz=128, num_updates=9900, lr=0.000444949, gnorm=1.096, loss_scale=16, train_wall=8, gb_free=19.9, wall=787
2022-05-28 11:57:36 | INFO | train_inner | epoch 002:   3987 / 6032 loss=6.476, nll_loss=5.029, ppl=32.66, wps=35044.5, ups=12.78, wpb=2742.1, bsz=128, num_updates=10000, lr=0.000442719, gnorm=1.09, loss_scale=16, train_wall=8, gb_free=15.7, wall=795
2022-05-28 11:57:44 | INFO | train_inner | epoch 002:   4087 / 6032 loss=6.524, nll_loss=5.084, ppl=33.92, wps=33765.1, ups=12.42, wpb=2717.9, bsz=126.8, num_updates=10100, lr=0.000440522, gnorm=1.145, loss_scale=16, train_wall=8, gb_free=20.7, wall=803
2022-05-28 11:57:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 401.69 MiB free; 21.64 GiB reserved in total by PyTorch)
2022-05-28 11:57:46 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21956 MB |   22256 MB |   61104 GB |   61083 GB |
|       from large pool |   21913 MB |   22213 MB |   59743 GB |   59721 GB |
|       from small pool |      42 MB |      73 MB |    1361 GB |    1361 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21956 MB |   22256 MB |   61104 GB |   61083 GB |
|       from large pool |   21913 MB |   22213 MB |   59743 GB |   59721 GB |
|       from small pool |      42 MB |      73 MB |    1361 GB |    1361 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22158 MB |   22458 MB |  290526 MB |  268368 MB |
|       from large pool |   22112 MB |   22412 MB |  285184 MB |  263072 MB |
|       from small pool |      46 MB |     202 MB |    5342 MB |    5296 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  206265 KB |    4324 MB |   79035 GB |   79035 GB |
|       from large pool |  202800 KB |    4318 MB |   77428 GB |   77428 GB |
|       from small pool |    3465 KB |      12 MB |    1606 GB |    1606 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   16089 K  |   16089 K  |
|       from large pool |     214    |     215    |    7599 K  |    7599 K  |
|       from small pool |     302    |     466    |    8490 K  |    8489 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   16089 K  |   16089 K  |
|       from large pool |     214    |     215    |    7599 K  |    7599 K  |
|       from small pool |     302    |     466    |    8490 K  |    8489 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     133    |    2818    |    2763    |
|       from large pool |      32    |      33    |     147    |     115    |
|       from small pool |      23    |     101    |    2671    |    2648    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      37    |    8145 K  |    8145 K  |
|       from large pool |      22    |      22    |    4279 K  |    4279 K  |
|       from small pool |      12    |      22    |    3865 K  |    3865 K  |
|===========================================================================|

2022-05-28 11:57:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:57:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 1.42 GiB free; 20.61 GiB reserved in total by PyTorch)
2022-05-28 11:57:47 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14034 MB |   18404 MB |   61170 GB |   61157 GB |
|       from large pool |   13994 MB |   18364 MB |   59808 GB |   59795 GB |
|       from small pool |      39 MB |      73 MB |    1362 GB |    1362 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14034 MB |   18404 MB |   61170 GB |   61157 GB |
|       from large pool |   13994 MB |   18364 MB |   59808 GB |   59795 GB |
|       from small pool |      39 MB |      73 MB |    1362 GB |    1362 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21106 MB |   22196 MB |  299308 MB |  278202 MB |
|       from large pool |   21062 MB |   22112 MB |  293928 MB |  272866 MB |
|       from small pool |      44 MB |      84 MB |    5380 MB |    5336 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2699 MB |    2701 MB |   79107 GB |   79104 GB |
|       from large pool |    2695 MB |    2697 MB |   77500 GB |   77497 GB |
|       from small pool |       4 MB |      16 MB |    1607 GB |    1607 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   16102 K  |   16102 K  |
|       from large pool |     271    |     275    |    7606 K  |    7606 K  |
|       from small pool |     321    |     466    |    8496 K  |    8495 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   16102 K  |   16102 K  |
|       from large pool |     271    |     275    |    7606 K  |    7606 K  |
|       from small pool |     321    |     466    |    8496 K  |    8495 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      53    |      74    |    2839    |    2786    |
|       from large pool |      31    |      32    |     149    |     118    |
|       from small pool |      22    |      42    |    2690    |    2668    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      60    |    8151 K  |    8151 K  |
|       from large pool |      22    |      23    |    4283 K  |    4283 K  |
|       from small pool |      12    |      48    |    3868 K  |    3868 K  |
|===========================================================================|

2022-05-28 11:57:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:57:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.69 GiB already allocated; 3.54 GiB free; 18.49 GiB reserved in total by PyTorch)
2022-05-28 11:57:53 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17087 MB |   17145 MB |   61624 GB |   61607 GB |
|       from large pool |   17047 MB |   17104 MB |   60252 GB |   60236 GB |
|       from small pool |      39 MB |      73 MB |    1371 GB |    1371 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17087 MB |   17145 MB |   61624 GB |   61607 GB |
|       from large pool |   17047 MB |   17104 MB |   60252 GB |   60236 GB |
|       from small pool |      39 MB |      73 MB |    1371 GB |    1371 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18932 MB |   22064 MB |  311418 MB |  292486 MB |
|       from large pool |   18888 MB |   21868 MB |  305724 MB |  286836 MB |
|       from small pool |      44 MB |     196 MB |    5694 MB |    5650 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1844 MB |    3399 MB |   79521 GB |   79519 GB |
|       from large pool |    1840 MB |    3395 MB |   77903 GB |   77901 GB |
|       from small pool |       4 MB |      21 MB |    1618 GB |    1618 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   16215 K  |   16214 K  |
|       from large pool |     271    |     275    |    7659 K  |    7659 K  |
|       from small pool |     321    |     466    |    8555 K  |    8555 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   16215 K  |   16214 K  |
|       from large pool |     271    |     275    |    7659 K  |    7659 K  |
|       from small pool |     321    |     466    |    8555 K  |    8555 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     129    |    2999    |    2947    |
|       from large pool |      30    |      31    |     152    |     122    |
|       from small pool |      22    |      98    |    2847    |    2825    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      59    |    8209 K  |    8209 K  |
|       from large pool |      20    |      20    |    4313 K  |    4313 K  |
|       from small pool |       8    |      45    |    3895 K  |    3895 K  |
|===========================================================================|

2022-05-28 11:57:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:57:53 | INFO | train_inner | epoch 002:   4190 / 6032 loss=6.404, nll_loss=4.944, ppl=30.78, wps=31073.1, ups=11.42, wpb=2721.4, bsz=128, num_updates=10200, lr=0.000438357, gnorm=1.077, loss_scale=16, train_wall=8, gb_free=19.4, wall=812
2022-05-28 11:58:01 | INFO | train_inner | epoch 002:   4290 / 6032 loss=6.458, nll_loss=5.007, ppl=32.16, wps=37320.6, ups=12.54, wpb=2975.2, bsz=128, num_updates=10300, lr=0.000436224, gnorm=1.096, loss_scale=16, train_wall=8, gb_free=18.6, wall=820
2022-05-28 11:58:08 | INFO | train_inner | epoch 002:   4390 / 6032 loss=6.287, nll_loss=4.813, ppl=28.1, wps=34501.1, ups=13.36, wpb=2582.2, bsz=128, num_updates=10400, lr=0.000434122, gnorm=1.119, loss_scale=16, train_wall=7, gb_free=20, wall=827
2022-05-28 11:58:16 | INFO | train_inner | epoch 002:   4490 / 6032 loss=6.188, nll_loss=4.699, ppl=25.97, wps=32186.3, ups=13.43, wpb=2396.3, bsz=128, num_updates=10500, lr=0.000432049, gnorm=1.131, loss_scale=16, train_wall=7, gb_free=19.7, wall=835
2022-05-28 11:58:23 | INFO | train_inner | epoch 002:   4590 / 6032 loss=6.197, nll_loss=4.709, ppl=26.16, wps=34551.3, ups=13.54, wpb=2551.5, bsz=128, num_updates=10600, lr=0.000430007, gnorm=1.061, loss_scale=16, train_wall=7, gb_free=19.2, wall=842
2022-05-28 11:58:31 | INFO | train_inner | epoch 002:   4690 / 6032 loss=6.336, nll_loss=4.868, ppl=29.2, wps=35960.7, ups=13.15, wpb=2733.8, bsz=128, num_updates=10700, lr=0.000427992, gnorm=1.124, loss_scale=16, train_wall=7, gb_free=19.6, wall=850
2022-05-28 11:58:38 | INFO | train_inner | epoch 002:   4790 / 6032 loss=6.161, nll_loss=4.67, ppl=25.45, wps=33714.7, ups=13.48, wpb=2501.3, bsz=128, num_updates=10800, lr=0.000426006, gnorm=1.09, loss_scale=16, train_wall=7, gb_free=18.2, wall=857
2022-05-28 11:58:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 23.70 GiB total capacity; 15.64 GiB already allocated; 3.36 GiB free; 18.68 GiB reserved in total by PyTorch)
2022-05-28 11:58:40 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12349 MB |   16011 MB |   65212 GB |   65200 GB |
|       from large pool |   12310 MB |   15972 MB |   63759 GB |   63747 GB |
|       from small pool |      39 MB |      73 MB |    1453 GB |    1453 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12349 MB |   16011 MB |   65212 GB |   65200 GB |
|       from large pool |   12310 MB |   15972 MB |   63759 GB |   63747 GB |
|       from small pool |      39 MB |      73 MB |    1453 GB |    1453 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19124 MB |   22314 MB |  318464 MB |  299340 MB |
|       from large pool |   19084 MB |   22112 MB |  312612 MB |  293528 MB |
|       from small pool |      40 MB |     202 MB |    5852 MB |    5812 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3110 MB |    3112 MB |   82986 GB |   82983 GB |
|       from large pool |    3109 MB |    3111 MB |   81271 GB |   81268 GB |
|       from small pool |       0 MB |      16 MB |    1715 GB |    1715 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   17181 K  |   17180 K  |
|       from large pool |     271    |     275    |    8114 K  |    8113 K  |
|       from small pool |     321    |     466    |    9067 K  |    9066 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   17181 K  |   17180 K  |
|       from large pool |     271    |     275    |    8114 K  |    8113 K  |
|       from small pool |     321    |     466    |    9067 K  |    9066 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     132    |    3080    |    3030    |
|       from large pool |      30    |      31    |     154    |     124    |
|       from small pool |      20    |     101    |    2926    |    2906    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      71    |    8701 K  |    8701 K  |
|       from large pool |      22    |      23    |    4571 K  |    4571 K  |
|       from small pool |       9    |      54    |    4130 K  |    4130 K  |
|===========================================================================|

2022-05-28 11:58:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:58:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 2.71 GiB free; 19.32 GiB reserved in total by PyTorch)
2022-05-28 11:58:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13956 MB |   18278 MB |   65597 GB |   65584 GB |
|       from large pool |   13916 MB |   18238 MB |   64135 GB |   64122 GB |
|       from small pool |      39 MB |      73 MB |    1462 GB |    1462 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13956 MB |   18278 MB |   65597 GB |   65584 GB |
|       from large pool |   13916 MB |   18238 MB |   64135 GB |   64122 GB |
|       from small pool |      39 MB |      73 MB |    1462 GB |    1462 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19782 MB |   19938 MB |  322942 MB |  303160 MB |
|       from large pool |   19742 MB |   19742 MB |  316934 MB |  297192 MB |
|       from small pool |      40 MB |     196 MB |    6008 MB |    5968 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1503 MB |    3288 MB |   83355 GB |   83353 GB |
|       from large pool |    1503 MB |    3287 MB |   81629 GB |   81627 GB |
|       from small pool |       0 MB |      11 MB |    1725 GB |    1725 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   17285 K  |   17285 K  |
|       from large pool |     271    |     275    |    8163 K  |    8162 K  |
|       from small pool |     321    |     466    |    9122 K  |    9122 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   17285 K  |   17285 K  |
|       from large pool |     271    |     275    |    8163 K  |    8162 K  |
|       from small pool |     321    |     466    |    9122 K  |    9122 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     128    |    3159    |    3109    |
|       from large pool |      30    |      30    |     155    |     125    |
|       from small pool |      20    |      98    |    3004    |    2984    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      53    |    8754 K  |    8754 K  |
|       from large pool |      23    |      23    |    4599 K  |    4599 K  |
|       from small pool |       4    |      38    |    4155 K  |    4155 K  |
|===========================================================================|

2022-05-28 11:58:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 11:58:46 | INFO | train_inner | epoch 002:   4892 / 6032 loss=6.205, nll_loss=4.72, ppl=26.35, wps=32026.3, ups=12.69, wpb=2524.1, bsz=128, num_updates=10900, lr=0.000424048, gnorm=1.066, loss_scale=16, train_wall=7, gb_free=20.9, wall=865
2022-05-28 11:58:53 | INFO | train_inner | epoch 002:   4992 / 6032 loss=6.18, nll_loss=4.691, ppl=25.83, wps=33246.1, ups=13.61, wpb=2442, bsz=128, num_updates=11000, lr=0.000422116, gnorm=1.183, loss_scale=16, train_wall=7, gb_free=17.7, wall=873
2022-05-28 11:59:01 | INFO | train_inner | epoch 002:   5092 / 6032 loss=6.294, nll_loss=4.822, ppl=28.28, wps=35741.8, ups=13.18, wpb=2711.4, bsz=128, num_updates=11100, lr=0.00042021, gnorm=1.106, loss_scale=16, train_wall=7, gb_free=20.7, wall=880
2022-05-28 11:59:09 | INFO | train_inner | epoch 002:   5192 / 6032 loss=6.285, nll_loss=4.812, ppl=28.1, wps=33365.9, ups=12.76, wpb=2615.4, bsz=128, num_updates=11200, lr=0.00041833, gnorm=1.117, loss_scale=16, train_wall=8, gb_free=19.7, wall=888
2022-05-28 11:59:16 | INFO | train_inner | epoch 002:   5292 / 6032 loss=6.184, nll_loss=4.695, ppl=25.91, wps=33584.3, ups=13.29, wpb=2526.5, bsz=128, num_updates=11300, lr=0.000416475, gnorm=1.068, loss_scale=16, train_wall=7, gb_free=18.8, wall=896
2022-05-28 11:59:24 | INFO | train_inner | epoch 002:   5392 / 6032 loss=6.345, nll_loss=4.881, ppl=29.46, wps=37582.2, ups=12.81, wpb=2932.8, bsz=128, num_updates=11400, lr=0.000414644, gnorm=1.065, loss_scale=16, train_wall=8, gb_free=20.4, wall=903
2022-05-28 11:59:32 | INFO | train_inner | epoch 002:   5492 / 6032 loss=6.355, nll_loss=4.89, ppl=29.66, wps=36814.2, ups=12.82, wpb=2871.8, bsz=128, num_updates=11500, lr=0.000412837, gnorm=1.057, loss_scale=16, train_wall=8, gb_free=20.2, wall=911
2022-05-28 11:59:40 | INFO | train_inner | epoch 002:   5592 / 6032 loss=6.339, nll_loss=4.873, ppl=29.31, wps=36146.4, ups=12.33, wpb=2932, bsz=128, num_updates=11600, lr=0.000411054, gnorm=1.055, loss_scale=16, train_wall=8, gb_free=18.3, wall=919
2022-05-28 11:59:48 | INFO | train_inner | epoch 002:   5692 / 6032 loss=6.089, nll_loss=4.588, ppl=24.05, wps=35611.6, ups=13.33, wpb=2671.2, bsz=128, num_updates=11700, lr=0.000409294, gnorm=1.088, loss_scale=16, train_wall=7, gb_free=20, wall=927
2022-05-28 11:59:55 | INFO | train_inner | epoch 002:   5792 / 6032 loss=6.176, nll_loss=4.685, ppl=25.73, wps=37785.5, ups=12.88, wpb=2933.1, bsz=128, num_updates=11800, lr=0.000407556, gnorm=1.058, loss_scale=16, train_wall=8, gb_free=19.2, wall=935
2022-05-28 12:00:03 | INFO | train_inner | epoch 002:   5892 / 6032 loss=6.026, nll_loss=4.516, ppl=22.87, wps=34025.3, ups=13.35, wpb=2549.3, bsz=128, num_updates=11900, lr=0.00040584, gnorm=1.053, loss_scale=16, train_wall=7, gb_free=7.6, wall=942
2022-05-28 12:00:10 | INFO | train_inner | epoch 002:   5992 / 6032 loss=6.064, nll_loss=4.562, ppl=23.62, wps=33091, ups=13.65, wpb=2424.4, bsz=128, num_updates=12000, lr=0.000404145, gnorm=1.055, loss_scale=16, train_wall=7, gb_free=16.5, wall=949
2022-05-28 12:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:00:32 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.074 | nll_loss 4.535 | ppl 23.18 | wps 110323 | wpb 2687.4 | bsz 128 | num_updates 12040 | best_loss 6.074
2022-05-28 12:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 12040 updates
2022-05-28 12:00:32 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint2.pt
2022-05-28 12:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint2.pt
2022-05-28 12:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint2.pt (epoch 2 @ 12040 updates, score 6.074) (writing took 9.587625837884843 seconds)
2022-05-28 12:00:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-05-28 12:00:42 | INFO | train | epoch 002 | loss 6.528 | nll_loss 5.087 | ppl 33.99 | wps 32858 | ups 12.28 | wpb 2675.4 | bsz 128 | num_updates 12040 | lr 0.000403473 | gnorm 1.175 | loss_scale 16 | train_wall 448 | gb_free 12.8 | wall 981
2022-05-28 12:00:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:00:42 | INFO | fairseq.trainer | begin training epoch 3
2022-05-28 12:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:00:46 | INFO | train_inner | epoch 003:     60 / 6032 loss=6.083, nll_loss=4.577, ppl=23.87, wps=7554.9, ups=2.78, wpb=2714, bsz=128, num_updates=12100, lr=0.000402472, gnorm=1.069, loss_scale=16, train_wall=8, gb_free=16.2, wall=985
2022-05-28 12:00:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 19.31 GiB already allocated; 1.52 GiB free; 20.51 GiB reserved in total by PyTorch)
2022-05-28 12:00:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15209 MB |   19775 MB |   74584 GB |   74569 GB |
|       from large pool |   15169 MB |   19735 MB |   72927 GB |   72912 GB |
|       from small pool |      39 MB |      73 MB |    1657 GB |    1657 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15209 MB |   19775 MB |   74584 GB |   74569 GB |
|       from large pool |   15169 MB |   19735 MB |   72927 GB |   72912 GB |
|       from small pool |      39 MB |      73 MB |    1657 GB |    1657 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21006 MB |   21168 MB |  347718 MB |  326712 MB |
|       from large pool |   20966 MB |   20966 MB |  341264 MB |  320298 MB |
|       from small pool |      40 MB |     202 MB |    6454 MB |    6414 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1230 MB |    5745 MB |   92335 GB |   92334 GB |
|       from large pool |    1230 MB |    5744 MB |   90380 GB |   90379 GB |
|       from small pool |       0 MB |       3 MB |    1955 GB |    1955 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   19613 K  |   19613 K  |
|       from large pool |     271    |     275    |    9283 K  |    9282 K  |
|       from small pool |     321    |     466    |   10330 K  |   10330 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   19613 K  |   19613 K  |
|       from large pool |     271    |     275    |    9283 K  |    9282 K  |
|       from small pool |     321    |     466    |   10330 K  |   10330 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      41    |     122    |    3388    |    3347    |
|       from large pool |      21    |      21    |     161    |     140    |
|       from small pool |      20    |     101    |    3227    |    3207    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      32    |    9949 K  |    9949 K  |
|       from large pool |      11    |      16    |    5239 K  |    5239 K  |
|       from small pool |       8    |      17    |    4709 K  |    4709 K  |
|===========================================================================|

2022-05-28 12:00:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:00:54 | INFO | train_inner | epoch 003:    161 / 6032 loss=5.766, nll_loss=4.213, ppl=18.55, wps=34499.8, ups=13.09, wpb=2635.1, bsz=128, num_updates=12200, lr=0.000400819, gnorm=0.981, loss_scale=16, train_wall=7, gb_free=19.9, wall=993
2022-05-28 12:01:01 | INFO | train_inner | epoch 003:    261 / 6032 loss=5.838, nll_loss=4.295, ppl=19.63, wps=34335, ups=13.47, wpb=2549.8, bsz=128, num_updates=12300, lr=0.000399186, gnorm=1.042, loss_scale=16, train_wall=7, gb_free=15.6, wall=1000
2022-05-28 12:01:09 | INFO | train_inner | epoch 003:    361 / 6032 loss=5.896, nll_loss=4.361, ppl=20.54, wps=33977.4, ups=13.17, wpb=2580.4, bsz=128, num_updates=12400, lr=0.000397573, gnorm=1.04, loss_scale=16, train_wall=7, gb_free=13.7, wall=1008
2022-05-28 12:01:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 717.69 MiB free; 21.33 GiB reserved in total by PyTorch)
2022-05-28 12:01:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18612 MB |   18675 MB |   76007 GB |   75989 GB |
|       from large pool |   18572 MB |   18634 MB |   74315 GB |   74297 GB |
|       from small pool |      40 MB |      73 MB |    1692 GB |    1692 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18612 MB |   18675 MB |   76007 GB |   75989 GB |
|       from large pool |   18572 MB |   18634 MB |   74315 GB |   74297 GB |
|       from small pool |      40 MB |      73 MB |    1692 GB |    1692 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21842 MB |   22022 MB |  353300 MB |  331458 MB |
|       from large pool |   21800 MB |   21820 MB |  346684 MB |  324884 MB |
|       from small pool |      42 MB |     202 MB |    6616 MB |    6574 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3229 MB |    7467 MB |   94689 GB |   94686 GB |
|       from large pool |    3227 MB |    7465 MB |   92693 GB |   92690 GB |
|       from small pool |       1 MB |      22 MB |    1995 GB |    1995 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   19995 K  |   19994 K  |
|       from large pool |     271    |     275    |    9459 K  |    9459 K  |
|       from small pool |     321    |     466    |   10535 K  |   10535 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   19995 K  |   19994 K  |
|       from large pool |     271    |     275    |    9459 K  |    9459 K  |
|       from small pool |     321    |     466    |   10535 K  |   10535 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      41    |     122    |    3470    |    3429    |
|       from large pool |      20    |      21    |     162    |     142    |
|       from small pool |      21    |     101    |    3308    |    3287    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      58    |   10141 K  |   10141 K  |
|       from large pool |      13    |      14    |    5337 K  |    5337 K  |
|       from small pool |       7    |      45    |    4803 K  |    4803 K  |
|===========================================================================|

2022-05-28 12:01:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:01:17 | INFO | train_inner | epoch 003:    462 / 6032 loss=6.17, nll_loss=4.674, ppl=25.52, wps=36475.3, ups=12.59, wpb=2897.6, bsz=128, num_updates=12500, lr=0.00039598, gnorm=1.023, loss_scale=16, train_wall=8, gb_free=19.5, wall=1016
2022-05-28 12:01:24 | INFO | train_inner | epoch 003:    562 / 6032 loss=5.938, nll_loss=4.408, ppl=21.23, wps=35064.8, ups=12.91, wpb=2715.7, bsz=128, num_updates=12600, lr=0.000394405, gnorm=1.015, loss_scale=16, train_wall=8, gb_free=10.7, wall=1024
2022-05-28 12:01:32 | INFO | train_inner | epoch 003:    662 / 6032 loss=5.978, nll_loss=4.453, ppl=21.9, wps=36288.9, ups=12.76, wpb=2843.4, bsz=128, num_updates=12700, lr=0.000392849, gnorm=1.012, loss_scale=16, train_wall=8, gb_free=18.6, wall=1031
2022-05-28 12:01:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 415.69 MiB free; 21.62 GiB reserved in total by PyTorch)
2022-05-28 12:01:36 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21956 MB |   22256 MB |   78054 GB |   78033 GB |
|       from large pool |   21913 MB |   22213 MB |   76318 GB |   76296 GB |
|       from small pool |      42 MB |      73 MB |    1736 GB |    1736 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21956 MB |   22256 MB |   78054 GB |   78033 GB |
|       from large pool |   21913 MB |   22213 MB |   76318 GB |   76296 GB |
|       from small pool |      42 MB |      73 MB |    1736 GB |    1736 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22144 MB |   22444 MB |  354060 MB |  331916 MB |
|       from large pool |   22100 MB |   22400 MB |  347284 MB |  325184 MB |
|       from small pool |      44 MB |     202 MB |    6776 MB |    6732 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  192209 KB |    7405 MB |   98111 GB |   98110 GB |
|       from large pool |  190792 KB |    7404 MB |   96063 GB |   96063 GB |
|       from small pool |    1417 KB |       6 MB |    2047 GB |    2047 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   20509 K  |   20508 K  |
|       from large pool |     214    |     215    |    9701 K  |    9700 K  |
|       from small pool |     302    |     466    |   10807 K  |   10807 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   20509 K  |   20508 K  |
|       from large pool |     214    |     215    |    9701 K  |    9700 K  |
|       from small pool |     302    |     466    |   10807 K  |   10807 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     122    |    3552    |    3509    |
|       from large pool |      21    |      22    |     164    |     143    |
|       from small pool |      22    |     101    |    3388    |    3366    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      34    |   10398 K  |   10398 K  |
|       from large pool |      12    |      15    |    5469 K  |    5469 K  |
|       from small pool |       9    |      20    |    4928 K  |    4928 K  |
|===========================================================================|

2022-05-28 12:01:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:01:40 | INFO | train_inner | epoch 003:    763 / 6032 loss=6.041, nll_loss=4.525, ppl=23.03, wps=35426.1, ups=12.42, wpb=2851.3, bsz=128, num_updates=12800, lr=0.000391312, gnorm=1.017, loss_scale=16, train_wall=8, gb_free=19.9, wall=1039
2022-05-28 12:01:48 | INFO | train_inner | epoch 003:    863 / 6032 loss=5.86, nll_loss=4.321, ppl=19.99, wps=32749, ups=13.79, wpb=2374.7, bsz=128, num_updates=12900, lr=0.000389792, gnorm=1.023, loss_scale=16, train_wall=7, gb_free=19.2, wall=1047
2022-05-28 12:01:55 | INFO | train_inner | epoch 003:    963 / 6032 loss=6.047, nll_loss=4.534, ppl=23.16, wps=36492.5, ups=12.7, wpb=2874, bsz=128, num_updates=13000, lr=0.00038829, gnorm=1.048, loss_scale=16, train_wall=8, gb_free=20.9, wall=1055
2022-05-28 12:02:03 | INFO | train_inner | epoch 003:   1063 / 6032 loss=6.092, nll_loss=4.584, ppl=23.98, wps=36605.8, ups=12.7, wpb=2882.8, bsz=128, num_updates=13100, lr=0.000386805, gnorm=1.003, loss_scale=16, train_wall=8, gb_free=20.1, wall=1062
2022-05-28 12:02:11 | INFO | train_inner | epoch 003:   1163 / 6032 loss=5.95, nll_loss=4.423, ppl=21.45, wps=34765, ups=13.19, wpb=2635.9, bsz=128, num_updates=13200, lr=0.000385337, gnorm=1.038, loss_scale=16, train_wall=7, gb_free=18, wall=1070
2022-05-28 12:02:19 | INFO | train_inner | epoch 003:   1263 / 6032 loss=5.979, nll_loss=4.457, ppl=21.96, wps=35932.3, ups=13.04, wpb=2754.9, bsz=128, num_updates=13300, lr=0.000383886, gnorm=1.015, loss_scale=16, train_wall=7, gb_free=20.3, wall=1078
2022-05-28 12:02:26 | INFO | train_inner | epoch 003:   1363 / 6032 loss=5.717, nll_loss=4.156, ppl=17.83, wps=34000.4, ups=13.71, wpb=2480.8, bsz=128, num_updates=13400, lr=0.000382451, gnorm=0.972, loss_scale=16, train_wall=7, gb_free=20.1, wall=1085
2022-05-28 12:02:34 | INFO | train_inner | epoch 003:   1463 / 6032 loss=5.959, nll_loss=4.432, ppl=21.59, wps=37620.5, ups=12.59, wpb=2987.7, bsz=128, num_updates=13500, lr=0.000381032, gnorm=1.021, loss_scale=16, train_wall=8, gb_free=19.6, wall=1093
2022-05-28 12:02:41 | INFO | train_inner | epoch 003:   1563 / 6032 loss=5.92, nll_loss=4.389, ppl=20.95, wps=34905.4, ups=13.26, wpb=2632.4, bsz=128, num_updates=13600, lr=0.000379628, gnorm=1.009, loss_scale=16, train_wall=7, gb_free=20.8, wall=1101
2022-05-28 12:02:49 | INFO | train_inner | epoch 003:   1663 / 6032 loss=5.944, nll_loss=4.416, ppl=21.35, wps=35242.3, ups=13.24, wpb=2661.4, bsz=128, num_updates=13700, lr=0.00037824, gnorm=0.993, loss_scale=16, train_wall=7, gb_free=20.2, wall=1108
2022-05-28 12:02:56 | INFO | train_inner | epoch 003:   1763 / 6032 loss=5.772, nll_loss=4.22, ppl=18.64, wps=34350.5, ups=13.5, wpb=2544.9, bsz=128, num_updates=13800, lr=0.000376867, gnorm=0.988, loss_scale=16, train_wall=7, gb_free=20.3, wall=1115
2022-05-28 12:03:04 | INFO | train_inner | epoch 003:   1863 / 6032 loss=5.88, nll_loss=4.344, ppl=20.31, wps=35076.8, ups=13.12, wpb=2674.5, bsz=128, num_updates=13900, lr=0.000375509, gnorm=1.027, loss_scale=16, train_wall=7, gb_free=19.8, wall=1123
2022-05-28 12:03:12 | INFO | train_inner | epoch 003:   1963 / 6032 loss=6.147, nll_loss=4.649, ppl=25.08, wps=36715.2, ups=12.58, wpb=2917.7, bsz=128, num_updates=14000, lr=0.000374166, gnorm=1.032, loss_scale=16, train_wall=8, gb_free=20.2, wall=1131
2022-05-28 12:03:20 | INFO | train_inner | epoch 003:   2063 / 6032 loss=5.912, nll_loss=4.38, ppl=20.82, wps=36777.9, ups=12.91, wpb=2849.9, bsz=128, num_updates=14100, lr=0.000372837, gnorm=0.946, loss_scale=16, train_wall=8, gb_free=20, wall=1139
2022-05-28 12:03:27 | INFO | train_inner | epoch 003:   2163 / 6032 loss=5.835, nll_loss=4.293, ppl=19.61, wps=36479.7, ups=12.9, wpb=2827.2, bsz=128, num_updates=14200, lr=0.000371521, gnorm=0.972, loss_scale=16, train_wall=8, gb_free=19.2, wall=1147
2022-05-28 12:03:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.68 GiB already allocated; 415.69 MiB free; 21.62 GiB reserved in total by PyTorch)
2022-05-28 12:03:28 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17084 MB |   17143 MB |   86804 GB |   86788 GB |
|       from large pool |   17044 MB |   17103 MB |   84883 GB |   84866 GB |
|       from small pool |      39 MB |      73 MB |    1921 GB |    1921 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17084 MB |   17143 MB |   86804 GB |   86788 GB |
|       from large pool |   17044 MB |   17103 MB |   84883 GB |   84866 GB |
|       from small pool |      39 MB |      73 MB |    1921 GB |    1921 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22144 MB |   22302 MB |  354218 MB |  332074 MB |
|       from large pool |   22100 MB |   22100 MB |  347284 MB |  325184 MB |
|       from small pool |      44 MB |     202 MB |    6934 MB |    6890 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5059 MB |    5348 MB |  111878 GB |  111873 GB |
|       from large pool |    5055 MB |    5343 MB |  109610 GB |  109605 GB |
|       from small pool |       4 MB |       7 MB |    2268 GB |    2268 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   22784 K  |   22783 K  |
|       from large pool |     271    |     275    |   10784 K  |   10783 K  |
|       from small pool |     321    |     466    |   12000 K  |   11999 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   22784 K  |   22783 K  |
|       from large pool |     271    |     275    |   10784 K  |   10783 K  |
|       from small pool |     321    |     466    |   12000 K  |   11999 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     122    |    3631    |    3588    |
|       from large pool |      21    |      21    |     164    |     143    |
|       from small pool |      22    |     101    |    3467    |    3445    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      28    |   11540 K  |   11540 K  |
|       from large pool |      13    |      13    |    6067 K  |    6067 K  |
|       from small pool |      10    |      18    |    5472 K  |    5472 K  |
|===========================================================================|

2022-05-28 12:03:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:03:35 | INFO | train_inner | epoch 003:   2264 / 6032 loss=5.914, nll_loss=4.383, ppl=20.87, wps=34601.1, ups=12.42, wpb=2786.8, bsz=128, num_updates=14300, lr=0.00037022, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=20.2, wall=1155
2022-05-28 12:03:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 12.98 GiB already allocated; 419.69 MiB free; 21.62 GiB reserved in total by PyTorch)
2022-05-28 12:03:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   13333 MB |   87927 GB |   87914 GB |
|       from large pool |   13250 MB |   13293 MB |   85982 GB |   85969 GB |
|       from small pool |      39 MB |      73 MB |    1944 GB |    1944 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   13333 MB |   87927 GB |   87914 GB |
|       from large pool |   13250 MB |   13293 MB |   85982 GB |   85969 GB |
|       from small pool |      39 MB |      73 MB |    1944 GB |    1944 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22140 MB |   22298 MB |  354372 MB |  332232 MB |
|       from large pool |   22100 MB |   22100 MB |  347284 MB |  325184 MB |
|       from small pool |      40 MB |     198 MB |    7088 MB |    7048 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    8850 MB |    8850 MB |  113670 GB |  113662 GB |
|       from large pool |    8849 MB |    8849 MB |  111375 GB |  111366 GB |
|       from small pool |       0 MB |       6 MB |    2295 GB |    2295 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   23070 K  |   23069 K  |
|       from large pool |     271    |     275    |   10920 K  |   10920 K  |
|       from small pool |     321    |     466    |   12149 K  |   12148 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   23070 K  |   23069 K  |
|       from large pool |     271    |     275    |   10920 K  |   10920 K  |
|       from small pool |     321    |     466    |   12149 K  |   12148 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      41    |     120    |    3708    |    3667    |
|       from large pool |      21    |      21    |     164    |     143    |
|       from small pool |      20    |      99    |    3544    |    3524    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      34    |   11683 K  |   11683 K  |
|       from large pool |      15    |      15    |    6143 K  |    6143 K  |
|       from small pool |       8    |      21    |    5540 K  |    5540 K  |
|===========================================================================|

2022-05-28 12:03:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:03:43 | INFO | train_inner | epoch 003:   2365 / 6032 loss=5.886, nll_loss=4.351, ppl=20.41, wps=34978.6, ups=12.92, wpb=2706.5, bsz=128, num_updates=14400, lr=0.000368932, gnorm=1.005, loss_scale=16, train_wall=7, gb_free=20.8, wall=1162
2022-05-28 12:03:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 417.69 MiB free; 21.62 GiB reserved in total by PyTorch)
2022-05-28 12:03:44 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 65        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12702 MB |   16633 MB |   88079 GB |   88066 GB |
|       from large pool |   12663 MB |   16594 MB |   86130 GB |   86117 GB |
|       from small pool |      39 MB |      73 MB |    1949 GB |    1949 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12702 MB |   16633 MB |   88079 GB |   88066 GB |
|       from large pool |   12663 MB |   16594 MB |   86130 GB |   86117 GB |
|       from small pool |      39 MB |      73 MB |    1949 GB |    1949 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22142 MB |   22302 MB |  354534 MB |  332392 MB |
|       from large pool |   22100 MB |   22100 MB |  347284 MB |  325184 MB |
|       from small pool |      42 MB |     202 MB |    7250 MB |    7208 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1983 MB |    5508 MB |  113907 GB |  113905 GB |
|       from large pool |    1980 MB |    5505 MB |  111606 GB |  111604 GB |
|       from small pool |       2 MB |       7 MB |    2300 GB |    2300 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   23107 K  |   23106 K  |
|       from large pool |     271    |     275    |   10936 K  |   10936 K  |
|       from small pool |     321    |     466    |   12171 K  |   12170 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   23107 K  |   23106 K  |
|       from large pool |     271    |     275    |   10936 K  |   10936 K  |
|       from small pool |     321    |     466    |   12171 K  |   12170 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      42    |     122    |    3789    |    3747    |
|       from large pool |      21    |      21    |     164    |     143    |
|       from small pool |      21    |     101    |    3625    |    3604    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |   11702 K  |   11702 K  |
|       from large pool |      10    |      15    |    6151 K  |    6151 K  |
|       from small pool |       9    |      18    |    5550 K  |    5550 K  |
|===========================================================================|

2022-05-28 12:03:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:03:51 | INFO | train_inner | epoch 003:   2466 / 6032 loss=5.814, nll_loss=4.269, ppl=19.28, wps=33602.5, ups=13.1, wpb=2564.3, bsz=128, num_updates=14500, lr=0.000367658, gnorm=1.015, loss_scale=16, train_wall=7, gb_free=20.7, wall=1170
2022-05-28 12:03:58 | INFO | train_inner | epoch 003:   2566 / 6032 loss=5.941, nll_loss=4.415, ppl=21.33, wps=35679, ups=13.22, wpb=2699.6, bsz=128, num_updates=14600, lr=0.000366397, gnorm=0.984, loss_scale=16, train_wall=7, gb_free=20.1, wall=1178
2022-05-28 12:04:06 | INFO | train_inner | epoch 003:   2666 / 6032 loss=6.121, nll_loss=4.62, ppl=24.59, wps=37458.7, ups=12.5, wpb=2996.8, bsz=128, num_updates=14700, lr=0.000365148, gnorm=1.033, loss_scale=16, train_wall=8, gb_free=21, wall=1186
2022-05-28 12:04:14 | INFO | train_inner | epoch 003:   2766 / 6032 loss=5.777, nll_loss=4.227, ppl=18.73, wps=34116.1, ups=13.38, wpb=2548.8, bsz=128, num_updates=14800, lr=0.000363913, gnorm=0.982, loss_scale=16, train_wall=7, gb_free=20, wall=1193
2022-05-28 12:04:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 13.50 GiB already allocated; 2.18 GiB free; 19.85 GiB reserved in total by PyTorch)
2022-05-28 12:04:21 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13821 MB |   13867 MB |   90900 GB |   90887 GB |
|       from large pool |   13781 MB |   13827 MB |   88886 GB |   88872 GB |
|       from small pool |      39 MB |      73 MB |    2014 GB |    2014 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13821 MB |   13867 MB |   90900 GB |   90887 GB |
|       from large pool |   13781 MB |   13827 MB |   88886 GB |   88872 GB |
|       from small pool |      39 MB |      73 MB |    2014 GB |    2014 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20328 MB |   20488 MB |  360336 MB |  340008 MB |
|       from large pool |   20286 MB |   20286 MB |  352926 MB |  332640 MB |
|       from small pool |      42 MB |     202 MB |    7410 MB |    7368 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6506 MB |    6506 MB |  118026 GB |  118020 GB |
|       from large pool |    6504 MB |    6504 MB |  115649 GB |  115642 GB |
|       from small pool |       2 MB |      25 MB |    2377 GB |    2377 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   23858 K  |   23858 K  |
|       from large pool |     271    |     275    |   11289 K  |   11288 K  |
|       from small pool |     321    |     466    |   12569 K  |   12569 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   23858 K  |   23858 K  |
|       from large pool |     271    |     275    |   11289 K  |   11288 K  |
|       from small pool |     321    |     466    |   12569 K  |   12569 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     123    |    3871    |    3828    |
|       from large pool |      22    |      22    |     166    |     144    |
|       from small pool |      21    |     101    |    3705    |    3684    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      42    |   12078 K  |   12078 K  |
|       from large pool |      18    |      18    |    6345 K  |    6345 K  |
|       from small pool |       8    |      35    |    5732 K  |    5732 K  |
|===========================================================================|

2022-05-28 12:04:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:04:21 | INFO | train_inner | epoch 003:   2867 / 6032 loss=5.801, nll_loss=4.256, ppl=19.11, wps=34458.9, ups=13.13, wpb=2623.9, bsz=128, num_updates=14900, lr=0.000362689, gnorm=0.944, loss_scale=16, train_wall=7, gb_free=20.2, wall=1201
2022-05-28 12:04:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.46 GiB (GPU 0; 23.70 GiB total capacity; 15.28 GiB already allocated; 2.18 GiB free; 19.86 GiB reserved in total by PyTorch)
2022-05-28 12:04:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12107 MB |   15647 MB |   91296 GB |   91285 GB |
|       from large pool |   12068 MB |   15608 MB |   89272 GB |   89260 GB |
|       from small pool |      39 MB |      73 MB |    2024 GB |    2024 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12107 MB |   15647 MB |   91296 GB |   91285 GB |
|       from large pool |   12068 MB |   15608 MB |   89272 GB |   89260 GB |
|       from small pool |      39 MB |      73 MB |    2024 GB |    2024 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20332 MB |   20482 MB |  360490 MB |  340158 MB |
|       from large pool |   20286 MB |   20286 MB |  352926 MB |  332640 MB |
|       from small pool |      46 MB |     196 MB |    7564 MB |    7518 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2804 MB |    4684 MB |  118532 GB |  118529 GB |
|       from large pool |    2797 MB |    4677 MB |  116142 GB |  116139 GB |
|       from small pool |       6 MB |      18 MB |    2389 GB |    2389 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   23972 K  |   23971 K  |
|       from large pool |     271    |     275    |   11341 K  |   11341 K  |
|       from small pool |     321    |     466    |   12630 K  |   12630 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   23972 K  |   23971 K  |
|       from large pool |     271    |     275    |   11341 K  |   11341 K  |
|       from small pool |     321    |     466    |   12630 K  |   12630 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     120    |    3948    |    3903    |
|       from large pool |      22    |      22    |     166    |     144    |
|       from small pool |      23    |      98    |    3782    |    3759    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      59    |   12135 K  |   12135 K  |
|       from large pool |      16    |      17    |    6374 K  |    6374 K  |
|       from small pool |      13    |      48    |    5760 K  |    5760 K  |
|===========================================================================|

2022-05-28 12:04:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:04:29 | INFO | train_inner | epoch 003:   2968 / 6032 loss=5.827, nll_loss=4.284, ppl=19.48, wps=32869, ups=13.11, wpb=2506.9, bsz=128, num_updates=15000, lr=0.000361478, gnorm=0.996, loss_scale=16, train_wall=7, gb_free=20.2, wall=1208
2022-05-28 12:04:37 | INFO | train_inner | epoch 003:   3068 / 6032 loss=5.726, nll_loss=4.171, ppl=18.02, wps=35149.6, ups=13.26, wpb=2650.2, bsz=128, num_updates=15100, lr=0.00036028, gnorm=1.014, loss_scale=16, train_wall=7, gb_free=20.4, wall=1216
2022-05-28 12:04:44 | INFO | train_inner | epoch 003:   3168 / 6032 loss=5.868, nll_loss=4.331, ppl=20.13, wps=35946.1, ups=13.13, wpb=2738.3, bsz=128, num_updates=15200, lr=0.000359092, gnorm=0.981, loss_scale=16, train_wall=7, gb_free=18.7, wall=1223
2022-05-28 12:04:52 | INFO | train_inner | epoch 003:   3268 / 6032 loss=5.832, nll_loss=4.294, ppl=19.61, wps=33003.2, ups=13.18, wpb=2503.9, bsz=128, num_updates=15300, lr=0.000357917, gnorm=1.04, loss_scale=16, train_wall=7, gb_free=20.3, wall=1231
2022-05-28 12:05:00 | INFO | train_inner | epoch 003:   3368 / 6032 loss=5.986, nll_loss=4.468, ppl=22.12, wps=35457.9, ups=12.66, wpb=2800.1, bsz=126.8, num_updates=15400, lr=0.000356753, gnorm=1.044, loss_scale=16, train_wall=8, gb_free=19.8, wall=1239
2022-05-28 12:05:07 | INFO | train_inner | epoch 003:   3468 / 6032 loss=5.895, nll_loss=4.364, ppl=20.59, wps=35136.1, ups=13.13, wpb=2676.8, bsz=128, num_updates=15500, lr=0.0003556, gnorm=1.038, loss_scale=16, train_wall=7, gb_free=20.2, wall=1247
2022-05-28 12:05:15 | INFO | train_inner | epoch 003:   3568 / 6032 loss=5.683, nll_loss=4.123, ppl=17.42, wps=32911.5, ups=13.56, wpb=2427.9, bsz=128, num_updates=15600, lr=0.000354459, gnorm=1.035, loss_scale=16, train_wall=7, gb_free=19.9, wall=1254
2022-05-28 12:05:22 | INFO | train_inner | epoch 003:   3668 / 6032 loss=5.878, nll_loss=4.347, ppl=20.35, wps=33475.3, ups=12.98, wpb=2578.4, bsz=128, num_updates=15700, lr=0.000353328, gnorm=1.004, loss_scale=16, train_wall=8, gb_free=20.2, wall=1262
2022-05-28 12:05:30 | INFO | train_inner | epoch 003:   3768 / 6032 loss=5.821, nll_loss=4.28, ppl=19.43, wps=35437.3, ups=13.37, wpb=2651.2, bsz=128, num_updates=15800, lr=0.000352208, gnorm=0.954, loss_scale=16, train_wall=7, gb_free=19.6, wall=1269
2022-05-28 12:05:38 | INFO | train_inner | epoch 003:   3868 / 6032 loss=5.815, nll_loss=4.273, ppl=19.33, wps=35101.7, ups=13.17, wpb=2665.5, bsz=128, num_updates=15900, lr=0.000351099, gnorm=0.983, loss_scale=16, train_wall=7, gb_free=19.3, wall=1277
2022-05-28 12:05:45 | INFO | train_inner | epoch 003:   3968 / 6032 loss=5.694, nll_loss=4.137, ppl=17.6, wps=32883.5, ups=13.63, wpb=2412, bsz=128, num_updates=16000, lr=0.00035, gnorm=0.99, loss_scale=16, train_wall=7, gb_free=21, wall=1284
2022-05-28 12:05:52 | INFO | train_inner | epoch 003:   4068 / 6032 loss=5.84, nll_loss=4.302, ppl=19.73, wps=35511.9, ups=13.19, wpb=2692.1, bsz=128, num_updates=16100, lr=0.000348911, gnorm=1.013, loss_scale=16, train_wall=7, gb_free=20.1, wall=1292
2022-05-28 12:05:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.53 GiB already allocated; 3.59 GiB free; 18.45 GiB reserved in total by PyTorch)
2022-05-28 12:05:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12951 MB |   16931 MB |   97930 GB |   97918 GB |
|       from large pool |   12912 MB |   16892 MB |   95746 GB |   95733 GB |
|       from small pool |      39 MB |      73 MB |    2184 GB |    2184 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12951 MB |   16931 MB |   97930 GB |   97918 GB |
|       from large pool |   12912 MB |   16892 MB |   95746 GB |   95733 GB |
|       from small pool |      39 MB |      73 MB |    2184 GB |    2184 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18888 MB |   22200 MB |  371758 MB |  352870 MB |
|       from large pool |   18846 MB |   21998 MB |  364038 MB |  345192 MB |
|       from small pool |      42 MB |     202 MB |    7720 MB |    7678 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1956 MB |    3029 MB |  126948 GB |  126946 GB |
|       from large pool |    1953 MB |    3026 MB |  124370 GB |  124368 GB |
|       from small pool |       2 MB |       6 MB |    2577 GB |    2577 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   25763 K  |   25762 K  |
|       from large pool |     271    |     275    |   12171 K  |   12170 K  |
|       from small pool |     321    |     466    |   13591 K  |   13591 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   25763 K  |   25762 K  |
|       from large pool |     271    |     275    |   12171 K  |   12170 K  |
|       from small pool |     321    |     466    |   13591 K  |   13591 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     124    |    4029    |    3986    |
|       from large pool |      22    |      23    |     169    |     147    |
|       from small pool |      21    |     101    |    3860    |    3839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      32    |   13036 K  |   13036 K  |
|       from large pool |      17    |      17    |    6834 K  |    6833 K  |
|       from small pool |       8    |      18    |    6202 K  |    6202 K  |
|===========================================================================|

2022-05-28 12:05:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:06:00 | INFO | train_inner | epoch 003:   4169 / 6032 loss=5.786, nll_loss=4.241, ppl=18.91, wps=34103.8, ups=12.63, wpb=2700.9, bsz=128, num_updates=16200, lr=0.000347833, gnorm=0.96, loss_scale=16, train_wall=7, gb_free=20.9, wall=1300
2022-05-28 12:06:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 3.63 GiB free; 18.40 GiB reserved in total by PyTorch)
2022-05-28 12:06:06 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 71        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13631 MB |   17562 MB |   98905 GB |   98892 GB |
|       from large pool |   13592 MB |   17522 MB |   96703 GB |   96690 GB |
|       from small pool |      39 MB |      73 MB |    2201 GB |    2201 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13631 MB |   17562 MB |   98905 GB |   98892 GB |
|       from large pool |   13592 MB |   17522 MB |   96703 GB |   96690 GB |
|       from small pool |      39 MB |      73 MB |    2201 GB |    2201 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18846 MB |   21410 MB |  382192 MB |  363346 MB |
|       from large pool |   18798 MB |   21214 MB |  374318 MB |  355520 MB |
|       from small pool |      48 MB |     196 MB |    7874 MB |    7826 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1282 MB |    3002 MB |  128189 GB |  128187 GB |
|       from large pool |    1273 MB |    2993 MB |  125590 GB |  125589 GB |
|       from small pool |       8 MB |      18 MB |    2598 GB |    2598 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   26005 K  |   26005 K  |
|       from large pool |     271    |     275    |   12290 K  |   12290 K  |
|       from small pool |     321    |     466    |   13715 K  |   13714 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   26005 K  |   26005 K  |
|       from large pool |     271    |     275    |   12290 K  |   12290 K  |
|       from small pool |     321    |     466    |   13715 K  |   13714 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     121    |    4109    |    4063    |
|       from large pool |      22    |      23    |     172    |     150    |
|       from small pool |      24    |      98    |    3937    |    3913    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      59    |   13157 K  |   13157 K  |
|       from large pool |      17    |      18    |    6899 K  |    6899 K  |
|       from small pool |      16    |      47    |    6257 K  |    6257 K  |
|===========================================================================|

2022-05-28 12:06:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:06:08 | INFO | train_inner | epoch 003:   4270 / 6032 loss=5.797, nll_loss=4.253, ppl=19.07, wps=35601.3, ups=12.43, wpb=2863.1, bsz=128, num_updates=16300, lr=0.000346764, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=18.6, wall=1308
2022-05-28 12:06:16 | INFO | train_inner | epoch 003:   4370 / 6032 loss=5.94, nll_loss=4.418, ppl=21.38, wps=35909.2, ups=13.24, wpb=2712.2, bsz=128, num_updates=16400, lr=0.000345705, gnorm=0.977, loss_scale=32, train_wall=7, gb_free=20.3, wall=1315
2022-05-28 12:06:24 | INFO | train_inner | epoch 003:   4470 / 6032 loss=5.828, nll_loss=4.289, ppl=19.55, wps=35567.3, ups=13.09, wpb=2716.4, bsz=128, num_updates=16500, lr=0.000344656, gnorm=0.978, loss_scale=32, train_wall=7, gb_free=18, wall=1323
2022-05-28 12:06:31 | INFO | train_inner | epoch 003:   4570 / 6032 loss=5.727, nll_loss=4.174, ppl=18.05, wps=35022.7, ups=13.16, wpb=2662.2, bsz=128, num_updates=16600, lr=0.000343616, gnorm=0.99, loss_scale=32, train_wall=7, gb_free=20.8, wall=1330
2022-05-28 12:06:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-05-28 12:06:39 | INFO | train_inner | epoch 003:   4671 / 6032 loss=5.704, nll_loss=4.149, ppl=17.74, wps=31537.1, ups=13.14, wpb=2400.2, bsz=128, num_updates=16700, lr=0.000342586, gnorm=0.999, loss_scale=16, train_wall=7, gb_free=20.6, wall=1338
2022-05-28 12:06:47 | INFO | train_inner | epoch 003:   4771 / 6032 loss=5.659, nll_loss=4.097, ppl=17.12, wps=35110.3, ups=12.9, wpb=2721.2, bsz=128, num_updates=16800, lr=0.000341565, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=20.4, wall=1346
2022-05-28 12:06:54 | INFO | train_inner | epoch 003:   4871 / 6032 loss=5.816, nll_loss=4.278, ppl=19.39, wps=34176.7, ups=12.96, wpb=2637.8, bsz=128, num_updates=16900, lr=0.000340553, gnorm=0.984, loss_scale=16, train_wall=8, gb_free=16.4, wall=1353
2022-05-28 12:07:02 | INFO | train_inner | epoch 003:   4971 / 6032 loss=5.609, nll_loss=4.04, ppl=16.45, wps=35107.2, ups=13.43, wpb=2614.4, bsz=128, num_updates=17000, lr=0.00033955, gnorm=0.957, loss_scale=16, train_wall=7, gb_free=20.4, wall=1361
2022-05-28 12:07:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 401.69 MiB free; 21.64 GiB reserved in total by PyTorch)
2022-05-28 12:07:06 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14034 MB |   18404 MB |  103428 GB |  103414 GB |
|       from large pool |   13995 MB |   18365 MB |  101124 GB |  101110 GB |
|       from small pool |      39 MB |      73 MB |    2303 GB |    2303 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14034 MB |   18404 MB |  103428 GB |  103414 GB |
|       from large pool |   13995 MB |   18365 MB |  101124 GB |  101110 GB |
|       from small pool |      39 MB |      73 MB |    2303 GB |    2303 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22158 MB |   22158 MB |  392578 MB |  370420 MB |
|       from large pool |   22118 MB |   22118 MB |  384550 MB |  362432 MB |
|       from small pool |      40 MB |     202 MB |    8028 MB |    7988 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3751 MB |    3753 MB |  133886 GB |  133882 GB |
|       from large pool |    3750 MB |    3752 MB |  131167 GB |  131163 GB |
|       from small pool |       0 MB |      13 MB |    2719 GB |    2719 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   27214 K  |   27213 K  |
|       from large pool |     271    |     275    |   12860 K  |   12860 K  |
|       from small pool |     321    |     466    |   14353 K  |   14353 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   27214 K  |   27213 K  |
|       from large pool |     271    |     275    |   12860 K  |   12860 K  |
|       from small pool |     321    |     466    |   14353 K  |   14353 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     124    |    4189    |    4146    |
|       from large pool |      23    |      23    |     175    |     152    |
|       from small pool |      20    |     101    |    4014    |    3994    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      59    |   13765 K  |   13765 K  |
|       from large pool |      15    |      16    |    7214 K  |    7214 K  |
|       from small pool |       7    |      47    |    6550 K  |    6550 K  |
|===========================================================================|

2022-05-28 12:07:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:07:10 | INFO | train_inner | epoch 003:   5072 / 6032 loss=5.744, nll_loss=4.194, ppl=18.31, wps=33867.8, ups=12.62, wpb=2684.3, bsz=128, num_updates=17100, lr=0.000338556, gnorm=1.013, loss_scale=16, train_wall=8, gb_free=19.1, wall=1369
2022-05-28 12:07:17 | INFO | train_inner | epoch 003:   5172 / 6032 loss=5.74, nll_loss=4.193, ppl=18.28, wps=33977, ups=13.59, wpb=2500.8, bsz=128, num_updates=17200, lr=0.00033757, gnorm=0.977, loss_scale=16, train_wall=7, gb_free=19.9, wall=1376
2022-05-28 12:07:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 3.30 GiB free; 18.73 GiB reserved in total by PyTorch)
2022-05-28 12:07:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13956 MB |   18279 MB |  104740 GB |  104726 GB |
|       from large pool |   13917 MB |   18239 MB |  102407 GB |  102393 GB |
|       from small pool |      39 MB |      73 MB |    2332 GB |    2332 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13956 MB |   18279 MB |  104740 GB |  104726 GB |
|       from large pool |   13917 MB |   18239 MB |  102407 GB |  102393 GB |
|       from small pool |      39 MB |      73 MB |    2332 GB |    2332 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19180 MB |   21166 MB |  403370 MB |  384190 MB |
|       from large pool |   19138 MB |   21020 MB |  395076 MB |  375938 MB |
|       from small pool |      42 MB |     146 MB |    8294 MB |    8252 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     901 MB |    2952 MB |  135533 GB |  135532 GB |
|       from large pool |     898 MB |    2949 MB |  132780 GB |  132779 GB |
|       from small pool |       2 MB |      15 MB |    2753 GB |    2753 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   27574 K  |   27573 K  |
|       from large pool |     271    |     275    |   13031 K  |   13030 K  |
|       from small pool |     321    |     466    |   14543 K  |   14542 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   27574 K  |   27573 K  |
|       from large pool |     271    |     275    |   13031 K  |   13030 K  |
|       from small pool |     321    |     466    |   14543 K  |   14542 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |      96    |    4325    |    4282    |
|       from large pool |      22    |      23    |     178    |     156    |
|       from small pool |      21    |      73    |    4147    |    4126    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      54    |   13945 K  |   13945 K  |
|       from large pool |      14    |      16    |    7309 K  |    7309 K  |
|       from small pool |       5    |      39    |    6636 K  |    6636 K  |
|===========================================================================|

2022-05-28 12:07:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:07:25 | INFO | train_inner | epoch 003:   5273 / 6032 loss=5.666, nll_loss=4.106, ppl=17.22, wps=31417, ups=12.68, wpb=2476.8, bsz=128, num_updates=17300, lr=0.000336593, gnorm=0.98, loss_scale=16, train_wall=7, gb_free=19.7, wall=1384
2022-05-28 12:07:33 | INFO | train_inner | epoch 003:   5373 / 6032 loss=5.754, nll_loss=4.206, ppl=18.46, wps=35027.6, ups=13.04, wpb=2686.5, bsz=128, num_updates=17400, lr=0.000335624, gnorm=0.975, loss_scale=16, train_wall=7, gb_free=20.9, wall=1392
2022-05-28 12:07:41 | INFO | train_inner | epoch 003:   5473 / 6032 loss=5.793, nll_loss=4.253, ppl=19.06, wps=31505.2, ups=12.37, wpb=2545.9, bsz=128, num_updates=17500, lr=0.000334664, gnorm=1.025, loss_scale=16, train_wall=8, gb_free=17.4, wall=1400
2022-05-28 12:07:48 | INFO | train_inner | epoch 003:   5573 / 6032 loss=5.639, nll_loss=4.078, ppl=16.88, wps=34363, ups=13.45, wpb=2555.5, bsz=128, num_updates=17600, lr=0.000333712, gnorm=0.975, loss_scale=16, train_wall=7, gb_free=20.4, wall=1407
2022-05-28 12:07:56 | INFO | train_inner | epoch 003:   5673 / 6032 loss=5.732, nll_loss=4.182, ppl=18.16, wps=36542.6, ups=13.01, wpb=2809.3, bsz=128, num_updates=17700, lr=0.000332768, gnorm=1.018, loss_scale=16, train_wall=8, gb_free=20.4, wall=1415
2022-05-28 12:08:03 | INFO | train_inner | epoch 003:   5773 / 6032 loss=5.643, nll_loss=4.08, ppl=16.92, wps=34677.8, ups=13.51, wpb=2566.4, bsz=128, num_updates=17800, lr=0.000331832, gnorm=0.938, loss_scale=16, train_wall=7, gb_free=20.2, wall=1422
2022-05-28 12:08:11 | INFO | train_inner | epoch 003:   5873 / 6032 loss=5.811, nll_loss=4.273, ppl=19.33, wps=35010.2, ups=12.92, wpb=2709.5, bsz=128, num_updates=17900, lr=0.000330904, gnorm=0.999, loss_scale=16, train_wall=8, gb_free=20, wall=1430
2022-05-28 12:08:18 | INFO | train_inner | epoch 003:   5973 / 6032 loss=5.787, nll_loss=4.246, ppl=18.97, wps=35737.5, ups=13.24, wpb=2698.9, bsz=128, num_updates=18000, lr=0.000329983, gnorm=0.966, loss_scale=16, train_wall=7, gb_free=20.1, wall=1438
2022-05-28 12:08:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:08:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.716 | nll_loss 4.129 | ppl 17.5 | wps 109314 | wpb 2687.4 | bsz 128 | num_updates 18059 | best_loss 5.716
2022-05-28 12:08:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 18059 updates
2022-05-28 12:08:42 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint3.pt
2022-05-28 12:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint3.pt
2022-05-28 12:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint3.pt (epoch 3 @ 18059 updates, score 5.716) (writing took 9.929627913981676 seconds)
2022-05-28 12:08:52 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-05-28 12:08:52 | INFO | train | epoch 003 | loss 5.853 | nll_loss 4.315 | ppl 19.91 | wps 32820.5 | ups 12.28 | wpb 2673.4 | bsz 128 | num_updates 18059 | lr 0.000329444 | gnorm 0.998 | loss_scale 16 | train_wall 448 | gb_free 20.6 | wall 1471
2022-05-28 12:08:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:08:52 | INFO | fairseq.trainer | begin training epoch 4
2022-05-28 12:08:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:08:55 | INFO | train_inner | epoch 004:     41 / 6032 loss=5.943, nll_loss=4.42, ppl=21.41, wps=8565, ups=2.7, wpb=3167.3, bsz=128, num_updates=18100, lr=0.00032907, gnorm=0.938, loss_scale=16, train_wall=8, gb_free=19.7, wall=1475
2022-05-28 12:09:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 3.36 GiB free; 18.67 GiB reserved in total by PyTorch)
2022-05-28 12:09:01 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15209 MB |   15260 MB |  111376 GB |  111361 GB |
|       from large pool |   15169 MB |   15220 MB |  108900 GB |  108886 GB |
|       from small pool |      39 MB |      73 MB |    2475 GB |    2475 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15209 MB |   15260 MB |  111376 GB |  111361 GB |
|       from large pool |   15169 MB |   15220 MB |  108900 GB |  108886 GB |
|       from small pool |      39 MB |      73 MB |    2475 GB |    2475 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19120 MB |   19268 MB |  434056 MB |  414936 MB |
|       from large pool |   19072 MB |   19072 MB |  425172 MB |  406100 MB |
|       from small pool |      48 MB |     196 MB |    8884 MB |    8836 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3910 MB |    5131 MB |  144181 GB |  144177 GB |
|       from large pool |    3902 MB |    5122 MB |  141262 GB |  141258 GB |
|       from small pool |       8 MB |      17 MB |    2919 GB |    2919 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   29281 K  |   29280 K  |
|       from large pool |     271    |     275    |   13856 K  |   13856 K  |
|       from small pool |     321    |     466    |   15424 K  |   15424 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   29281 K  |   29280 K  |
|       from large pool |     271    |     275    |   13856 K  |   13856 K  |
|       from small pool |     321    |     466    |   15424 K  |   15424 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     117    |    4628    |    4585    |
|       from large pool |      19    |      19    |     186    |     167    |
|       from small pool |      24    |      98    |    4442    |    4418    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      68    |   14791 K  |   14791 K  |
|       from large pool |      11    |      20    |    7768 K  |    7768 K  |
|       from small pool |      13    |      48    |    7023 K  |    7023 K  |
|===========================================================================|

2022-05-28 12:09:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:09:03 | INFO | train_inner | epoch 004:    142 / 6032 loss=5.56, nll_loss=3.979, ppl=15.77, wps=33302.4, ups=12.54, wpb=2655.2, bsz=128, num_updates=18200, lr=0.000328165, gnorm=0.948, loss_scale=16, train_wall=8, gb_free=20.6, wall=1483
2022-05-28 12:09:11 | INFO | train_inner | epoch 004:    242 / 6032 loss=5.49, nll_loss=3.899, ppl=14.92, wps=34786.7, ups=13.02, wpb=2670.8, bsz=128, num_updates=18300, lr=0.000327267, gnorm=0.922, loss_scale=16, train_wall=7, gb_free=18.5, wall=1490
2022-05-28 12:09:19 | INFO | train_inner | epoch 004:    342 / 6032 loss=5.425, nll_loss=3.825, ppl=14.18, wps=31834.9, ups=12.47, wpb=2552.1, bsz=128, num_updates=18400, lr=0.000326377, gnorm=0.938, loss_scale=16, train_wall=8, gb_free=20.5, wall=1498
2022-05-28 12:09:27 | INFO | train_inner | epoch 004:    442 / 6032 loss=5.494, nll_loss=3.902, ppl=14.95, wps=32112.2, ups=12.87, wpb=2495.3, bsz=128, num_updates=18500, lr=0.000325493, gnorm=0.962, loss_scale=16, train_wall=8, gb_free=17.2, wall=1506
2022-05-28 12:09:35 | INFO | train_inner | epoch 004:    542 / 6032 loss=5.491, nll_loss=3.898, ppl=14.91, wps=34204.3, ups=12.5, wpb=2736.9, bsz=128, num_updates=18600, lr=0.000324617, gnorm=0.912, loss_scale=16, train_wall=8, gb_free=20, wall=1514
2022-05-28 12:09:43 | INFO | train_inner | epoch 004:    642 / 6032 loss=5.647, nll_loss=4.078, ppl=16.89, wps=35244.5, ups=12.35, wpb=2854, bsz=128, num_updates=18700, lr=0.000323748, gnorm=0.94, loss_scale=16, train_wall=8, gb_free=17.3, wall=1522
2022-05-28 12:09:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 13.63 GiB already allocated; 3.37 GiB free; 18.66 GiB reserved in total by PyTorch)
2022-05-28 12:09:47 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13957 MB |   14005 MB |  114812 GB |  114799 GB |
|       from large pool |   13918 MB |   13965 MB |  112261 GB |  112247 GB |
|       from small pool |      39 MB |      73 MB |    2551 GB |    2551 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13957 MB |   14005 MB |  114812 GB |  114799 GB |
|       from large pool |   13918 MB |   13965 MB |  112261 GB |  112247 GB |
|       from small pool |      39 MB |      73 MB |    2551 GB |    2551 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19112 MB |   19274 MB |  434210 MB |  415098 MB |
|       from large pool |   19072 MB |   19072 MB |  425172 MB |  406100 MB |
|       from small pool |      40 MB |     202 MB |    9038 MB |    8998 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5154 MB |    5154 MB |  150587 GB |  150582 GB |
|       from large pool |    5153 MB |    5153 MB |  147578 GB |  147573 GB |
|       from small pool |       0 MB |      12 MB |    3008 GB |    3008 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   30194 K  |   30194 K  |
|       from large pool |     271    |     275    |   14289 K  |   14289 K  |
|       from small pool |     321    |     466    |   15905 K  |   15905 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   30194 K  |   30194 K  |
|       from large pool |     271    |     275    |   14289 K  |   14289 K  |
|       from small pool |     321    |     466    |   15905 K  |   15905 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      39    |     120    |    4705    |    4666    |
|       from large pool |      19    |      19    |     186    |     167    |
|       from small pool |      20    |     101    |    4519    |    4499    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      18    |      51    |   15250 K  |   15250 K  |
|       from large pool |      11    |      13    |    8007 K  |    8007 K  |
|       from small pool |       7    |      39    |    7243 K  |    7243 K  |
|===========================================================================|

2022-05-28 12:09:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:09:51 | INFO | train_inner | epoch 004:    743 / 6032 loss=5.553, nll_loss=3.97, ppl=15.67, wps=35885.7, ups=12.59, wpb=2851.4, bsz=128, num_updates=18800, lr=0.000322886, gnorm=0.908, loss_scale=16, train_wall=8, gb_free=19.3, wall=1530
2022-05-28 12:09:58 | INFO | train_inner | epoch 004:    843 / 6032 loss=5.489, nll_loss=3.899, ppl=14.91, wps=32724.1, ups=13.31, wpb=2458.8, bsz=128, num_updates=18900, lr=0.000322031, gnorm=0.989, loss_scale=16, train_wall=7, gb_free=16, wall=1538
2022-05-28 12:10:06 | INFO | train_inner | epoch 004:    943 / 6032 loss=5.369, nll_loss=3.762, ppl=13.57, wps=32480.8, ups=13.46, wpb=2412.9, bsz=128, num_updates=19000, lr=0.000321182, gnorm=1.009, loss_scale=16, train_wall=7, gb_free=19.1, wall=1545
2022-05-28 12:10:14 | INFO | train_inner | epoch 004:   1043 / 6032 loss=5.587, nll_loss=4.009, ppl=16.1, wps=33298, ups=11.68, wpb=2850.5, bsz=128, num_updates=19100, lr=0.00032034, gnorm=0.95, loss_scale=16, train_wall=8, gb_free=19.2, wall=1554
2022-05-28 12:10:22 | INFO | train_inner | epoch 004:   1143 / 6032 loss=5.503, nll_loss=3.914, ppl=15.08, wps=33719.4, ups=12.49, wpb=2698.9, bsz=128, num_updates=19200, lr=0.000319505, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=18.3, wall=1562
2022-05-28 12:10:30 | INFO | train_inner | epoch 004:   1243 / 6032 loss=5.559, nll_loss=3.978, ppl=15.76, wps=34720.5, ups=12.71, wpb=2732.1, bsz=128, num_updates=19300, lr=0.000318676, gnorm=0.939, loss_scale=16, train_wall=8, gb_free=20.7, wall=1569
2022-05-28 12:10:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 1.91 GiB free; 20.12 GiB reserved in total by PyTorch)
2022-05-28 12:10:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14033 MB |   18403 MB |  118283 GB |  118269 GB |
|       from large pool |   13993 MB |   18363 MB |  115653 GB |  115639 GB |
|       from small pool |      39 MB |      73 MB |    2630 GB |    2630 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14033 MB |   18403 MB |  118283 GB |  118269 GB |
|       from large pool |   13993 MB |   18363 MB |  115653 GB |  115639 GB |
|       from small pool |      39 MB |      73 MB |    2630 GB |    2630 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20604 MB |   20604 MB |  480168 MB |  459564 MB |
|       from large pool |   20556 MB |   20556 MB |  470656 MB |  450100 MB |
|       from small pool |      48 MB |     202 MB |    9512 MB |    9464 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2198 MB |    2200 MB |  155563 GB |  155560 GB |
|       from large pool |    2190 MB |    2192 MB |  152461 GB |  152459 GB |
|       from small pool |       8 MB |      15 MB |    3101 GB |    3101 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   31103 K  |   31102 K  |
|       from large pool |     271    |     275    |   14715 K  |   14715 K  |
|       from small pool |     321    |     466    |   16387 K  |   16387 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   31103 K  |   31102 K  |
|       from large pool |     271    |     275    |   14715 K  |   14715 K  |
|       from small pool |     321    |     466    |   16387 K  |   16387 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     128    |    4979    |    4929    |
|       from large pool |      26    |      27    |     223    |     197    |
|       from small pool |      24    |     101    |    4756    |    4732    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      52    |   15708 K  |   15708 K  |
|       from large pool |      19    |      20    |    8243 K  |    8243 K  |
|       from small pool |      10    |      36    |    7464 K  |    7464 K  |
|===========================================================================|

2022-05-28 12:10:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:10:38 | INFO | train_inner | epoch 004:   1344 / 6032 loss=5.593, nll_loss=4.017, ppl=16.19, wps=32736, ups=12.33, wpb=2655, bsz=128, num_updates=19400, lr=0.000317854, gnorm=0.929, loss_scale=16, train_wall=8, gb_free=19.8, wall=1578
2022-05-28 12:10:46 | INFO | train_inner | epoch 004:   1444 / 6032 loss=5.352, nll_loss=3.742, ppl=13.38, wps=33745.9, ups=13.22, wpb=2552.8, bsz=128, num_updates=19500, lr=0.000317038, gnorm=0.935, loss_scale=16, train_wall=7, gb_free=19.7, wall=1585
2022-05-28 12:10:54 | INFO | train_inner | epoch 004:   1544 / 6032 loss=5.522, nll_loss=3.937, ppl=15.31, wps=33076.8, ups=13.25, wpb=2495.7, bsz=128, num_updates=19600, lr=0.000316228, gnorm=0.958, loss_scale=16, train_wall=7, gb_free=20.2, wall=1593
2022-05-28 12:11:01 | INFO | train_inner | epoch 004:   1644 / 6032 loss=5.478, nll_loss=3.887, ppl=14.79, wps=33817.9, ups=12.62, wpb=2679.7, bsz=128, num_updates=19700, lr=0.000315424, gnorm=0.957, loss_scale=16, train_wall=8, gb_free=18.5, wall=1601
2022-05-28 12:11:09 | INFO | train_inner | epoch 004:   1744 / 6032 loss=5.302, nll_loss=3.688, ppl=12.89, wps=31765.8, ups=13.11, wpb=2423.2, bsz=128, num_updates=19800, lr=0.000314627, gnorm=0.947, loss_scale=16, train_wall=7, gb_free=19.5, wall=1608
2022-05-28 12:11:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 3.09 GiB free; 18.94 GiB reserved in total by PyTorch)
2022-05-28 12:11:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 88        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13822 MB |   17924 MB |  120948 GB |  120934 GB |
|       from large pool |   13782 MB |   17884 MB |  118253 GB |  118240 GB |
|       from small pool |      39 MB |      73 MB |    2694 GB |    2694 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13822 MB |   17924 MB |  120948 GB |  120934 GB |
|       from large pool |   13782 MB |   17884 MB |  118253 GB |  118240 GB |
|       from small pool |      39 MB |      73 MB |    2694 GB |    2694 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19398 MB |   19398 MB |  486646 MB |  467248 MB |
|       from large pool |   19358 MB |   19358 MB |  476980 MB |  457622 MB |
|       from small pool |      40 MB |     202 MB |    9666 MB |    9626 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1473 MB |    1473 MB |  158250 GB |  158248 GB |
|       from large pool |    1473 MB |    1473 MB |  155072 GB |  155070 GB |
|       from small pool |       0 MB |       8 MB |    3177 GB |    3177 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   31837 K  |   31836 K  |
|       from large pool |     271    |     275    |   15058 K  |   15058 K  |
|       from small pool |     321    |     466    |   16778 K  |   16778 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   31837 K  |   31836 K  |
|       from large pool |     271    |     275    |   15058 K  |   15058 K  |
|       from small pool |     321    |     466    |   16778 K  |   16778 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     127    |    5058    |    5012    |
|       from large pool |      26    |      26    |     225    |     199    |
|       from small pool |      20    |     101    |    4833    |    4813    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      33    |   16079 K  |   16079 K  |
|       from large pool |      20    |      20    |    8434 K  |    8434 K  |
|       from small pool |       7    |      21    |    7644 K  |    7644 K  |
|===========================================================================|

2022-05-28 12:11:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:11:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.45 GiB already allocated; 427.69 MiB free; 21.61 GiB reserved in total by PyTorch)
2022-05-28 12:11:14 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 90        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21963 MB |   22263 MB |  121276 GB |  121254 GB |
|       from large pool |   21920 MB |   22220 MB |  118575 GB |  118554 GB |
|       from small pool |      42 MB |      73 MB |    2700 GB |    2700 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21963 MB |   22263 MB |  121276 GB |  121254 GB |
|       from large pool |   21920 MB |   22220 MB |  118575 GB |  118554 GB |
|       from small pool |      42 MB |      73 MB |    2700 GB |    2700 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22132 MB |   22432 MB |  493934 MB |  471802 MB |
|       from large pool |   22088 MB |   22388 MB |  484112 MB |  462024 MB |
|       from small pool |      44 MB |     196 MB |    9822 MB |    9778 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  172989 KB |    4551 MB |  158585 GB |  158585 GB |
|       from large pool |  171572 KB |    4549 MB |  155400 GB |  155400 GB |
|       from small pool |    1417 KB |      18 MB |    3185 GB |    3185 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   31916 K  |   31916 K  |
|       from large pool |     214    |     215    |   15096 K  |   15096 K  |
|       from small pool |     302    |     466    |   16820 K  |   16819 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   31916 K  |   31916 K  |
|       from large pool |     214    |     215    |   15096 K  |   15096 K  |
|       from small pool |     302    |     466    |   16820 K  |   16819 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     162    |    5176    |    5090    |
|       from large pool |      64    |      65    |     265    |     201    |
|       from small pool |      22    |      98    |    4911    |    4889    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      64    |   16118 K  |   16118 K  |
|       from large pool |      33    |      33    |    8455 K  |    8455 K  |
|       from small pool |       9    |      49    |    7663 K  |    7663 K  |
|===========================================================================|

2022-05-28 12:11:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:11:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 3.71 GiB free; 18.32 GiB reserved in total by PyTorch)
2022-05-28 12:11:16 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 92        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   17220 MB |  121354 GB |  121341 GB |
|       from large pool |   13250 MB |   17180 MB |  118651 GB |  118638 GB |
|       from small pool |      39 MB |      73 MB |    2702 GB |    2702 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   17220 MB |  121354 GB |  121341 GB |
|       from large pool |   13250 MB |   17180 MB |  118651 GB |  118638 GB |
|       from small pool |      39 MB |      73 MB |    2702 GB |    2702 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18756 MB |   21680 MB |  503752 MB |  484996 MB |
|       from large pool |   18712 MB |   21520 MB |  493660 MB |  474948 MB |
|       from small pool |      44 MB |     160 MB |   10092 MB |   10048 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1534 MB |    2302 MB |  158662 GB |  158661 GB |
|       from large pool |    1529 MB |    2297 MB |  155475 GB |  155474 GB |
|       from small pool |       4 MB |      16 MB |    3187 GB |    3187 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   31931 K  |   31930 K  |
|       from large pool |     271    |     275    |   15103 K  |   15102 K  |
|       from small pool |     321    |     466    |   16828 K  |   16827 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   31931 K  |   31930 K  |
|       from large pool |     271    |     275    |   15103 K  |   15102 K  |
|       from small pool |     321    |     466    |   16828 K  |   16827 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |     131    |    5314    |    5242    |
|       from large pool |      50    |      51    |     268    |     218    |
|       from small pool |      22    |      80    |    5046    |    5024    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      61    |   16126 K  |   16126 K  |
|       from large pool |      40    |      41    |    8458 K  |    8458 K  |
|       from small pool |       9    |      47    |    7667 K  |    7667 K  |
|===========================================================================|

2022-05-28 12:11:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:11:19 | INFO | train_inner | epoch 004:   1847 / 6032 loss=5.548, nll_loss=3.966, ppl=15.62, wps=28449.9, ups=10.3, wpb=2762.5, bsz=128, num_updates=19900, lr=0.000313835, gnorm=0.964, loss_scale=16, train_wall=9, gb_free=16.1, wall=1618
2022-05-28 12:11:26 | INFO | train_inner | epoch 004:   1947 / 6032 loss=5.519, nll_loss=3.934, ppl=15.29, wps=34347.6, ups=13.03, wpb=2636.1, bsz=128, num_updates=20000, lr=0.00031305, gnorm=0.938, loss_scale=16, train_wall=7, gb_free=19.5, wall=1626
2022-05-28 12:11:34 | INFO | train_inner | epoch 004:   2047 / 6032 loss=5.562, nll_loss=3.982, ppl=15.8, wps=35507.4, ups=12.94, wpb=2743.9, bsz=128, num_updates=20100, lr=0.00031227, gnorm=0.943, loss_scale=16, train_wall=8, gb_free=20.3, wall=1633
2022-05-28 12:11:42 | INFO | train_inner | epoch 004:   2147 / 6032 loss=5.591, nll_loss=4.015, ppl=16.17, wps=33668.3, ups=12.31, wpb=2735.1, bsz=128, num_updates=20200, lr=0.000311496, gnorm=0.962, loss_scale=16, train_wall=8, gb_free=19.8, wall=1641
2022-05-28 12:11:50 | INFO | train_inner | epoch 004:   2247 / 6032 loss=5.477, nll_loss=3.887, ppl=14.79, wps=34227.8, ups=13.06, wpb=2620.7, bsz=128, num_updates=20300, lr=0.000310728, gnorm=0.926, loss_scale=16, train_wall=7, gb_free=19.7, wall=1649
2022-05-28 12:11:58 | INFO | train_inner | epoch 004:   2347 / 6032 loss=5.674, nll_loss=4.113, ppl=17.3, wps=33477.6, ups=11.99, wpb=2793, bsz=128, num_updates=20400, lr=0.000309965, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=18.7, wall=1657
2022-05-28 12:12:06 | INFO | train_inner | epoch 004:   2447 / 6032 loss=5.419, nll_loss=3.819, ppl=14.12, wps=33289.6, ups=12.85, wpb=2589.8, bsz=128, num_updates=20500, lr=0.000309208, gnorm=0.995, loss_scale=16, train_wall=8, gb_free=20.2, wall=1665
2022-05-28 12:12:14 | INFO | train_inner | epoch 004:   2547 / 6032 loss=5.62, nll_loss=4.049, ppl=16.55, wps=33724.9, ups=12.06, wpb=2797.4, bsz=128, num_updates=20600, lr=0.000308457, gnorm=0.95, loss_scale=16, train_wall=8, gb_free=18.9, wall=1674
2022-05-28 12:12:22 | INFO | train_inner | epoch 004:   2647 / 6032 loss=5.44, nll_loss=3.845, ppl=14.37, wps=32959.7, ups=13.29, wpb=2479.7, bsz=128, num_updates=20700, lr=0.000307711, gnorm=0.959, loss_scale=16, train_wall=7, gb_free=20.6, wall=1681
2022-05-28 12:12:30 | INFO | train_inner | epoch 004:   2747 / 6032 loss=5.68, nll_loss=4.119, ppl=17.38, wps=34887.2, ups=12.59, wpb=2770.7, bsz=128, num_updates=20800, lr=0.00030697, gnorm=0.98, loss_scale=16, train_wall=8, gb_free=19.2, wall=1689
2022-05-28 12:12:38 | INFO | train_inner | epoch 004:   2847 / 6032 loss=5.592, nll_loss=4.019, ppl=16.21, wps=34846.8, ups=13.04, wpb=2671.7, bsz=128, num_updates=20900, lr=0.000306235, gnorm=0.928, loss_scale=16, train_wall=7, gb_free=18.2, wall=1697
2022-05-28 12:12:45 | INFO | train_inner | epoch 004:   2947 / 6032 loss=5.576, nll_loss=3.999, ppl=15.99, wps=35107.1, ups=12.89, wpb=2722.7, bsz=128, num_updates=21000, lr=0.000305505, gnorm=0.947, loss_scale=16, train_wall=8, gb_free=19.3, wall=1704
2022-05-28 12:12:54 | INFO | train_inner | epoch 004:   3047 / 6032 loss=5.778, nll_loss=4.231, ppl=18.78, wps=35318.1, ups=11.32, wpb=3119.3, bsz=128, num_updates=21100, lr=0.00030478, gnorm=0.954, loss_scale=16, train_wall=9, gb_free=16.3, wall=1713
2022-05-28 12:13:02 | INFO | train_inner | epoch 004:   3147 / 6032 loss=5.59, nll_loss=4.018, ppl=16.21, wps=33120.4, ups=12.25, wpb=2704.5, bsz=126.8, num_updates=21200, lr=0.000304061, gnorm=0.951, loss_scale=16, train_wall=8, gb_free=20.6, wall=1721
2022-05-28 12:13:10 | INFO | train_inner | epoch 004:   3247 / 6032 loss=5.649, nll_loss=4.084, ppl=16.96, wps=35910.5, ups=12.5, wpb=2873.8, bsz=128, num_updates=21300, lr=0.000303346, gnorm=0.913, loss_scale=16, train_wall=8, gb_free=19.8, wall=1729
2022-05-28 12:13:18 | INFO | train_inner | epoch 004:   3347 / 6032 loss=5.601, nll_loss=4.029, ppl=16.33, wps=34219.5, ups=12.18, wpb=2808.5, bsz=128, num_updates=21400, lr=0.000302636, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=20.5, wall=1738
2022-05-28 12:13:26 | INFO | train_inner | epoch 004:   3447 / 6032 loss=5.456, nll_loss=3.864, ppl=14.56, wps=32437.8, ups=13.16, wpb=2464.7, bsz=128, num_updates=21500, lr=0.000301932, gnorm=0.998, loss_scale=16, train_wall=7, gb_free=19.2, wall=1745
2022-05-28 12:13:34 | INFO | train_inner | epoch 004:   3547 / 6032 loss=5.592, nll_loss=4.02, ppl=16.22, wps=33857.8, ups=12.83, wpb=2638.9, bsz=128, num_updates=21600, lr=0.000301232, gnorm=0.949, loss_scale=16, train_wall=8, gb_free=19.3, wall=1753
2022-05-28 12:13:42 | INFO | train_inner | epoch 004:   3647 / 6032 loss=5.499, nll_loss=3.914, ppl=15.07, wps=35470.6, ups=12.63, wpb=2809.3, bsz=128, num_updates=21700, lr=0.000300537, gnorm=0.923, loss_scale=16, train_wall=8, gb_free=19.4, wall=1761
2022-05-28 12:13:50 | INFO | train_inner | epoch 004:   3747 / 6032 loss=5.595, nll_loss=4.023, ppl=16.25, wps=35091, ups=12.55, wpb=2796.4, bsz=128, num_updates=21800, lr=0.000299847, gnorm=0.951, loss_scale=16, train_wall=8, gb_free=20.7, wall=1769
2022-05-28 12:13:57 | INFO | train_inner | epoch 004:   3847 / 6032 loss=5.553, nll_loss=3.975, ppl=15.73, wps=34230.4, ups=13.13, wpb=2607.1, bsz=128, num_updates=21900, lr=0.000299162, gnorm=0.949, loss_scale=16, train_wall=7, gb_free=20, wall=1777
2022-05-28 12:14:05 | INFO | train_inner | epoch 004:   3947 / 6032 loss=5.554, nll_loss=3.978, ppl=15.76, wps=34149, ups=13.04, wpb=2618.7, bsz=128, num_updates=22000, lr=0.000298481, gnorm=0.957, loss_scale=16, train_wall=7, gb_free=18.2, wall=1784
2022-05-28 12:14:13 | INFO | train_inner | epoch 004:   4047 / 6032 loss=5.581, nll_loss=4.007, ppl=16.08, wps=34459.1, ups=12.77, wpb=2697.7, bsz=128, num_updates=22100, lr=0.000297805, gnorm=0.941, loss_scale=16, train_wall=8, gb_free=21, wall=1792
2022-05-28 12:14:21 | INFO | train_inner | epoch 004:   4147 / 6032 loss=5.437, nll_loss=3.844, ppl=14.36, wps=34904, ups=12.53, wpb=2785.1, bsz=128, num_updates=22200, lr=0.000297133, gnorm=0.922, loss_scale=16, train_wall=8, gb_free=20.4, wall=1800
2022-05-28 12:14:29 | INFO | train_inner | epoch 004:   4247 / 6032 loss=5.557, nll_loss=3.98, ppl=15.78, wps=34952.2, ups=13.06, wpb=2676.4, bsz=128, num_updates=22300, lr=0.000296466, gnorm=0.928, loss_scale=16, train_wall=7, gb_free=20.3, wall=1808
2022-05-28 12:14:36 | INFO | train_inner | epoch 004:   4347 / 6032 loss=5.428, nll_loss=3.835, ppl=14.27, wps=33272.8, ups=12.77, wpb=2606.1, bsz=128, num_updates=22400, lr=0.000295804, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=20.4, wall=1816
2022-05-28 12:14:45 | INFO | train_inner | epoch 004:   4447 / 6032 loss=5.512, nll_loss=3.93, ppl=15.25, wps=31664, ups=11.52, wpb=2748.1, bsz=128, num_updates=22500, lr=0.000295146, gnorm=0.944, loss_scale=16, train_wall=8, gb_free=19.5, wall=1824
2022-05-28 12:14:53 | INFO | train_inner | epoch 004:   4547 / 6032 loss=5.48, nll_loss=3.894, ppl=14.87, wps=33401.1, ups=13.28, wpb=2515.7, bsz=128, num_updates=22600, lr=0.000294492, gnorm=0.939, loss_scale=16, train_wall=7, gb_free=19.9, wall=1832
2022-05-28 12:15:01 | INFO | train_inner | epoch 004:   4647 / 6032 loss=5.555, nll_loss=3.98, ppl=15.78, wps=33606.3, ups=11.98, wpb=2805.2, bsz=128, num_updates=22700, lr=0.000293843, gnorm=0.946, loss_scale=16, train_wall=8, gb_free=18.3, wall=1840
2022-05-28 12:15:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 1.29 GiB free; 20.74 GiB reserved in total by PyTorch)
2022-05-28 12:15:02 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 102       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18614 MB |   18677 MB |  138322 GB |  138304 GB |
|       from large pool |   18574 MB |   18637 MB |  135245 GB |  135226 GB |
|       from small pool |      40 MB |      73 MB |    3077 GB |    3077 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18614 MB |   18677 MB |  138322 GB |  138304 GB |
|       from large pool |   18574 MB |   18637 MB |  135245 GB |  135226 GB |
|       from small pool |      40 MB |      73 MB |    3077 GB |    3077 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21234 MB |   21234 MB |  591562 MB |  570328 MB |
|       from large pool |   21186 MB |   21186 MB |  580236 MB |  559050 MB |
|       from small pool |      48 MB |     196 MB |   11326 MB |   11278 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2619 MB |    3279 MB |  182757 GB |  182755 GB |
|       from large pool |    2611 MB |    3271 MB |  179127 GB |  179125 GB |
|       from small pool |       7 MB |      14 MB |    3630 GB |    3630 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   36346 K  |   36346 K  |
|       from large pool |     271    |     275    |   17179 K  |   17178 K  |
|       from small pool |     321    |     466    |   19167 K  |   19167 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   36346 K  |   36346 K  |
|       from large pool |     271    |     275    |   17179 K  |   17178 K  |
|       from small pool |     321    |     466    |   19167 K  |   19167 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     131    |    5981    |    5926    |
|       from large pool |      31    |      33    |     318    |     287    |
|       from small pool |      24    |      98    |    5663    |    5639    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      41    |   18399 K  |   18399 K  |
|       from large pool |      20    |      21    |    9667 K  |    9667 K  |
|       from small pool |      12    |      26    |    8732 K  |    8732 K  |
|===========================================================================|

2022-05-28 12:15:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:15:09 | INFO | train_inner | epoch 004:   4748 / 6032 loss=5.522, nll_loss=3.941, ppl=15.36, wps=36117.7, ups=12.18, wpb=2965.2, bsz=128, num_updates=22800, lr=0.000293198, gnorm=0.931, loss_scale=16, train_wall=8, gb_free=20.5, wall=1848
2022-05-28 12:15:17 | INFO | train_inner | epoch 004:   4848 / 6032 loss=5.382, nll_loss=3.783, ppl=13.76, wps=32713.6, ups=13.3, wpb=2459.2, bsz=128, num_updates=22900, lr=0.000292557, gnorm=0.933, loss_scale=16, train_wall=7, gb_free=20.5, wall=1856
2022-05-28 12:15:24 | INFO | train_inner | epoch 004:   4948 / 6032 loss=5.518, nll_loss=3.938, ppl=15.32, wps=34589.4, ups=13.12, wpb=2637, bsz=128, num_updates=23000, lr=0.00029192, gnorm=0.962, loss_scale=16, train_wall=7, gb_free=19.8, wall=1863
2022-05-28 12:15:32 | INFO | train_inner | epoch 004:   5048 / 6032 loss=5.408, nll_loss=3.813, ppl=14.05, wps=33440.8, ups=13.17, wpb=2539, bsz=128, num_updates=23100, lr=0.000291288, gnorm=0.951, loss_scale=16, train_wall=7, gb_free=20.9, wall=1871
2022-05-28 12:15:40 | INFO | train_inner | epoch 004:   5148 / 6032 loss=5.645, nll_loss=4.082, ppl=16.93, wps=35294.8, ups=12.64, wpb=2792.8, bsz=128, num_updates=23200, lr=0.000290659, gnorm=0.91, loss_scale=16, train_wall=8, gb_free=18.7, wall=1879
2022-05-28 12:15:48 | INFO | train_inner | epoch 004:   5248 / 6032 loss=5.456, nll_loss=3.867, ppl=14.59, wps=33767.9, ups=12.24, wpb=2759.9, bsz=128, num_updates=23300, lr=0.000290035, gnorm=0.911, loss_scale=16, train_wall=8, gb_free=20, wall=1887
2022-05-28 12:15:56 | INFO | train_inner | epoch 004:   5348 / 6032 loss=5.479, nll_loss=3.894, ppl=14.86, wps=33317, ups=12.82, wpb=2598.8, bsz=128, num_updates=23400, lr=0.000289414, gnorm=0.925, loss_scale=16, train_wall=8, gb_free=19.1, wall=1895
2022-05-28 12:16:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.69 GiB already allocated; 4.45 GiB free; 17.58 GiB reserved in total by PyTorch)
2022-05-28 12:16:03 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 103       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17088 MB |   17147 MB |  142925 GB |  142909 GB |
|       from large pool |   17048 MB |   17106 MB |  139748 GB |  139731 GB |
|       from small pool |      39 MB |      73 MB |    3177 GB |    3177 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17088 MB |   17147 MB |  142925 GB |  142909 GB |
|       from large pool |   17048 MB |   17106 MB |  139748 GB |  139731 GB |
|       from small pool |      39 MB |      73 MB |    3177 GB |    3177 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18004 MB |   21388 MB |  591716 MB |  573712 MB |
|       from large pool |   17962 MB |   21186 MB |  580236 MB |  562274 MB |
|       from small pool |      42 MB |     202 MB |   11480 MB |   11438 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     913 MB |    3238 MB |  187726 GB |  187725 GB |
|       from large pool |     913 MB |    3238 MB |  183977 GB |  183976 GB |
|       from small pool |       0 MB |      11 MB |    3748 GB |    3748 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   37569 K  |   37569 K  |
|       from large pool |     271    |     275    |   17761 K  |   17760 K  |
|       from small pool |     321    |     466    |   19808 K  |   19808 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   37569 K  |   37569 K  |
|       from large pool |     271    |     275    |   17761 K  |   17760 K  |
|       from small pool |     321    |     466    |   19808 K  |   19808 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     132    |    6058    |    6007    |
|       from large pool |      30    |      31    |     318    |     288    |
|       from small pool |      21    |     101    |    5740    |    5719    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      57    |   19024 K  |   19024 K  |
|       from large pool |      16    |      23    |    9999 K  |    9999 K  |
|       from small pool |       6    |      35    |    9024 K  |    9024 K  |
|===========================================================================|

2022-05-28 12:16:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:16:04 | INFO | train_inner | epoch 004:   5449 / 6032 loss=5.552, nll_loss=3.975, ppl=15.72, wps=34664.9, ups=11.99, wpb=2890.2, bsz=128, num_updates=23500, lr=0.000288798, gnorm=0.92, loss_scale=16, train_wall=8, gb_free=19.3, wall=1903
2022-05-28 12:16:12 | INFO | train_inner | epoch 004:   5549 / 6032 loss=5.492, nll_loss=3.909, ppl=15.02, wps=33999.3, ups=13.15, wpb=2585.9, bsz=128, num_updates=23600, lr=0.000288185, gnorm=0.952, loss_scale=16, train_wall=7, gb_free=18, wall=1911
2022-05-28 12:16:19 | INFO | train_inner | epoch 004:   5649 / 6032 loss=5.371, nll_loss=3.771, ppl=13.65, wps=33420, ups=13.29, wpb=2515.6, bsz=128, num_updates=23700, lr=0.000287577, gnorm=0.912, loss_scale=16, train_wall=7, gb_free=19.7, wall=1918
2022-05-28 12:16:27 | INFO | train_inner | epoch 004:   5749 / 6032 loss=5.461, nll_loss=3.876, ppl=14.68, wps=33168.3, ups=13.43, wpb=2469.2, bsz=128, num_updates=23800, lr=0.000286972, gnorm=0.935, loss_scale=16, train_wall=7, gb_free=20.5, wall=1926
2022-05-28 12:16:35 | INFO | train_inner | epoch 004:   5849 / 6032 loss=5.623, nll_loss=4.058, ppl=16.65, wps=35816.1, ups=12.75, wpb=2809.9, bsz=128, num_updates=23900, lr=0.000286371, gnorm=0.929, loss_scale=16, train_wall=8, gb_free=21, wall=1934
2022-05-28 12:16:42 | INFO | train_inner | epoch 004:   5949 / 6032 loss=5.503, nll_loss=3.924, ppl=15.18, wps=33065.4, ups=12.81, wpb=2581.4, bsz=128, num_updates=24000, lr=0.000285774, gnorm=0.96, loss_scale=16, train_wall=8, gb_free=7.4, wall=1941
2022-05-28 12:16:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:17:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.542 | nll_loss 3.93 | ppl 15.25 | wps 107660 | wpb 2687.4 | bsz 128 | num_updates 24083 | best_loss 5.542
2022-05-28 12:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 24083 updates
2022-05-28 12:17:07 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint4.pt
2022-05-28 12:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint4.pt
2022-05-28 12:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint4.pt (epoch 4 @ 24083 updates, score 5.542) (writing took 9.909238300751895 seconds)
2022-05-28 12:17:17 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-05-28 12:17:17 | INFO | train | epoch 004 | loss 5.529 | nll_loss 3.947 | ppl 15.42 | wps 31928.6 | ups 11.92 | wpb 2679.5 | bsz 128 | num_updates 24083 | lr 0.000285281 | gnorm 0.944 | loss_scale 16 | train_wall 463 | gb_free 19.5 | wall 1977
2022-05-28 12:17:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:17:17 | INFO | fairseq.trainer | begin training epoch 5
2022-05-28 12:17:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:17:19 | INFO | train_inner | epoch 005:     17 / 6032 loss=5.419, nll_loss=3.826, ppl=14.18, wps=6841.6, ups=2.74, wpb=2499.5, bsz=128, num_updates=24100, lr=0.00028518, gnorm=0.916, loss_scale=16, train_wall=7, gb_free=19.3, wall=1978
2022-05-28 12:17:27 | INFO | train_inner | epoch 005:    117 / 6032 loss=5.251, nll_loss=3.628, ppl=12.36, wps=35588, ups=12.85, wpb=2769.2, bsz=128, num_updates=24200, lr=0.00028459, gnorm=0.921, loss_scale=16, train_wall=8, gb_free=19.2, wall=1986
2022-05-28 12:17:35 | INFO | train_inner | epoch 005:    217 / 6032 loss=5.392, nll_loss=3.786, ppl=13.79, wps=37416.8, ups=12.35, wpb=3029.2, bsz=128, num_updates=24300, lr=0.000284004, gnorm=0.918, loss_scale=16, train_wall=8, gb_free=19.6, wall=1994
2022-05-28 12:17:43 | INFO | train_inner | epoch 005:    317 / 6032 loss=5.321, nll_loss=3.705, ppl=13.04, wps=36580.5, ups=12.6, wpb=2904, bsz=128, num_updates=24400, lr=0.000283422, gnorm=0.896, loss_scale=16, train_wall=8, gb_free=19.6, wall=2002
2022-05-28 12:17:50 | INFO | train_inner | epoch 005:    417 / 6032 loss=5.156, nll_loss=3.518, ppl=11.45, wps=33463.9, ups=13.25, wpb=2525.1, bsz=128, num_updates=24500, lr=0.000282843, gnorm=0.925, loss_scale=16, train_wall=7, gb_free=19.1, wall=2009
2022-05-28 12:17:58 | INFO | train_inner | epoch 005:    517 / 6032 loss=5.401, nll_loss=3.797, ppl=13.9, wps=34720, ups=12.52, wpb=2774.1, bsz=128, num_updates=24600, lr=0.000282267, gnorm=0.927, loss_scale=16, train_wall=8, gb_free=18.9, wall=2017
2022-05-28 12:18:06 | INFO | train_inner | epoch 005:    617 / 6032 loss=5.235, nll_loss=3.608, ppl=12.19, wps=32328.4, ups=12.78, wpb=2530.2, bsz=128, num_updates=24700, lr=0.000281695, gnorm=0.966, loss_scale=16, train_wall=8, gb_free=17, wall=2025
2022-05-28 12:18:14 | INFO | train_inner | epoch 005:    717 / 6032 loss=5.214, nll_loss=3.584, ppl=11.99, wps=31325.9, ups=12.01, wpb=2609.2, bsz=128, num_updates=24800, lr=0.000281127, gnorm=0.926, loss_scale=16, train_wall=8, gb_free=19.4, wall=2034
2022-05-28 12:18:23 | INFO | train_inner | epoch 005:    817 / 6032 loss=5.304, nll_loss=3.688, ppl=12.88, wps=32514.5, ups=11.88, wpb=2737.9, bsz=128, num_updates=24900, lr=0.000280562, gnorm=0.908, loss_scale=16, train_wall=8, gb_free=20.4, wall=2042
2022-05-28 12:18:31 | INFO | train_inner | epoch 005:    917 / 6032 loss=5.438, nll_loss=3.839, ppl=14.31, wps=34124.5, ups=11.91, wpb=2865.5, bsz=128, num_updates=25000, lr=0.00028, gnorm=0.943, loss_scale=16, train_wall=8, gb_free=20.9, wall=2050
2022-05-28 12:18:39 | INFO | train_inner | epoch 005:   1017 / 6032 loss=5.22, nll_loss=3.591, ppl=12.05, wps=33854.4, ups=13.25, wpb=2554.8, bsz=128, num_updates=25100, lr=0.000279442, gnorm=0.931, loss_scale=16, train_wall=7, gb_free=18.6, wall=2058
2022-05-28 12:18:47 | INFO | train_inner | epoch 005:   1117 / 6032 loss=5.378, nll_loss=3.77, ppl=13.65, wps=33190.4, ups=11.94, wpb=2779.6, bsz=128, num_updates=25200, lr=0.000278887, gnorm=0.944, loss_scale=16, train_wall=8, gb_free=17.1, wall=2066
2022-05-28 12:18:55 | INFO | train_inner | epoch 005:   1217 / 6032 loss=5.186, nll_loss=3.554, ppl=11.74, wps=31724.8, ups=13.01, wpb=2437.7, bsz=128, num_updates=25300, lr=0.000278335, gnorm=0.934, loss_scale=16, train_wall=7, gb_free=20.2, wall=2074
2022-05-28 12:19:03 | INFO | train_inner | epoch 005:   1317 / 6032 loss=5.355, nll_loss=3.744, ppl=13.4, wps=35436.7, ups=12.76, wpb=2776.7, bsz=126.8, num_updates=25400, lr=0.000277787, gnorm=0.946, loss_scale=16, train_wall=8, gb_free=19, wall=2082
2022-05-28 12:19:10 | INFO | train_inner | epoch 005:   1417 / 6032 loss=5.249, nll_loss=3.624, ppl=12.33, wps=34256.4, ups=12.78, wpb=2680.2, bsz=128, num_updates=25500, lr=0.000277241, gnorm=0.971, loss_scale=16, train_wall=8, gb_free=17.2, wall=2090
2022-05-28 12:19:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.62 GiB (GPU 0; 23.70 GiB total capacity; 15.66 GiB already allocated; 3.50 GiB free; 18.54 GiB reserved in total by PyTorch)
2022-05-28 12:19:11 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 106       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12326 MB |   16037 MB |  156007 GB |  155995 GB |
|       from large pool |   12287 MB |   15998 MB |  152529 GB |  152517 GB |
|       from small pool |      39 MB |      73 MB |    3477 GB |    3477 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12326 MB |   16037 MB |  156007 GB |  155995 GB |
|       from large pool |   12287 MB |   15998 MB |  152529 GB |  152517 GB |
|       from small pool |      39 MB |      73 MB |    3477 GB |    3477 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18980 MB |   19142 MB |  608328 MB |  589348 MB |
|       from large pool |   18940 MB |   18940 MB |  596412 MB |  577472 MB |
|       from small pool |      40 MB |     202 MB |   11916 MB |   11876 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6653 MB |    6653 MB |  207790 GB |  207783 GB |
|       from large pool |    6652 MB |    6652 MB |  203691 GB |  203684 GB |
|       from small pool |       0 MB |      13 MB |    4099 GB |    4099 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   41022 K  |   41021 K  |
|       from large pool |     271    |     275    |   19396 K  |   19396 K  |
|       from small pool |     321    |     465    |   21625 K  |   21625 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   41022 K  |   41021 K  |
|       from large pool |     271    |     275    |   19396 K  |   19396 K  |
|       from small pool |     321    |     465    |   21625 K  |   21625 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     124    |    6280    |    6237    |
|       from large pool |      23    |      23    |     322    |     299    |
|       from small pool |      20    |     101    |    5958    |    5938    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      63    |   20774 K  |   20774 K  |
|       from large pool |      18    |      18    |   10930 K  |   10930 K  |
|       from small pool |       6    |      46    |    9843 K  |    9843 K  |
|===========================================================================|

2022-05-28 12:19:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:19:19 | INFO | train_inner | epoch 005:   1518 / 6032 loss=5.348, nll_loss=3.738, ppl=13.34, wps=34159.9, ups=12.19, wpb=2803.2, bsz=128, num_updates=25600, lr=0.000276699, gnorm=0.913, loss_scale=16, train_wall=8, gb_free=19.1, wall=2098
2022-05-28 12:19:27 | INFO | train_inner | epoch 005:   1618 / 6032 loss=5.363, nll_loss=3.754, ppl=13.49, wps=34661.6, ups=12.53, wpb=2767.3, bsz=128, num_updates=25700, lr=0.00027616, gnorm=0.928, loss_scale=16, train_wall=8, gb_free=18.5, wall=2106
2022-05-28 12:19:34 | INFO | train_inner | epoch 005:   1718 / 6032 loss=5.189, nll_loss=3.557, ppl=11.77, wps=33446.1, ups=12.91, wpb=2591.5, bsz=128, num_updates=25800, lr=0.000275625, gnorm=0.91, loss_scale=16, train_wall=8, gb_free=20, wall=2114
2022-05-28 12:19:42 | INFO | train_inner | epoch 005:   1818 / 6032 loss=5.372, nll_loss=3.764, ppl=13.59, wps=34484, ups=12.45, wpb=2770, bsz=128, num_updates=25900, lr=0.000275092, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=19.6, wall=2122
2022-05-28 12:19:50 | INFO | train_inner | epoch 005:   1918 / 6032 loss=5.132, nll_loss=3.493, ppl=11.26, wps=30680.4, ups=12.44, wpb=2465.7, bsz=128, num_updates=26000, lr=0.000274563, gnorm=0.92, loss_scale=16, train_wall=8, gb_free=18.3, wall=2130
2022-05-28 12:19:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 2.23 GiB free; 19.80 GiB reserved in total by PyTorch)
2022-05-28 12:19:51 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 109       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14033 MB |   18403 MB |  159002 GB |  158989 GB |
|       from large pool |   13994 MB |   18364 MB |  155459 GB |  155445 GB |
|       from small pool |      39 MB |      73 MB |    3543 GB |    3543 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14033 MB |   18403 MB |  159002 GB |  158989 GB |
|       from large pool |   13994 MB |   18364 MB |  155459 GB |  155445 GB |
|       from small pool |      39 MB |      73 MB |    3543 GB |    3543 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20272 MB |   20430 MB |  620960 MB |  600688 MB |
|       from large pool |   20228 MB |   20228 MB |  608722 MB |  588494 MB |
|       from small pool |      44 MB |     202 MB |   12238 MB |   12194 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1866 MB |    5447 MB |  212987 GB |  212985 GB |
|       from large pool |    1861 MB |    5442 MB |  208810 GB |  208808 GB |
|       from small pool |       4 MB |      14 MB |    4176 GB |    4176 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   41820 K  |   41819 K  |
|       from large pool |     271    |     275    |   19776 K  |   19776 K  |
|       from small pool |     321    |     466    |   22044 K  |   22043 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   41820 K  |   41819 K  |
|       from large pool |     271    |     275    |   19776 K  |   19776 K  |
|       from small pool |     321    |     466    |   22044 K  |   22043 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     125    |    6444    |    6398    |
|       from large pool |      24    |      24    |     325    |     301    |
|       from small pool |      22    |     101    |    6119    |    6097    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      56    |   21180 K  |   21180 K  |
|       from large pool |      16    |      17    |   11144 K  |   11144 K  |
|       from small pool |       9    |      40    |   10035 K  |   10035 K  |
|===========================================================================|

2022-05-28 12:19:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:19:59 | INFO | train_inner | epoch 005:   2019 / 6032 loss=5.286, nll_loss=3.668, ppl=12.71, wps=32404.9, ups=11.52, wpb=2813.3, bsz=128, num_updates=26100, lr=0.000274036, gnorm=0.915, loss_scale=16, train_wall=8, gb_free=18.2, wall=2138
2022-05-28 12:20:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 3.04 GiB free; 18.99 GiB reserved in total by PyTorch)
2022-05-28 12:20:03 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 114       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18612 MB |   18675 MB |  159862 GB |  159844 GB |
|       from large pool |   18572 MB |   18634 MB |  156303 GB |  156285 GB |
|       from small pool |      40 MB |      73 MB |    3558 GB |    3558 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18612 MB |   18675 MB |  159862 GB |  159844 GB |
|       from large pool |   18572 MB |   18634 MB |  156303 GB |  156285 GB |
|       from small pool |      40 MB |      73 MB |    3558 GB |    3558 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19444 MB |   22070 MB |  654644 MB |  635200 MB |
|       from large pool |   19402 MB |   21886 MB |  641950 MB |  622548 MB |
|       from small pool |      42 MB |     184 MB |   12694 MB |   12652 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     831 MB |    4048 MB |  214153 GB |  214152 GB |
|       from large pool |     829 MB |    4046 MB |  209957 GB |  209956 GB |
|       from small pool |       1 MB |      46 MB |    4195 GB |    4195 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   42031 K  |   42031 K  |
|       from large pool |     271    |     275    |   19879 K  |   19879 K  |
|       from small pool |     321    |     466    |   22152 K  |   22151 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   42031 K  |   42031 K  |
|       from large pool |     271    |     275    |   19879 K  |   19879 K  |
|       from small pool |     321    |     466    |   22152 K  |   22151 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     126    |    6696    |    6644    |
|       from large pool |      31    |      34    |     349    |     318    |
|       from small pool |      21    |      92    |    6347    |    6326    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      76    |   21288 K  |   21288 K  |
|       from large pool |      24    |      25    |   11203 K  |   11203 K  |
|       from small pool |      11    |      68    |   10084 K  |   10084 K  |
|===========================================================================|

2022-05-28 12:20:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:20:07 | INFO | train_inner | epoch 005:   2120 / 6032 loss=5.315, nll_loss=3.702, ppl=13.01, wps=31699.1, ups=12.28, wpb=2581.2, bsz=128, num_updates=26200, lr=0.000273513, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=20.4, wall=2146
2022-05-28 12:20:15 | INFO | train_inner | epoch 005:   2220 / 6032 loss=5.377, nll_loss=3.773, ppl=13.67, wps=34371.7, ups=12.73, wpb=2700.8, bsz=128, num_updates=26300, lr=0.000272992, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=18.4, wall=2154
2022-05-28 12:20:23 | INFO | train_inner | epoch 005:   2320 / 6032 loss=5.274, nll_loss=3.655, ppl=12.6, wps=34815.4, ups=12.8, wpb=2719.9, bsz=128, num_updates=26400, lr=0.000272475, gnorm=0.911, loss_scale=16, train_wall=8, gb_free=20.3, wall=2162
2022-05-28 12:20:31 | INFO | train_inner | epoch 005:   2420 / 6032 loss=5.161, nll_loss=3.526, ppl=11.52, wps=32258, ups=13.23, wpb=2438.1, bsz=128, num_updates=26500, lr=0.00027196, gnorm=0.935, loss_scale=16, train_wall=7, gb_free=19.4, wall=2170
2022-05-28 12:20:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 4.02 GiB free; 18.01 GiB reserved in total by PyTorch)
2022-05-28 12:20:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 115       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15205 MB |   15257 MB |  162145 GB |  162130 GB |
|       from large pool |   15165 MB |   15217 MB |  158531 GB |  158516 GB |
|       from small pool |      39 MB |      73 MB |    3614 GB |    3614 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15205 MB |   15257 MB |  162145 GB |  162130 GB |
|       from large pool |   15165 MB |   15217 MB |  158531 GB |  158516 GB |
|       from small pool |      39 MB |      73 MB |    3614 GB |    3614 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18440 MB |   22534 MB |  657734 MB |  639294 MB |
|       from large pool |   18400 MB |   22332 MB |  644880 MB |  626480 MB |
|       from small pool |      40 MB |     202 MB |   12854 MB |   12814 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3234 MB |    3234 MB |  216377 GB |  216373 GB |
|       from large pool |    3234 MB |    3234 MB |  212116 GB |  212112 GB |
|       from small pool |       0 MB |      15 MB |    4260 GB |    4260 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   42646 K  |   42646 K  |
|       from large pool |     271    |     275    |   20164 K  |   20164 K  |
|       from small pool |     321    |     466    |   22482 K  |   22481 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   42646 K  |   42646 K  |
|       from large pool |     271    |     275    |   20164 K  |   20164 K  |
|       from small pool |     321    |     466    |   22482 K  |   22481 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     133    |    6777    |    6726    |
|       from large pool |      31    |      32    |     350    |     319    |
|       from small pool |      20    |     101    |    6427    |    6407    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      68    |   21602 K  |   21602 K  |
|       from large pool |      26    |      26    |   11364 K  |   11364 K  |
|       from small pool |       7    |      46    |   10237 K  |   10237 K  |
|===========================================================================|

2022-05-28 12:20:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:20:38 | INFO | train_inner | epoch 005:   2521 / 6032 loss=5.12, nll_loss=3.481, ppl=11.17, wps=31444.1, ups=12.85, wpb=2446.8, bsz=128, num_updates=26600, lr=0.000271448, gnorm=0.955, loss_scale=16, train_wall=7, gb_free=18.3, wall=2177
2022-05-28 12:20:46 | INFO | train_inner | epoch 005:   2621 / 6032 loss=5.34, nll_loss=3.73, ppl=13.27, wps=32048.8, ups=12.43, wpb=2578.5, bsz=128, num_updates=26700, lr=0.00027094, gnorm=0.926, loss_scale=16, train_wall=8, gb_free=18.7, wall=2185
2022-05-28 12:20:54 | INFO | train_inner | epoch 005:   2721 / 6032 loss=5.131, nll_loss=3.493, ppl=11.26, wps=32031.7, ups=12.98, wpb=2467.8, bsz=128, num_updates=26800, lr=0.000270434, gnorm=0.944, loss_scale=16, train_wall=7, gb_free=20.8, wall=2193
2022-05-28 12:21:02 | INFO | train_inner | epoch 005:   2821 / 6032 loss=5.473, nll_loss=3.883, ppl=14.75, wps=34320.9, ups=12.41, wpb=2764.7, bsz=128, num_updates=26900, lr=0.00026993, gnorm=0.926, loss_scale=16, train_wall=8, gb_free=19.5, wall=2201
2022-05-28 12:21:10 | INFO | train_inner | epoch 005:   2921 / 6032 loss=5.338, nll_loss=3.73, ppl=13.27, wps=33706.3, ups=12.83, wpb=2627.2, bsz=128, num_updates=27000, lr=0.00026943, gnorm=0.993, loss_scale=16, train_wall=8, gb_free=19.4, wall=2209
2022-05-28 12:21:18 | INFO | train_inner | epoch 005:   3021 / 6032 loss=5.314, nll_loss=3.702, ppl=13.01, wps=33968.6, ups=12.57, wpb=2702, bsz=128, num_updates=27100, lr=0.000268933, gnorm=0.939, loss_scale=16, train_wall=8, gb_free=19.9, wall=2217
2022-05-28 12:21:25 | INFO | train_inner | epoch 005:   3121 / 6032 loss=5.221, nll_loss=3.597, ppl=12.1, wps=31292.4, ups=13.41, wpb=2333.3, bsz=128, num_updates=27200, lr=0.000268438, gnorm=0.961, loss_scale=16, train_wall=7, gb_free=19.1, wall=2224
2022-05-28 12:21:33 | INFO | train_inner | epoch 005:   3221 / 6032 loss=5.302, nll_loss=3.689, ppl=12.9, wps=33480.1, ups=12.73, wpb=2630.8, bsz=128, num_updates=27300, lr=0.000267946, gnorm=0.949, loss_scale=16, train_wall=8, gb_free=19.2, wall=2232
2022-05-28 12:21:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 23.70 GiB total capacity; 15.64 GiB already allocated; 1.50 GiB free; 20.54 GiB reserved in total by PyTorch)
2022-05-28 12:21:39 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 119       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12349 MB |   16011 MB |  166887 GB |  166875 GB |
|       from large pool |   12310 MB |   15972 MB |  163162 GB |  163150 GB |
|       from small pool |      39 MB |      73 MB |    3725 GB |    3725 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12349 MB |   16011 MB |  166887 GB |  166875 GB |
|       from large pool |   12310 MB |   15972 MB |  163162 GB |  163150 GB |
|       from small pool |      39 MB |      73 MB |    3725 GB |    3725 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21028 MB |   21028 MB |  675166 MB |  654138 MB |
|       from large pool |   20988 MB |   20988 MB |  661828 MB |  640840 MB |
|       from small pool |      40 MB |     202 MB |   13338 MB |   13298 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5014 MB |    5016 MB |  221054 GB |  221049 GB |
|       from large pool |    5013 MB |    5015 MB |  216662 GB |  216657 GB |
|       from small pool |       0 MB |      13 MB |    4392 GB |    4392 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   43933 K  |   43933 K  |
|       from large pool |     271    |     275    |   20767 K  |   20766 K  |
|       from small pool |     321    |     466    |   23166 K  |   23166 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   43933 K  |   43933 K  |
|       from large pool |     271    |     275    |   20767 K  |   20766 K  |
|       from small pool |     321    |     466    |   23166 K  |   23166 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     131    |    7024    |    6974    |
|       from large pool |      30    |      30    |     355    |     325    |
|       from small pool |      20    |     101    |    6669    |    6649    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      58    |   22257 K  |   22257 K  |
|       from large pool |      25    |      26    |   11706 K  |   11706 K  |
|       from small pool |       8    |      39    |   10551 K  |   10551 K  |
|===========================================================================|

2022-05-28 12:21:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:21:42 | INFO | train_inner | epoch 005:   3322 / 6032 loss=5.367, nll_loss=3.761, ppl=13.56, wps=31418.7, ups=11.21, wpb=2802.2, bsz=128, num_updates=27400, lr=0.000267456, gnorm=0.917, loss_scale=16, train_wall=8, gb_free=20.4, wall=2241
2022-05-28 12:21:50 | INFO | train_inner | epoch 005:   3422 / 6032 loss=5.264, nll_loss=3.645, ppl=12.51, wps=33139.2, ups=12.68, wpb=2613.8, bsz=128, num_updates=27500, lr=0.00026697, gnorm=0.978, loss_scale=16, train_wall=8, gb_free=19.7, wall=2249
2022-05-28 12:21:58 | INFO | train_inner | epoch 005:   3522 / 6032 loss=5.511, nll_loss=3.928, ppl=15.22, wps=33683.8, ups=12.16, wpb=2769.4, bsz=128, num_updates=27600, lr=0.000266485, gnorm=0.937, loss_scale=16, train_wall=8, gb_free=20.3, wall=2257
2022-05-28 12:21:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.68 GiB already allocated; 1.92 GiB free; 20.11 GiB reserved in total by PyTorch)
2022-05-28 12:21:58 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 120       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17085 MB |   17143 MB |  168300 GB |  168283 GB |
|       from large pool |   17045 MB |   17103 MB |  164548 GB |  164531 GB |
|       from small pool |      39 MB |      73 MB |    3752 GB |    3752 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17085 MB |   17143 MB |  168300 GB |  168283 GB |
|       from large pool |   17045 MB |   17103 MB |  164548 GB |  164531 GB |
|       from small pool |      39 MB |      73 MB |    3752 GB |    3752 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20590 MB |   20750 MB |  678552 MB |  657962 MB |
|       from large pool |   20548 MB |   20548 MB |  665052 MB |  644504 MB |
|       from small pool |      42 MB |     202 MB |   13500 MB |   13458 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3502 MB |    3504 MB |  222471 GB |  222468 GB |
|       from large pool |    3502 MB |    3502 MB |  218047 GB |  218043 GB |
|       from small pool |       0 MB |       3 MB |    4424 GB |    4424 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   44296 K  |   44295 K  |
|       from large pool |     271    |     275    |   20942 K  |   20942 K  |
|       from small pool |     321    |     465    |   23353 K  |   23353 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   44296 K  |   44295 K  |
|       from large pool |     271    |     275    |   20942 K  |   20942 K  |
|       from small pool |     321    |     465    |   23353 K  |   23353 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     131    |    7106    |    7055    |
|       from large pool |      30    |      30    |     356    |     326    |
|       from small pool |      21    |     101    |    6750    |    6729    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      30    |   22442 K  |   22442 K  |
|       from large pool |      20    |      20    |   11805 K  |   11805 K  |
|       from small pool |       8    |      13    |   10636 K  |   10636 K  |
|===========================================================================|

2022-05-28 12:21:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:22:06 | INFO | train_inner | epoch 005:   3623 / 6032 loss=5.157, nll_loss=3.523, ppl=11.5, wps=30788, ups=12.16, wpb=2531, bsz=128, num_updates=27700, lr=0.000266004, gnorm=0.93, loss_scale=16, train_wall=8, gb_free=17.9, wall=2266
2022-05-28 12:22:15 | INFO | train_inner | epoch 005:   3723 / 6032 loss=5.342, nll_loss=3.735, ppl=13.32, wps=30997.6, ups=11.58, wpb=2676.3, bsz=128, num_updates=27800, lr=0.000265525, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=20.8, wall=2274
2022-05-28 12:22:23 | INFO | train_inner | epoch 005:   3823 / 6032 loss=5.282, nll_loss=3.664, ppl=12.68, wps=33596.1, ups=12.53, wpb=2680.7, bsz=128, num_updates=27900, lr=0.000265049, gnorm=0.93, loss_scale=16, train_wall=8, gb_free=20.7, wall=2282
2022-05-28 12:22:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 1.02 GiB free; 21.01 GiB reserved in total by PyTorch)
2022-05-28 12:22:29 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 123       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12703 MB |   16634 MB |  170475 GB |  170463 GB |
|       from large pool |   12664 MB |   16595 MB |  166671 GB |  166658 GB |
|       from small pool |      39 MB |      73 MB |    3804 GB |    3804 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12703 MB |   16634 MB |  170475 GB |  170463 GB |
|       from large pool |   12664 MB |   16595 MB |  166671 GB |  166658 GB |
|       from small pool |      39 MB |      73 MB |    3804 GB |    3804 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21518 MB |   21518 MB |  689938 MB |  668420 MB |
|       from large pool |   21476 MB |   21476 MB |  676116 MB |  654640 MB |
|       from small pool |      42 MB |     202 MB |   13822 MB |   13780 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4882 MB |    4883 MB |  224703 GB |  224699 GB |
|       from large pool |    4879 MB |    4880 MB |  220218 GB |  220213 GB |
|       from small pool |       2 MB |      10 MB |    4485 GB |    4485 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   44872 K  |   44871 K  |
|       from large pool |     271    |     275    |   21208 K  |   21208 K  |
|       from small pool |     321    |     466    |   23663 K  |   23663 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   44872 K  |   44871 K  |
|       from large pool |     271    |     275    |   21208 K  |   21208 K  |
|       from small pool |     321    |     466    |   23663 K  |   23663 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     131    |    7270    |    7219    |
|       from large pool |      30    |      30    |     359    |     329    |
|       from small pool |      21    |     101    |    6911    |    6890    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      41    |   22736 K  |   22736 K  |
|       from large pool |      19    |      20    |   11956 K  |   11956 K  |
|       from small pool |      11    |      23    |   10780 K  |   10780 K  |
|===========================================================================|

2022-05-28 12:22:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:22:31 | INFO | train_inner | epoch 005:   3924 / 6032 loss=5.307, nll_loss=3.695, ppl=12.95, wps=32447, ups=12.3, wpb=2638.8, bsz=128, num_updates=28000, lr=0.000264575, gnorm=0.917, loss_scale=16, train_wall=8, gb_free=19.6, wall=2290
2022-05-28 12:22:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 1.02 GiB free; 21.02 GiB reserved in total by PyTorch)
2022-05-28 12:22:32 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 124       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   17220 MB |  170673 GB |  170660 GB |
|       from large pool |   13250 MB |   17180 MB |  166863 GB |  166850 GB |
|       from small pool |      39 MB |      73 MB |    3810 GB |    3810 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   17220 MB |  170673 GB |  170660 GB |
|       from large pool |   13250 MB |   17180 MB |  166863 GB |  166850 GB |
|       from small pool |      39 MB |      73 MB |    3810 GB |    3810 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21520 MB |   21674 MB |  694026 MB |  672506 MB |
|       from large pool |   21476 MB |   21476 MB |  680048 MB |  658572 MB |
|       from small pool |      44 MB |     198 MB |   13978 MB |   13934 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4298 MB |    4299 MB |  224898 GB |  224894 GB |
|       from large pool |    4293 MB |    4295 MB |  220405 GB |  220401 GB |
|       from small pool |       4 MB |      48 MB |    4492 GB |    4492 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   44928 K  |   44928 K  |
|       from large pool |     271    |     275    |   21233 K  |   21233 K  |
|       from small pool |     321    |     466    |   23694 K  |   23694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   44928 K  |   44928 K  |
|       from large pool |     271    |     275    |   21233 K  |   21233 K  |
|       from small pool |     321    |     466    |   23694 K  |   23694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     129    |    7349    |    7297    |
|       from large pool |      30    |      30    |     360    |     330    |
|       from small pool |      22    |      99    |    6989    |    6967    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      82    |   22765 K  |   22765 K  |
|       from large pool |      21    |      22    |   11970 K  |   11970 K  |
|       from small pool |      11    |      74    |   10794 K  |   10794 K  |
|===========================================================================|

2022-05-28 12:22:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:22:39 | INFO | train_inner | epoch 005:   4025 / 6032 loss=5.4, nll_loss=3.8, ppl=13.93, wps=32640.3, ups=12.25, wpb=2665.1, bsz=128, num_updates=28100, lr=0.000264104, gnorm=0.967, loss_scale=16, train_wall=8, gb_free=17, wall=2298
2022-05-28 12:22:47 | INFO | train_inner | epoch 005:   4125 / 6032 loss=5.308, nll_loss=3.696, ppl=12.96, wps=32425.6, ups=12.53, wpb=2588.7, bsz=128, num_updates=28200, lr=0.000263635, gnorm=0.945, loss_scale=16, train_wall=8, gb_free=20.8, wall=2306
2022-05-28 12:22:55 | INFO | train_inner | epoch 005:   4225 / 6032 loss=5.282, nll_loss=3.666, ppl=12.7, wps=33490.7, ups=12.82, wpb=2611.8, bsz=128, num_updates=28300, lr=0.000263169, gnorm=0.901, loss_scale=16, train_wall=8, gb_free=18.9, wall=2314
2022-05-28 12:22:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.51 GiB already allocated; 1.21 GiB free; 20.82 GiB reserved in total by PyTorch)
2022-05-28 12:22:59 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 126       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13823 MB |   17925 MB |  172673 GB |  172659 GB |
|       from large pool |   13783 MB |   17885 MB |  168820 GB |  168806 GB |
|       from small pool |      39 MB |      73 MB |    3853 GB |    3853 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13823 MB |   17925 MB |  172673 GB |  172659 GB |
|       from large pool |   13783 MB |   17885 MB |  168820 GB |  168806 GB |
|       from small pool |      39 MB |      73 MB |    3853 GB |    3853 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21320 MB |   21320 MB |  701486 MB |  680166 MB |
|       from large pool |   21280 MB |   21280 MB |  687350 MB |  666070 MB |
|       from small pool |      40 MB |     202 MB |   14136 MB |   14096 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3394 MB |    3394 MB |  226903 GB |  226900 GB |
|       from large pool |    3394 MB |    3394 MB |  222360 GB |  222356 GB |
|       from small pool |       0 MB |       3 MB |    4543 GB |    4543 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   45461 K  |   45460 K  |
|       from large pool |     271    |     275    |   21488 K  |   21487 K  |
|       from small pool |     321    |     466    |   23972 K  |   23972 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   45461 K  |   45460 K  |
|       from large pool |     271    |     275    |   21488 K  |   21487 K  |
|       from small pool |     321    |     466    |   23972 K  |   23972 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     131    |    7430    |    7380    |
|       from large pool |      30    |      30    |     362    |     332    |
|       from small pool |      20    |     101    |    7068    |    7048    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      36    |   23036 K  |   23036 K  |
|       from large pool |      19    |      21    |   12114 K  |   12114 K  |
|       from small pool |       7    |      18    |   10922 K  |   10922 K  |
|===========================================================================|

2022-05-28 12:22:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:23:04 | INFO | train_inner | epoch 005:   4326 / 6032 loss=5.435, nll_loss=3.841, ppl=14.33, wps=35480, ups=11.84, wpb=2997.4, bsz=128, num_updates=28400, lr=0.000262705, gnorm=0.892, loss_scale=16, train_wall=8, gb_free=20.4, wall=2323
2022-05-28 12:23:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.46 GiB (GPU 0; 23.70 GiB total capacity; 15.28 GiB already allocated; 1.76 GiB free; 20.27 GiB reserved in total by PyTorch)
2022-05-28 12:23:11 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 127       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12110 MB |   15650 MB |  173574 GB |  173562 GB |
|       from large pool |   12071 MB |   15611 MB |  169705 GB |  169693 GB |
|       from small pool |      39 MB |      73 MB |    3869 GB |    3869 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12110 MB |   15650 MB |  173574 GB |  173562 GB |
|       from large pool |   12071 MB |   15611 MB |  169705 GB |  169693 GB |
|       from small pool |      39 MB |      73 MB |    3869 GB |    3869 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20760 MB |   20918 MB |  705186 MB |  684426 MB |
|       from large pool |   20720 MB |   20720 MB |  690892 MB |  670172 MB |
|       from small pool |      40 MB |     198 MB |   14294 MB |   14254 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5107 MB |    5109 MB |  227826 GB |  227821 GB |
|       from large pool |    5106 MB |    5108 MB |  223262 GB |  223257 GB |
|       from small pool |       0 MB |      11 MB |    4563 GB |    4563 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   45686 K  |   45685 K  |
|       from large pool |     271    |     275    |   21598 K  |   21597 K  |
|       from small pool |     321    |     466    |   24088 K  |   24088 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   45686 K  |   45685 K  |
|       from large pool |     271    |     275    |   21598 K  |   21597 K  |
|       from small pool |     321    |     466    |   24088 K  |   24088 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     129    |    7510    |    7460    |
|       from large pool |      30    |      30    |     363    |     333    |
|       from small pool |      20    |      99    |    7147    |    7127    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      60    |   23151 K  |   23151 K  |
|       from large pool |      17    |      18    |   12176 K  |   12176 K  |
|       from small pool |       8    |      44    |   10975 K  |   10975 K  |
|===========================================================================|

2022-05-28 12:23:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:23:12 | INFO | train_inner | epoch 005:   4427 / 6032 loss=5.165, nll_loss=3.533, ppl=11.58, wps=31792.4, ups=12.1, wpb=2627.7, bsz=128, num_updates=28500, lr=0.000262244, gnorm=0.892, loss_scale=16, train_wall=8, gb_free=19.8, wall=2331
2022-05-28 12:23:20 | INFO | train_inner | epoch 005:   4527 / 6032 loss=5.453, nll_loss=3.862, ppl=14.54, wps=33352.4, ups=11.68, wpb=2854.4, bsz=128, num_updates=28600, lr=0.000261785, gnorm=0.936, loss_scale=16, train_wall=8, gb_free=20.3, wall=2340
2022-05-28 12:23:29 | INFO | train_inner | epoch 005:   4627 / 6032 loss=5.348, nll_loss=3.743, ppl=13.39, wps=34436, ups=12.24, wpb=2813.6, bsz=128, num_updates=28700, lr=0.000261329, gnorm=0.92, loss_scale=16, train_wall=8, gb_free=16.8, wall=2348
2022-05-28 12:23:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 245.69 MiB free; 21.79 GiB reserved in total by PyTorch)
2022-05-28 12:23:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 129       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21958 MB |   21958 MB |  175252 GB |  175231 GB |
|       from large pool |   21915 MB |   21915 MB |  171348 GB |  171326 GB |
|       from small pool |      42 MB |      73 MB |    3904 GB |    3904 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21958 MB |   21958 MB |  175252 GB |  175231 GB |
|       from large pool |   21915 MB |   21915 MB |  171348 GB |  171326 GB |
|       from small pool |      42 MB |      73 MB |    3904 GB |    3904 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22314 MB |   22468 MB |  713710 MB |  691396 MB |
|       from large pool |   22270 MB |   22270 MB |  699184 MB |  676914 MB |
|       from small pool |      44 MB |     198 MB |   14526 MB |   14482 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  364104 KB |    5452 MB |  229531 GB |  229530 GB |
|       from large pool |  362687 KB |    5449 MB |  224926 GB |  224925 GB |
|       from small pool |    1417 KB |       7 MB |    4604 GB |    4604 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   46106 K  |   46105 K  |
|       from large pool |     214    |     214    |   21797 K  |   21796 K  |
|       from small pool |     302    |     466    |   24309 K  |   24308 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   46106 K  |   46105 K  |
|       from large pool |     214    |     214    |   21797 K  |   21796 K  |
|       from small pool |     302    |     466    |   24309 K  |   24308 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     132    |    7631    |    7576    |
|       from large pool |      33    |      33    |     368    |     335    |
|       from small pool |      22    |      99    |    7263    |    7241    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      33    |   23365 K  |   23365 K  |
|       from large pool |      21    |      22    |   12288 K  |   12288 K  |
|       from small pool |       9    |      19    |   11076 K  |   11076 K  |
|===========================================================================|

2022-05-28 12:23:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:23:37 | INFO | train_inner | epoch 005:   4728 / 6032 loss=5.317, nll_loss=3.707, ppl=13.06, wps=33097.1, ups=11.58, wpb=2857.2, bsz=128, num_updates=28800, lr=0.000260875, gnorm=0.904, loss_scale=16, train_wall=8, gb_free=20.2, wall=2356
2022-05-28 12:23:45 | INFO | train_inner | epoch 005:   4828 / 6032 loss=5.375, nll_loss=3.776, ppl=13.7, wps=31806, ups=12.69, wpb=2506.7, bsz=128, num_updates=28900, lr=0.000260423, gnorm=0.941, loss_scale=16, train_wall=8, gb_free=19.2, wall=2364
2022-05-28 12:23:53 | INFO | train_inner | epoch 005:   4928 / 6032 loss=5.411, nll_loss=3.815, ppl=14.07, wps=31743.5, ups=12.18, wpb=2606.6, bsz=128, num_updates=29000, lr=0.000259973, gnorm=0.93, loss_scale=16, train_wall=8, gb_free=20.3, wall=2372
2022-05-28 12:24:01 | INFO | train_inner | epoch 005:   5028 / 6032 loss=5.481, nll_loss=3.895, ppl=14.87, wps=35481.7, ups=12.18, wpb=2913.5, bsz=128, num_updates=29100, lr=0.000259526, gnorm=0.916, loss_scale=16, train_wall=8, gb_free=20.7, wall=2381
2022-05-28 12:24:09 | INFO | train_inner | epoch 005:   5128 / 6032 loss=5.234, nll_loss=3.614, ppl=12.24, wps=32873.2, ups=13.1, wpb=2510.3, bsz=128, num_updates=29200, lr=0.000259082, gnorm=0.96, loss_scale=16, train_wall=7, gb_free=15.9, wall=2388
2022-05-28 12:24:17 | INFO | train_inner | epoch 005:   5228 / 6032 loss=5.342, nll_loss=3.735, ppl=13.32, wps=35493.6, ups=12.68, wpb=2798.6, bsz=128, num_updates=29300, lr=0.000258639, gnorm=0.878, loss_scale=16, train_wall=8, gb_free=19.2, wall=2396
2022-05-28 12:24:25 | INFO | train_inner | epoch 005:   5328 / 6032 loss=5.281, nll_loss=3.667, ppl=12.7, wps=32543.6, ups=12.53, wpb=2598, bsz=128, num_updates=29400, lr=0.000258199, gnorm=0.918, loss_scale=16, train_wall=8, gb_free=19.3, wall=2404
2022-05-28 12:24:33 | INFO | train_inner | epoch 005:   5428 / 6032 loss=5.455, nll_loss=3.867, ppl=14.59, wps=34766.6, ups=12.58, wpb=2763, bsz=128, num_updates=29500, lr=0.000257761, gnorm=0.913, loss_scale=16, train_wall=8, gb_free=20.4, wall=2412
2022-05-28 12:24:41 | INFO | train_inner | epoch 005:   5528 / 6032 loss=5.32, nll_loss=3.712, ppl=13.1, wps=33200, ups=12.77, wpb=2600.1, bsz=128, num_updates=29600, lr=0.000257325, gnorm=0.931, loss_scale=16, train_wall=8, gb_free=19.8, wall=2420
2022-05-28 12:24:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 619.69 MiB free; 21.43 GiB reserved in total by PyTorch)
2022-05-28 12:24:47 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 132       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13956 MB |   18278 MB |  180659 GB |  180646 GB |
|       from large pool |   13917 MB |   18239 MB |  176635 GB |  176621 GB |
|       from small pool |      39 MB |      73 MB |    4024 GB |    4024 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13956 MB |   18278 MB |  180659 GB |  180646 GB |
|       from large pool |   13917 MB |   18239 MB |  176635 GB |  176621 GB |
|       from small pool |      39 MB |      73 MB |    4024 GB |    4024 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21940 MB |   21940 MB |  725288 MB |  703348 MB |
|       from large pool |   21900 MB |   21900 MB |  710442 MB |  688542 MB |
|       from small pool |      40 MB |     202 MB |   14846 MB |   14806 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3661 MB |    3661 MB |  235050 GB |  235046 GB |
|       from large pool |    3660 MB |    3660 MB |  230303 GB |  230299 GB |
|       from small pool |       0 MB |      16 MB |    4747 GB |    4747 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   47527 K  |   47527 K  |
|       from large pool |     271    |     275    |   22466 K  |   22466 K  |
|       from small pool |     321    |     466    |   25060 K  |   25060 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   47527 K  |   47527 K  |
|       from large pool |     271    |     275    |   22466 K  |   22466 K  |
|       from small pool |     321    |     466    |   25060 K  |   25060 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     132    |    7794    |    7743    |
|       from large pool |      31    |      31    |     371    |     340    |
|       from small pool |      20    |     101    |    7423    |    7403    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      64    |   24088 K  |   24088 K  |
|       from large pool |      24    |      24    |   12667 K  |   12667 K  |
|       from small pool |       8    |      53    |   11420 K  |   11420 K  |
|===========================================================================|

2022-05-28 12:24:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:24:49 | INFO | train_inner | epoch 005:   5629 / 6032 loss=5.312, nll_loss=3.704, ppl=13.03, wps=31460.1, ups=12.01, wpb=2620, bsz=128, num_updates=29700, lr=0.000256892, gnorm=0.974, loss_scale=16, train_wall=8, gb_free=20.1, wall=2428
2022-05-28 12:24:57 | INFO | train_inner | epoch 005:   5729 / 6032 loss=5.211, nll_loss=3.587, ppl=12.02, wps=31511, ups=12.65, wpb=2491.3, bsz=128, num_updates=29800, lr=0.00025646, gnorm=0.948, loss_scale=16, train_wall=8, gb_free=19.7, wall=2436
2022-05-28 12:25:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 959.69 MiB free; 21.09 GiB reserved in total by PyTorch)
2022-05-28 12:25:00 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 133       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12954 MB |   16935 MB |  181606 GB |  181594 GB |
|       from large pool |   12915 MB |   16895 MB |  177562 GB |  177550 GB |
|       from small pool |      39 MB |      73 MB |    4044 GB |    4044 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12954 MB |   16935 MB |  181606 GB |  181594 GB |
|       from large pool |   12915 MB |   16895 MB |  177562 GB |  177550 GB |
|       from small pool |      39 MB |      73 MB |    4044 GB |    4044 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21600 MB |   21756 MB |  729426 MB |  707826 MB |
|       from large pool |   21558 MB |   21558 MB |  714422 MB |  692864 MB |
|       from small pool |      42 MB |     198 MB |   15004 MB |   14962 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4665 MB |    4665 MB |  236038 GB |  236033 GB |
|       from large pool |    4662 MB |    4662 MB |  231267 GB |  231262 GB |
|       from small pool |       2 MB |       5 MB |    4770 GB |    4770 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   47779 K  |   47778 K  |
|       from large pool |     271    |     275    |   22587 K  |   22586 K  |
|       from small pool |     321    |     466    |   25192 K  |   25191 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   47779 K  |   47778 K  |
|       from large pool |     271    |     275    |   22587 K  |   22586 K  |
|       from small pool |     321    |     466    |   25192 K  |   25191 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     130    |    7874    |    7822    |
|       from large pool |      31    |      31    |     372    |     341    |
|       from small pool |      21    |      99    |    7502    |    7481    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      35    |   24216 K  |   24216 K  |
|       from large pool |      21    |      21    |   12735 K  |   12735 K  |
|       from small pool |       9    |      18    |   11480 K  |   11480 K  |
|===========================================================================|

2022-05-28 12:25:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:25:05 | INFO | train_inner | epoch 005:   5830 / 6032 loss=5.374, nll_loss=3.774, ppl=13.68, wps=32353.1, ups=12.02, wpb=2690.6, bsz=128, num_updates=29900, lr=0.000256031, gnorm=0.928, loss_scale=16, train_wall=8, gb_free=19.4, wall=2444
2022-05-28 12:25:13 | INFO | train_inner | epoch 005:   5930 / 6032 loss=5.205, nll_loss=3.581, ppl=11.97, wps=33630.8, ups=13.1, wpb=2568, bsz=128, num_updates=30000, lr=0.000255604, gnorm=0.919, loss_scale=16, train_wall=7, gb_free=19.8, wall=2452
2022-05-28 12:25:21 | INFO | train_inner | epoch 005:   6030 / 6032 loss=5.441, nll_loss=3.85, ppl=14.42, wps=35870.4, ups=12.49, wpb=2871.3, bsz=128, num_updates=30100, lr=0.000255179, gnorm=0.893, loss_scale=16, train_wall=8, gb_free=20, wall=2460
2022-05-28 12:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:25:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.461 | nll_loss 3.824 | ppl 14.16 | wps 103338 | wpb 2687.4 | bsz 128 | num_updates 30102 | best_loss 5.461
2022-05-28 12:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 30102 updates
2022-05-28 12:25:41 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint5.pt
2022-05-28 12:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint5.pt
2022-05-28 12:25:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint5.pt (epoch 5 @ 30102 updates, score 5.461) (writing took 10.162272515241057 seconds)
2022-05-28 12:25:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-05-28 12:25:51 | INFO | train | epoch 005 | loss 5.313 | nll_loss 3.7 | ppl 13 | wps 31328.6 | ups 11.72 | wpb 2673.5 | bsz 128 | num_updates 30102 | lr 0.00025517 | gnorm 0.932 | loss_scale 16 | train_wall 468 | gb_free 20 | wall 2490
2022-05-28 12:25:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:25:51 | INFO | fairseq.trainer | begin training epoch 6
2022-05-28 12:25:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:25:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 2.17 GiB free; 19.86 GiB reserved in total by PyTorch)
2022-05-28 12:25:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 136       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13632 MB |   17562 MB |  184581 GB |  184568 GB |
|       from large pool |   13592 MB |   17523 MB |  180475 GB |  180461 GB |
|       from small pool |      39 MB |      73 MB |    4106 GB |    4106 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13632 MB |   17562 MB |  184581 GB |  184568 GB |
|       from large pool |   13592 MB |   17523 MB |  180475 GB |  180461 GB |
|       from small pool |      39 MB |      73 MB |    4106 GB |    4106 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20334 MB |   20434 MB |  748156 MB |  727822 MB |
|       from large pool |   20292 MB |   20292 MB |  732794 MB |  712502 MB |
|       from small pool |      42 MB |     142 MB |   15362 MB |   15320 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2769 MB |    6656 MB |  238866 GB |  238863 GB |
|       from large pool |    2767 MB |    6653 MB |  234025 GB |  234022 GB |
|       from small pool |       2 MB |       6 MB |    4840 GB |    4840 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   48539 K  |   48538 K  |
|       from large pool |     271    |     275    |   22968 K  |   22968 K  |
|       from small pool |     321    |     466    |   25570 K  |   25570 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   48539 K  |   48538 K  |
|       from large pool |     271    |     275    |   22968 K  |   22968 K  |
|       from small pool |     321    |     466    |   25570 K  |   25570 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |      93    |    8057    |    8014    |
|       from large pool |      22    |      22    |     376    |     354    |
|       from small pool |      21    |      71    |    7681    |    7660    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      37    |   24594 K  |   24594 K  |
|       from large pool |      17    |      19    |   12953 K  |   12953 K  |
|       from small pool |      10    |      21    |   11640 K  |   11640 K  |
|===========================================================================|

2022-05-28 12:25:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:25:59 | INFO | train_inner | epoch 006:     99 / 6032 loss=5.29, nll_loss=3.67, ppl=12.73, wps=7654.7, ups=2.6, wpb=2947.1, bsz=128, num_updates=30200, lr=0.000254756, gnorm=0.913, loss_scale=16, train_wall=8, gb_free=19.6, wall=2499
2022-05-28 12:26:08 | INFO | train_inner | epoch 006:    199 / 6032 loss=5.061, nll_loss=3.41, ppl=10.63, wps=32958.7, ups=12.43, wpb=2651.6, bsz=128, num_updates=30300, lr=0.000254335, gnorm=0.937, loss_scale=16, train_wall=8, gb_free=20, wall=2507
2022-05-28 12:26:15 | INFO | train_inner | epoch 006:    299 / 6032 loss=5.116, nll_loss=3.473, ppl=11.1, wps=33597.8, ups=12.68, wpb=2649.2, bsz=128, num_updates=30400, lr=0.000253917, gnorm=0.916, loss_scale=16, train_wall=8, gb_free=19.9, wall=2515
2022-05-28 12:26:23 | INFO | train_inner | epoch 006:    399 / 6032 loss=5.056, nll_loss=3.403, ppl=10.58, wps=33063.1, ups=12.82, wpb=2578.1, bsz=128, num_updates=30500, lr=0.0002535, gnorm=0.901, loss_scale=16, train_wall=8, gb_free=20.6, wall=2522
2022-05-28 12:26:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 11.76 GiB already allocated; 3.27 GiB free; 18.76 GiB reserved in total by PyTorch)
2022-05-28 12:26:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 137       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12039 MB |   12066 MB |  187029 GB |  187017 GB |
|       from large pool |   11999 MB |   12026 MB |  182871 GB |  182859 GB |
|       from small pool |      39 MB |      73 MB |    4157 GB |    4157 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12039 MB |   12066 MB |  187029 GB |  187017 GB |
|       from large pool |   11999 MB |   12026 MB |  182871 GB |  182859 GB |
|       from small pool |      39 MB |      73 MB |    4157 GB |    4157 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19210 MB |   19370 MB |  751124 MB |  731914 MB |
|       from large pool |   19168 MB |   19168 MB |  735602 MB |  716434 MB |
|       from small pool |      42 MB |     202 MB |   15522 MB |   15480 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    7170 MB |    8120 MB |  242511 GB |  242504 GB |
|       from large pool |    7168 MB |    8116 MB |  237610 GB |  237603 GB |
|       from small pool |       2 MB |       5 MB |    4901 GB |    4901 GB |
|---------------------------------------------------------------------------|
| Allocations           |     590    |     593    |   49170 K  |   49170 K  |
|       from large pool |     273    |     275    |   23268 K  |   23268 K  |
|       from small pool |     317    |     466    |   25902 K  |   25901 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     590    |     593    |   49170 K  |   49170 K  |
|       from large pool |     273    |     275    |   23268 K  |   23268 K  |
|       from small pool |     317    |     466    |   25902 K  |   25901 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     123    |    8138    |    8095    |
|       from large pool |      22    |      22    |     377    |     355    |
|       from small pool |      21    |     101    |    7761    |    7740    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      31    |   24915 K  |   24915 K  |
|       from large pool |      11    |      14    |   13122 K  |   13122 K  |
|       from small pool |      10    |      18    |   11793 K  |   11793 K  |
|===========================================================================|

2022-05-28 12:26:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:26:32 | INFO | train_inner | epoch 006:    500 / 6032 loss=5.256, nll_loss=3.63, ppl=12.38, wps=34167.6, ups=11.74, wpb=2910.7, bsz=128, num_updates=30600, lr=0.000253086, gnorm=0.932, loss_scale=16, train_wall=8, gb_free=19.1, wall=2531
2022-05-28 12:26:40 | INFO | train_inner | epoch 006:    600 / 6032 loss=5.13, nll_loss=3.486, ppl=11.21, wps=33318.5, ups=12.47, wpb=2672.1, bsz=128, num_updates=30700, lr=0.000252673, gnorm=0.93, loss_scale=16, train_wall=8, gb_free=20.4, wall=2539
2022-05-28 12:26:48 | INFO | train_inner | epoch 006:    700 / 6032 loss=5.113, nll_loss=3.468, ppl=11.06, wps=34711, ups=12.13, wpb=2861.7, bsz=128, num_updates=30800, lr=0.000252262, gnorm=0.892, loss_scale=16, train_wall=8, gb_free=19.6, wall=2547
2022-05-28 12:26:56 | INFO | train_inner | epoch 006:    800 / 6032 loss=5.045, nll_loss=3.391, ppl=10.49, wps=33486.1, ups=12.93, wpb=2589.3, bsz=128, num_updates=30900, lr=0.000251854, gnorm=0.898, loss_scale=16, train_wall=8, gb_free=18.7, wall=2555
2022-05-28 12:27:04 | INFO | train_inner | epoch 006:    900 / 6032 loss=5.182, nll_loss=3.546, ppl=11.68, wps=33981.1, ups=12.5, wpb=2718, bsz=128, num_updates=31000, lr=0.000251447, gnorm=0.929, loss_scale=16, train_wall=8, gb_free=20.5, wall=2563
2022-05-28 12:27:12 | INFO | train_inner | epoch 006:   1000 / 6032 loss=5.199, nll_loss=3.566, ppl=11.84, wps=33797.1, ups=12.23, wpb=2764.1, bsz=128, num_updates=31100, lr=0.000251043, gnorm=0.954, loss_scale=16, train_wall=8, gb_free=19.6, wall=2571
2022-05-28 12:27:20 | INFO | train_inner | epoch 006:   1100 / 6032 loss=5.19, nll_loss=3.556, ppl=11.76, wps=34514.9, ups=12.58, wpb=2744.6, bsz=128, num_updates=31200, lr=0.00025064, gnorm=0.929, loss_scale=16, train_wall=8, gb_free=18.2, wall=2579
2022-05-28 12:27:28 | INFO | train_inner | epoch 006:   1200 / 6032 loss=5.217, nll_loss=3.588, ppl=12.03, wps=33842.8, ups=12.31, wpb=2748.9, bsz=128, num_updates=31300, lr=0.00025024, gnorm=0.929, loss_scale=16, train_wall=8, gb_free=20.5, wall=2587
2022-05-28 12:27:36 | INFO | train_inner | epoch 006:   1300 / 6032 loss=5.153, nll_loss=3.514, ppl=11.42, wps=33456.6, ups=12.53, wpb=2669.1, bsz=128, num_updates=31400, lr=0.000249841, gnorm=0.917, loss_scale=16, train_wall=8, gb_free=17.9, wall=2595
2022-05-28 12:27:44 | INFO | train_inner | epoch 006:   1400 / 6032 loss=5.092, nll_loss=3.446, ppl=10.89, wps=31781.8, ups=12.44, wpb=2555.7, bsz=128, num_updates=31500, lr=0.000249444, gnorm=0.922, loss_scale=16, train_wall=8, gb_free=16.1, wall=2603
2022-05-28 12:27:52 | INFO | train_inner | epoch 006:   1500 / 6032 loss=5.128, nll_loss=3.487, ppl=11.21, wps=33021.2, ups=12.69, wpb=2602.4, bsz=128, num_updates=31600, lr=0.000249049, gnorm=0.953, loss_scale=16, train_wall=8, gb_free=20.6, wall=2611
2022-05-28 12:28:00 | INFO | train_inner | epoch 006:   1600 / 6032 loss=5.138, nll_loss=3.497, ppl=11.29, wps=34326.7, ups=12.69, wpb=2705.3, bsz=128, num_updates=31700, lr=0.000248656, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=18.8, wall=2619
2022-05-28 12:28:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.70 GiB total capacity; 15.42 GiB already allocated; 3.27 GiB free; 18.76 GiB reserved in total by PyTorch)
2022-05-28 12:28:04 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 138       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12221 MB |   15786 MB |  194361 GB |  194349 GB |
|       from large pool |   12182 MB |   15747 MB |  190042 GB |  190030 GB |
|       from small pool |      39 MB |      73 MB |    4319 GB |    4319 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12221 MB |   15786 MB |  194361 GB |  194349 GB |
|       from large pool |   12182 MB |   15747 MB |  190042 GB |  190030 GB |
|       from small pool |      39 MB |      73 MB |    4319 GB |    4319 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19210 MB |   19370 MB |  751284 MB |  732074 MB |
|       from large pool |   19168 MB |   19168 MB |  735602 MB |  716434 MB |
|       from small pool |      42 MB |     202 MB |   15682 MB |   15640 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6988 MB |    6988 MB |  253642 GB |  253635 GB |
|       from large pool |    6985 MB |    6985 MB |  248549 GB |  248543 GB |
|       from small pool |       2 MB |      17 MB |    5092 GB |    5092 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   51098 K  |   51097 K  |
|       from large pool |     271    |     275    |   24178 K  |   24178 K  |
|       from small pool |     321    |     466    |   26919 K  |   26919 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   51098 K  |   51097 K  |
|       from large pool |     271    |     275    |   24178 K  |   24178 K  |
|       from small pool |     321    |     466    |   26919 K  |   26919 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     123    |    8218    |    8175    |
|       from large pool |      22    |      22    |     377    |     355    |
|       from small pool |      21    |     101    |    7841    |    7820    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      45    |   25891 K  |   25891 K  |
|       from large pool |      11    |      17    |   13632 K  |   13632 K  |
|       from small pool |      10    |      31    |   12258 K  |   12258 K  |
|===========================================================================|

2022-05-28 12:28:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:28:08 | INFO | train_inner | epoch 006:   1701 / 6032 loss=5.081, nll_loss=3.433, ppl=10.8, wps=30776.9, ups=12.78, wpb=2408, bsz=128, num_updates=31800, lr=0.000248264, gnorm=0.959, loss_scale=16, train_wall=7, gb_free=20.5, wall=2627
2022-05-28 12:28:15 | INFO | train_inner | epoch 006:   1801 / 6032 loss=5.186, nll_loss=3.552, ppl=11.73, wps=34242.6, ups=12.75, wpb=2684.8, bsz=126.8, num_updates=31900, lr=0.000247875, gnorm=0.943, loss_scale=16, train_wall=8, gb_free=20, wall=2635
2022-05-28 12:28:23 | INFO | train_inner | epoch 006:   1901 / 6032 loss=5.182, nll_loss=3.547, ppl=11.69, wps=34742.4, ups=12.56, wpb=2766.4, bsz=128, num_updates=32000, lr=0.000247487, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=18.8, wall=2643
2022-05-28 12:28:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 13.63 GiB already allocated; 3.27 GiB free; 18.76 GiB reserved in total by PyTorch)
2022-05-28 12:28:31 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 139       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13955 MB |   14003 MB |  196335 GB |  196322 GB |
|       from large pool |   13916 MB |   13963 MB |  191968 GB |  191954 GB |
|       from small pool |      39 MB |      73 MB |    4367 GB |    4367 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13955 MB |   14003 MB |  196335 GB |  196322 GB |
|       from large pool |   13916 MB |   13963 MB |  191968 GB |  191954 GB |
|       from small pool |      39 MB |      73 MB |    4367 GB |    4367 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19210 MB |   19370 MB |  751444 MB |  732234 MB |
|       from large pool |   19168 MB |   19168 MB |  735602 MB |  716434 MB |
|       from small pool |      42 MB |     202 MB |   15842 MB |   15800 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5254 MB |    5516 MB |  256602 GB |  256597 GB |
|       from large pool |    5251 MB |    5512 MB |  251453 GB |  251448 GB |
|       from small pool |       2 MB |       6 MB |    5148 GB |    5148 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   51616 K  |   51616 K  |
|       from large pool |     271    |     275    |   24418 K  |   24417 K  |
|       from small pool |     321    |     466    |   27198 K  |   27198 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   51616 K  |   51616 K  |
|       from large pool |     271    |     275    |   24418 K  |   24417 K  |
|       from small pool |     321    |     466    |   27198 K  |   27198 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     123    |    8298    |    8255    |
|       from large pool |      22    |      22    |     377    |     355    |
|       from small pool |      21    |     101    |    7921    |    7900    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      38    |   26155 K  |   26155 K  |
|       from large pool |      18    |      18    |   13766 K  |   13766 K  |
|       from small pool |      11    |      21    |   12388 K  |   12388 K  |
|===========================================================================|

2022-05-28 12:28:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:28:32 | INFO | train_inner | epoch 006:   2002 / 6032 loss=5.134, nll_loss=3.492, ppl=11.25, wps=32812.8, ups=12.17, wpb=2696.3, bsz=128, num_updates=32100, lr=0.000247102, gnorm=0.923, loss_scale=16, train_wall=8, gb_free=20.7, wall=2651
2022-05-28 12:28:40 | INFO | train_inner | epoch 006:   2102 / 6032 loss=5.179, nll_loss=3.546, ppl=11.68, wps=34473.4, ups=12.45, wpb=2769.8, bsz=128, num_updates=32200, lr=0.000246718, gnorm=0.923, loss_scale=16, train_wall=8, gb_free=18.7, wall=2659
2022-05-28 12:28:47 | INFO | train_inner | epoch 006:   2202 / 6032 loss=5.016, nll_loss=3.36, ppl=10.27, wps=33362.3, ups=13.03, wpb=2560.3, bsz=128, num_updates=32300, lr=0.000246335, gnorm=0.91, loss_scale=16, train_wall=7, gb_free=19.8, wall=2666
2022-05-28 12:28:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 3.27 GiB free; 18.76 GiB reserved in total by PyTorch)
2022-05-28 12:28:48 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 140       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15205 MB |   15256 MB |  197566 GB |  197551 GB |
|       from large pool |   15165 MB |   15216 MB |  193171 GB |  193156 GB |
|       from small pool |      39 MB |      73 MB |    4394 GB |    4394 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15205 MB |   15256 MB |  197566 GB |  197551 GB |
|       from large pool |   15165 MB |   15216 MB |  193171 GB |  193156 GB |
|       from small pool |      39 MB |      73 MB |    4394 GB |    4394 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19210 MB |   19366 MB |  751600 MB |  732390 MB |
|       from large pool |   19168 MB |   19168 MB |  735602 MB |  716434 MB |
|       from small pool |      42 MB |     198 MB |   15998 MB |   15956 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4004 MB |    5451 MB |  258453 GB |  258449 GB |
|       from large pool |    4002 MB |    5448 MB |  253271 GB |  253267 GB |
|       from small pool |       2 MB |      27 MB |    5181 GB |    5181 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   51945 K  |   51945 K  |
|       from large pool |     271    |     275    |   24574 K  |   24573 K  |
|       from small pool |     321    |     466    |   27371 K  |   27371 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   51945 K  |   51945 K  |
|       from large pool |     271    |     275    |   24574 K  |   24573 K  |
|       from small pool |     321    |     466    |   27371 K  |   27371 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     121    |    8376    |    8333    |
|       from large pool |      22    |      22    |     377    |     355    |
|       from small pool |      21    |      99    |    7999    |    7978    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      70    |   26321 K  |   26321 K  |
|       from large pool |      17    |      17    |   13853 K  |   13853 K  |
|       from small pool |       8    |      62    |   12467 K  |   12467 K  |
|===========================================================================|

2022-05-28 12:28:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:28:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.45 GiB already allocated; 523.69 MiB free; 21.52 GiB reserved in total by PyTorch)
2022-05-28 12:28:50 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 141       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21960 MB |   22260 MB |  197740 GB |  197719 GB |
|       from large pool |   21917 MB |   22217 MB |  193338 GB |  193316 GB |
|       from small pool |      42 MB |      73 MB |    4402 GB |    4402 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21960 MB |   22260 MB |  197740 GB |  197719 GB |
|       from large pool |   21917 MB |   22217 MB |  193338 GB |  193316 GB |
|       from small pool |      42 MB |      73 MB |    4402 GB |    4402 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22036 MB |   22496 MB |  754886 MB |  732850 MB |
|       from large pool |   21992 MB |   22292 MB |  738726 MB |  716734 MB |
|       from small pool |      44 MB |     204 MB |   16160 MB |   16116 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   77411 KB |    7484 MB |  258686 GB |  258686 GB |
|       from large pool |   75994 KB |    7483 MB |  253496 GB |  253496 GB |
|       from small pool |    1417 KB |       3 MB |    5189 GB |    5189 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   51999 K  |   51999 K  |
|       from large pool |     214    |     215    |   24594 K  |   24594 K  |
|       from small pool |     302    |     466    |   27404 K  |   27404 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   51999 K  |   51999 K  |
|       from large pool |     214    |     215    |   24594 K  |   24594 K  |
|       from small pool |     302    |     466    |   27404 K  |   27404 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      60    |     141    |    8474    |    8414    |
|       from large pool |      38    |      39    |     394    |     356    |
|       from small pool |      22    |     102    |    8080    |    8058    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      31    |   26349 K  |   26349 K  |
|       from large pool |      19    |      19    |   13865 K  |   13865 K  |
|       from small pool |      12    |      17    |   12483 K  |   12483 K  |
|===========================================================================|

2022-05-28 12:28:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:28:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 527.69 MiB free; 21.52 GiB reserved in total by PyTorch)
2022-05-28 12:28:51 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 142       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12703 MB |   16634 MB |  197796 GB |  197783 GB |
|       from large pool |   12664 MB |   16595 MB |  193393 GB |  193381 GB |
|       from small pool |      39 MB |      73 MB |    4402 GB |    4402 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12703 MB |   16634 MB |  197796 GB |  197783 GB |
|       from large pool |   12664 MB |   16595 MB |  193393 GB |  193381 GB |
|       from small pool |      39 MB |      73 MB |    4402 GB |    4402 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22032 MB |   22086 MB |  754936 MB |  732904 MB |
|       from large pool |   21992 MB |   21992 MB |  738726 MB |  716734 MB |
|       from small pool |      40 MB |      94 MB |   16210 MB |   16170 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1872 MB |    5397 MB |  258757 GB |  258755 GB |
|       from large pool |    1871 MB |    5396 MB |  253567 GB |  253565 GB |
|       from small pool |       0 MB |       4 MB |    5190 GB |    5190 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   52009 K  |   52008 K  |
|       from large pool |     271    |     275    |   24600 K  |   24599 K  |
|       from small pool |     321    |     466    |   27409 K  |   27409 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   52009 K  |   52008 K  |
|       from large pool |     271    |     275    |   24600 K  |   24599 K  |
|       from small pool |     321    |     466    |   27409 K  |   27409 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |      85    |    8499    |    8441    |
|       from large pool |      38    |      38    |     394    |     356    |
|       from small pool |      20    |      47    |    8105    |    8085    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      36    |   26354 K  |   26354 K  |
|       from large pool |      24    |      25    |   13868 K  |   13868 K  |
|       from small pool |      10    |      19    |   12485 K  |   12485 K  |
|===========================================================================|

2022-05-28 12:28:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:28:56 | INFO | train_inner | epoch 006:   2305 / 6032 loss=5.037, nll_loss=3.384, ppl=10.44, wps=28205.8, ups=11.42, wpb=2469.5, bsz=128, num_updates=32400, lr=0.000245955, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=20, wall=2675
2022-05-28 12:29:04 | INFO | train_inner | epoch 006:   2405 / 6032 loss=5.132, nll_loss=3.49, ppl=11.24, wps=34378.3, ups=12.44, wpb=2762.9, bsz=128, num_updates=32500, lr=0.000245576, gnorm=0.916, loss_scale=16, train_wall=8, gb_free=18.4, wall=2683
2022-05-28 12:29:12 | INFO | train_inner | epoch 006:   2505 / 6032 loss=5.042, nll_loss=3.389, ppl=10.48, wps=33264.9, ups=13.01, wpb=2556, bsz=128, num_updates=32600, lr=0.000245199, gnorm=0.926, loss_scale=16, train_wall=7, gb_free=19.2, wall=2691
2022-05-28 12:29:20 | INFO | train_inner | epoch 006:   2605 / 6032 loss=5.065, nll_loss=3.417, ppl=10.68, wps=32824.5, ups=12.88, wpb=2548.1, bsz=128, num_updates=32700, lr=0.000244824, gnorm=0.973, loss_scale=16, train_wall=8, gb_free=20, wall=2699
2022-05-28 12:29:28 | INFO | train_inner | epoch 006:   2705 / 6032 loss=5.164, nll_loss=3.528, ppl=11.54, wps=34258.4, ups=12.29, wpb=2787.7, bsz=128, num_updates=32800, lr=0.000244451, gnorm=0.921, loss_scale=16, train_wall=8, gb_free=19.3, wall=2707
2022-05-28 12:29:36 | INFO | train_inner | epoch 006:   2805 / 6032 loss=5.278, nll_loss=3.659, ppl=12.63, wps=33603.6, ups=11.96, wpb=2808.9, bsz=128, num_updates=32900, lr=0.000244079, gnorm=0.938, loss_scale=16, train_wall=8, gb_free=19.9, wall=2715
2022-05-28 12:29:44 | INFO | train_inner | epoch 006:   2905 / 6032 loss=5.188, nll_loss=3.557, ppl=11.77, wps=34464.8, ups=12.19, wpb=2826.8, bsz=128, num_updates=33000, lr=0.000243709, gnorm=0.94, loss_scale=16, train_wall=8, gb_free=16.1, wall=2723
2022-05-28 12:29:52 | INFO | train_inner | epoch 006:   3005 / 6032 loss=5.021, nll_loss=3.367, ppl=10.32, wps=31100.7, ups=13.44, wpb=2314.9, bsz=128, num_updates=33100, lr=0.00024334, gnorm=0.957, loss_scale=32, train_wall=7, gb_free=16.1, wall=2731
2022-05-28 12:29:59 | INFO | train_inner | epoch 006:   3105 / 6032 loss=5.059, nll_loss=3.41, ppl=10.63, wps=35200, ups=12.97, wpb=2713.7, bsz=128, num_updates=33200, lr=0.000242974, gnorm=0.897, loss_scale=32, train_wall=8, gb_free=20, wall=2739
2022-05-28 12:30:07 | INFO | train_inner | epoch 006:   3205 / 6032 loss=5.186, nll_loss=3.555, ppl=11.76, wps=32360, ups=13.27, wpb=2438, bsz=128, num_updates=33300, lr=0.000242608, gnorm=0.962, loss_scale=32, train_wall=7, gb_free=19.4, wall=2746
2022-05-28 12:30:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 383.69 MiB free; 21.66 GiB reserved in total by PyTorch)
2022-05-28 12:30:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 146       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   17220 MB |  203462 GB |  203449 GB |
|       from large pool |   13250 MB |   17180 MB |  198928 GB |  198915 GB |
|       from small pool |      39 MB |      73 MB |    4533 GB |    4533 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   17220 MB |  203462 GB |  203449 GB |
|       from large pool |   13250 MB |   17180 MB |  198928 GB |  198915 GB |
|       from small pool |      39 MB |      73 MB |    4533 GB |    4533 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22176 MB |   22176 MB |  779274 MB |  757098 MB |
|       from large pool |   22132 MB |   22132 MB |  762590 MB |  740458 MB |
|       from small pool |      44 MB |     202 MB |   16684 MB |   16640 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4954 MB |    4955 MB |  264453 GB |  264448 GB |
|       from large pool |    4949 MB |    4951 MB |  259108 GB |  259103 GB |
|       from small pool |       4 MB |      10 MB |    5345 GB |    5345 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   53523 K  |   53523 K  |
|       from large pool |     271    |     275    |   25309 K  |   25309 K  |
|       from small pool |     321    |     466    |   28214 K  |   28214 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   53523 K  |   53523 K  |
|       from large pool |     271    |     275    |   25309 K  |   25309 K  |
|       from small pool |     321    |     466    |   28214 K  |   28214 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |     140    |    8743    |    8682    |
|       from large pool |      39    |      39    |     401    |     362    |
|       from small pool |      22    |     101    |    8342    |    8320    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      39    |      41    |   27139 K  |   27139 K  |
|       from large pool |      29    |      30    |   14284 K  |   14284 K  |
|       from small pool |      10    |      20    |   12855 K  |   12855 K  |
|===========================================================================|

2022-05-28 12:30:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:30:15 | INFO | train_inner | epoch 006:   3306 / 6032 loss=5.184, nll_loss=3.553, ppl=11.73, wps=32962.9, ups=12.46, wpb=2644.6, bsz=128, num_updates=33400, lr=0.000242245, gnorm=0.952, loss_scale=32, train_wall=8, gb_free=20.3, wall=2754
2022-05-28 12:30:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 12.94 GiB already allocated; 4.23 GiB free; 17.80 GiB reserved in total by PyTorch)
2022-05-28 12:30:21 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 147       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13254 MB |   13286 MB |  204450 GB |  204437 GB |
|       from large pool |   13214 MB |   13245 MB |  199895 GB |  199882 GB |
|       from small pool |      40 MB |      73 MB |    4555 GB |    4555 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13254 MB |   13286 MB |  204450 GB |  204437 GB |
|       from large pool |   13214 MB |   13245 MB |  199895 GB |  199882 GB |
|       from small pool |      40 MB |      73 MB |    4555 GB |    4555 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18224 MB |   18402 MB |  779432 MB |  761208 MB |
|       from large pool |   18180 MB |   18200 MB |  762590 MB |  744410 MB |
|       from small pool |      44 MB |     202 MB |   16842 MB |   16798 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4969 MB |    4969 MB |  265458 GB |  265453 GB |
|       from large pool |    4965 MB |    4965 MB |  260087 GB |  260083 GB |
|       from small pool |       3 MB |       7 MB |    5370 GB |    5370 GB |
|---------------------------------------------------------------------------|
| Allocations           |     590    |     593    |   53782 K  |   53782 K  |
|       from large pool |     273    |     275    |   25431 K  |   25431 K  |
|       from small pool |     317    |     466    |   28350 K  |   28350 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     590    |     593    |   53782 K  |   53782 K  |
|       from large pool |     273    |     275    |   25431 K  |   25431 K  |
|       from small pool |     317    |     466    |   28350 K  |   28350 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      59    |     139    |    8822    |    8763    |
|       from large pool |      37    |      38    |     401    |     364    |
|       from small pool |      22    |     101    |    8421    |    8399    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      44    |   27272 K  |   27272 K  |
|       from large pool |      30    |      32    |   14355 K  |   14355 K  |
|       from small pool |      12    |      19    |   12917 K  |   12917 K  |
|===========================================================================|

2022-05-28 12:30:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:30:23 | INFO | train_inner | epoch 006:   3407 / 6032 loss=5.281, nll_loss=3.661, ppl=12.65, wps=34358.2, ups=12.54, wpb=2740.8, bsz=128, num_updates=33500, lr=0.000241883, gnorm=0.956, loss_scale=32, train_wall=8, gb_free=20.1, wall=2762
2022-05-28 12:30:31 | INFO | train_inner | epoch 006:   3507 / 6032 loss=5.231, nll_loss=3.606, ppl=12.18, wps=34221, ups=12.85, wpb=2662.7, bsz=128, num_updates=33600, lr=0.000241523, gnorm=0.941, loss_scale=32, train_wall=8, gb_free=20.8, wall=2770
2022-05-28 12:30:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.51 GiB already allocated; 231.69 MiB free; 21.80 GiB reserved in total by PyTorch)
2022-05-28 12:30:33 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 64           |        cudaMalloc retries: 148       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13823 MB |   17925 MB |  205355 GB |  205342 GB |
|       from large pool |   13783 MB |   17885 MB |  200778 GB |  200764 GB |
|       from small pool |      39 MB |      73 MB |    4577 GB |    4577 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13823 MB |   17925 MB |  205355 GB |  205342 GB |
|       from large pool |   13783 MB |   17885 MB |  200778 GB |  200764 GB |
|       from small pool |      39 MB |      73 MB |    4577 GB |    4577 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22328 MB |   22484 MB |  783692 MB |  761364 MB |
|       from large pool |   22282 MB |   22282 MB |  766692 MB |  744410 MB |
|       from small pool |      46 MB |     202 MB |   17000 MB |   16954 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4402 MB |    4402 MB |  266388 GB |  266384 GB |
|       from large pool |    4396 MB |    4396 MB |  260992 GB |  260987 GB |
|       from small pool |       6 MB |      18 MB |    5396 GB |    5396 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   54018 K  |   54017 K  |
|       from large pool |     271    |     275    |   25540 K  |   25540 K  |
|       from small pool |     321    |     466    |   28477 K  |   28477 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   54018 K  |   54017 K  |
|       from large pool |     271    |     275    |   25540 K  |   25540 K  |
|       from small pool |     321    |     466    |   28477 K  |   28477 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |     139    |    8902    |    8841    |
|       from large pool |      38    |      38    |     402    |     364    |
|       from small pool |      23    |     101    |    8500    |    8477    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      63    |   27394 K  |   27394 K  |
|       from large pool |      28    |      28    |   14418 K  |   14418 K  |
|       from small pool |      12    |      49    |   12976 K  |   12976 K  |
|===========================================================================|

2022-05-28 12:30:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:30:39 | INFO | train_inner | epoch 006:   3608 / 6032 loss=5.134, nll_loss=3.497, ppl=11.29, wps=32740.4, ups=12.53, wpb=2613.9, bsz=128, num_updates=33700, lr=0.000241164, gnorm=0.95, loss_scale=32, train_wall=8, gb_free=18.8, wall=2778
2022-05-28 12:30:47 | INFO | train_inner | epoch 006:   3708 / 6032 loss=5.202, nll_loss=3.571, ppl=11.89, wps=35116, ups=12.63, wpb=2780.1, bsz=128, num_updates=33800, lr=0.000240807, gnorm=0.93, loss_scale=32, train_wall=8, gb_free=20.2, wall=2786
2022-05-28 12:30:54 | INFO | train_inner | epoch 006:   3808 / 6032 loss=5.073, nll_loss=3.427, ppl=10.76, wps=30467.3, ups=12.94, wpb=2353.9, bsz=128, num_updates=33900, lr=0.000240452, gnorm=0.965, loss_scale=32, train_wall=8, gb_free=20.1, wall=2793
2022-05-28 12:30:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 113.69 MiB free; 21.92 GiB reserved in total by PyTorch)
2022-05-28 12:30:59 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 65           |        cudaMalloc retries: 151       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14035 MB |   18406 MB |  207231 GB |  207217 GB |
|       from large pool |   13996 MB |   18366 MB |  202612 GB |  202599 GB |
|       from small pool |      39 MB |      73 MB |    4618 GB |    4618 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14035 MB |   18406 MB |  207231 GB |  207217 GB |
|       from large pool |   13996 MB |   18366 MB |  202612 GB |  202599 GB |
|       from small pool |      39 MB |      73 MB |    4618 GB |    4618 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22446 MB |   22446 MB |     776 GB |  772256 MB |
|       from large pool |   22406 MB |   22406 MB |     759 GB |  754984 MB |
|       from small pool |      40 MB |     198 MB |      16 GB |   17272 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4038 MB |    4039 MB |  268287 GB |  268283 GB |
|       from large pool |    4037 MB |    4039 MB |  262842 GB |  262838 GB |
|       from small pool |       0 MB |      10 MB |    5444 GB |    5444 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   54512 K  |   54512 K  |
|       from large pool |     271    |     275    |   25774 K  |   25774 K  |
|       from small pool |     321    |     466    |   28738 K  |   28737 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   54512 K  |   54512 K  |
|       from large pool |     271    |     275    |   25774 K  |   25774 K  |
|       from small pool |     321    |     466    |   28738 K  |   28737 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      58    |     137    |    9061    |    9003    |
|       from large pool |      38    |      38    |     405    |     367    |
|       from small pool |      20    |      99    |    8656    |    8636    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      38    |   27647 K  |   27647 K  |
|       from large pool |      27    |      28    |   14553 K  |   14553 K  |
|       from small pool |       9    |      21    |   13094 K  |   13094 K  |
|===========================================================================|

2022-05-28 12:30:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:31:03 | INFO | train_inner | epoch 006:   3909 / 6032 loss=5.179, nll_loss=3.547, ppl=11.69, wps=34171.1, ups=11.81, wpb=2894.2, bsz=128, num_updates=34000, lr=0.000240098, gnorm=0.894, loss_scale=32, train_wall=8, gb_free=19.8, wall=2802
2022-05-28 12:31:11 | INFO | train_inner | epoch 006:   4009 / 6032 loss=5.266, nll_loss=3.647, ppl=12.53, wps=35707, ups=12.41, wpb=2877.3, bsz=128, num_updates=34100, lr=0.000239746, gnorm=0.933, loss_scale=32, train_wall=8, gb_free=20.1, wall=2810
2022-05-28 12:31:18 | INFO | train_inner | epoch 006:   4109 / 6032 loss=4.988, nll_loss=3.331, ppl=10.06, wps=32572.7, ups=13.13, wpb=2480.6, bsz=128, num_updates=34200, lr=0.000239395, gnorm=0.933, loss_scale=32, train_wall=7, gb_free=19.1, wall=2818
2022-05-28 12:31:26 | INFO | train_inner | epoch 006:   4209 / 6032 loss=5.192, nll_loss=3.562, ppl=11.81, wps=34851.4, ups=13.01, wpb=2678.7, bsz=128, num_updates=34300, lr=0.000239046, gnorm=0.94, loss_scale=32, train_wall=7, gb_free=19.8, wall=2825
2022-05-28 12:31:34 | INFO | train_inner | epoch 006:   4309 / 6032 loss=5.3, nll_loss=3.686, ppl=12.87, wps=34875.2, ups=12.68, wpb=2750.8, bsz=128, num_updates=34400, lr=0.000238698, gnorm=0.944, loss_scale=32, train_wall=8, gb_free=18.4, wall=2833
2022-05-28 12:31:42 | INFO | train_inner | epoch 006:   4409 / 6032 loss=5.217, nll_loss=3.591, ppl=12.05, wps=35029.1, ups=12.57, wpb=2787.7, bsz=128, num_updates=34500, lr=0.000238352, gnorm=0.939, loss_scale=32, train_wall=8, gb_free=18.7, wall=2841
2022-05-28 12:31:50 | INFO | train_inner | epoch 006:   4509 / 6032 loss=5.143, nll_loss=3.506, ppl=11.36, wps=34000.7, ups=12.78, wpb=2661.4, bsz=128, num_updates=34600, lr=0.000238007, gnorm=0.916, loss_scale=32, train_wall=8, gb_free=19.8, wall=2849
2022-05-28 12:31:58 | INFO | train_inner | epoch 006:   4609 / 6032 loss=5.054, nll_loss=3.406, ppl=10.6, wps=32535.2, ups=12.92, wpb=2518.9, bsz=128, num_updates=34700, lr=0.000237664, gnorm=0.94, loss_scale=32, train_wall=8, gb_free=16.2, wall=2857
2022-05-28 12:32:06 | INFO | train_inner | epoch 006:   4709 / 6032 loss=5.213, nll_loss=3.587, ppl=12.02, wps=34576, ups=12.32, wpb=2806.2, bsz=128, num_updates=34800, lr=0.000237322, gnorm=0.924, loss_scale=32, train_wall=8, gb_free=14.5, wall=2865
2022-05-28 12:32:14 | INFO | train_inner | epoch 006:   4809 / 6032 loss=5.167, nll_loss=3.534, ppl=11.58, wps=33244.1, ups=11.98, wpb=2774.4, bsz=128, num_updates=34900, lr=0.000236982, gnorm=0.928, loss_scale=32, train_wall=8, gb_free=13.2, wall=2873
2022-05-28 12:32:22 | INFO | train_inner | epoch 006:   4909 / 6032 loss=5.136, nll_loss=3.499, ppl=11.31, wps=35205.7, ups=12.51, wpb=2814.2, bsz=128, num_updates=35000, lr=0.000236643, gnorm=0.924, loss_scale=32, train_wall=8, gb_free=20.5, wall=2881
2022-05-28 12:32:30 | INFO | train_inner | epoch 006:   5009 / 6032 loss=4.86, nll_loss=3.186, ppl=9.1, wps=29611.9, ups=13, wpb=2277.7, bsz=128, num_updates=35100, lr=0.000236306, gnorm=0.944, loss_scale=32, train_wall=7, gb_free=17.5, wall=2889
2022-05-28 12:32:38 | INFO | train_inner | epoch 006:   5109 / 6032 loss=5.319, nll_loss=3.709, ppl=13.08, wps=34601.8, ups=12.33, wpb=2806.7, bsz=128, num_updates=35200, lr=0.00023597, gnorm=0.957, loss_scale=32, train_wall=8, gb_free=18.6, wall=2897
2022-05-28 12:32:46 | INFO | train_inner | epoch 006:   5209 / 6032 loss=5.123, nll_loss=3.486, ppl=11.2, wps=31410.1, ups=12.9, wpb=2435.5, bsz=128, num_updates=35300, lr=0.000235635, gnorm=0.972, loss_scale=32, train_wall=8, gb_free=17.4, wall=2905
2022-05-28 12:32:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 355.69 MiB free; 21.68 GiB reserved in total by PyTorch)
2022-05-28 12:32:48 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 66           |        cudaMalloc retries: 154       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12954 MB |   16934 MB |  215326 GB |  215313 GB |
|       from large pool |   12914 MB |   16894 MB |  210530 GB |  210517 GB |
|       from small pool |      39 MB |      73 MB |    4796 GB |    4796 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12954 MB |   16934 MB |  215326 GB |  215313 GB |
|       from large pool |   12914 MB |   16894 MB |  210530 GB |  210517 GB |
|       from small pool |      39 MB |      73 MB |    4796 GB |    4796 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22204 MB |   22204 MB |     790 GB |     769 GB |
|       from large pool |   22160 MB |   22160 MB |     773 GB |     752 GB |
|       from small pool |      44 MB |     202 MB |      17 GB |      17 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5269 MB |    6324 MB |  276539 GB |  276534 GB |
|       from large pool |    5265 MB |    6320 MB |  270883 GB |  270878 GB |
|       from small pool |       4 MB |       7 MB |    5655 GB |    5655 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   56658 K  |   56658 K  |
|       from large pool |     271    |     275    |   26789 K  |   26788 K  |
|       from small pool |     321    |     466    |   29869 K  |   29869 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   56658 K  |   56658 K  |
|       from large pool |     271    |     275    |   26789 K  |   26788 K  |
|       from small pool |     321    |     466    |   29869 K  |   29869 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      60    |     139    |    9227    |    9167    |
|       from large pool |      38    |      38    |     409    |     371    |
|       from small pool |      22    |     101    |    8818    |    8796    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      46    |   28752 K  |   28752 K  |
|       from large pool |      30    |      30    |   15139 K  |   15139 K  |
|       from small pool |      12    |      19    |   13613 K  |   13612 K  |
|===========================================================================|

2022-05-28 12:32:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:32:54 | INFO | train_inner | epoch 006:   5310 / 6032 loss=5.267, nll_loss=3.648, ppl=12.54, wps=33056.2, ups=11.85, wpb=2790.4, bsz=128, num_updates=35400, lr=0.000235302, gnorm=0.938, loss_scale=32, train_wall=8, gb_free=20.1, wall=2913
2022-05-28 12:33:02 | INFO | train_inner | epoch 006:   5410 / 6032 loss=5.282, nll_loss=3.665, ppl=12.69, wps=36458.9, ups=12.44, wpb=2930.2, bsz=128, num_updates=35500, lr=0.000234971, gnorm=0.922, loss_scale=32, train_wall=8, gb_free=18.7, wall=2921
2022-05-28 12:33:10 | INFO | train_inner | epoch 006:   5510 / 6032 loss=5.091, nll_loss=3.449, ppl=10.92, wps=32974.6, ups=12.23, wpb=2695.5, bsz=128, num_updates=35600, lr=0.000234641, gnorm=0.931, loss_scale=32, train_wall=8, gb_free=19.6, wall=2929
2022-05-28 12:33:18 | INFO | train_inner | epoch 006:   5610 / 6032 loss=5.075, nll_loss=3.432, ppl=10.79, wps=32652.2, ups=13.04, wpb=2503.9, bsz=128, num_updates=35700, lr=0.000234312, gnorm=0.944, loss_scale=32, train_wall=7, gb_free=20.6, wall=2937
2022-05-28 12:33:26 | INFO | train_inner | epoch 006:   5710 / 6032 loss=5.164, nll_loss=3.533, ppl=11.57, wps=33095.4, ups=12.93, wpb=2559, bsz=128, num_updates=35800, lr=0.000233984, gnorm=0.955, loss_scale=32, train_wall=8, gb_free=19.6, wall=2945
2022-05-28 12:33:33 | INFO | train_inner | epoch 006:   5810 / 6032 loss=5.176, nll_loss=3.546, ppl=11.68, wps=33772.1, ups=12.93, wpb=2611.7, bsz=128, num_updates=35900, lr=0.000233658, gnorm=0.927, loss_scale=32, train_wall=8, gb_free=19.1, wall=2953
2022-05-28 12:33:42 | INFO | train_inner | epoch 006:   5910 / 6032 loss=5.257, nll_loss=3.638, ppl=12.45, wps=34436.2, ups=12.02, wpb=2865.5, bsz=128, num_updates=36000, lr=0.000233333, gnorm=0.926, loss_scale=32, train_wall=8, gb_free=20.3, wall=2961
2022-05-28 12:33:49 | INFO | train_inner | epoch 006:   6010 / 6032 loss=5.144, nll_loss=3.51, ppl=11.4, wps=34390.1, ups=12.9, wpb=2665.7, bsz=128, num_updates=36100, lr=0.00023301, gnorm=0.914, loss_scale=32, train_wall=8, gb_free=20, wall=2969
2022-05-28 12:33:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:34:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.395 | nll_loss 3.743 | ppl 13.39 | wps 105747 | wpb 2687.4 | bsz 128 | num_updates 36122 | best_loss 5.395
2022-05-28 12:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 36122 updates
2022-05-28 12:34:10 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint6.pt
2022-05-28 12:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint6.pt
2022-05-28 12:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint6.pt (epoch 6 @ 36122 updates, score 5.395) (writing took 10.003645821940154 seconds)
2022-05-28 12:34:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-05-28 12:34:20 | INFO | train | epoch 006 | loss 5.154 | nll_loss 3.517 | ppl 11.45 | wps 31601.3 | ups 11.82 | wpb 2674.2 | bsz 128 | num_updates 36122 | lr 0.000232939 | gnorm 0.934 | loss_scale 32 | train_wall 465 | gb_free 18.9 | wall 3000
2022-05-28 12:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:34:20 | INFO | fairseq.trainer | begin training epoch 7
2022-05-28 12:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:34:27 | INFO | train_inner | epoch 007:     78 / 6032 loss=4.867, nll_loss=3.188, ppl=9.11, wps=6731.6, ups=2.69, wpb=2503.4, bsz=128, num_updates=36200, lr=0.000232688, gnorm=0.903, loss_scale=32, train_wall=8, gb_free=19.9, wall=3006
2022-05-28 12:34:35 | INFO | train_inner | epoch 007:    178 / 6032 loss=4.9, nll_loss=3.224, ppl=9.34, wps=33375.4, ups=12.56, wpb=2656.7, bsz=128, num_updates=36300, lr=0.000232367, gnorm=0.911, loss_scale=32, train_wall=8, gb_free=20.4, wall=3014
2022-05-28 12:34:43 | INFO | train_inner | epoch 007:    278 / 6032 loss=5.025, nll_loss=3.365, ppl=10.3, wps=34751.6, ups=12.59, wpb=2761, bsz=128, num_updates=36400, lr=0.000232048, gnorm=0.896, loss_scale=32, train_wall=8, gb_free=21, wall=3022
2022-05-28 12:34:50 | INFO | train_inner | epoch 007:    378 / 6032 loss=4.999, nll_loss=3.337, ppl=10.11, wps=33879.4, ups=12.9, wpb=2627.1, bsz=128, num_updates=36500, lr=0.00023173, gnorm=0.942, loss_scale=32, train_wall=8, gb_free=20.7, wall=3029
2022-05-28 12:34:58 | INFO | train_inner | epoch 007:    478 / 6032 loss=4.988, nll_loss=3.324, ppl=10.01, wps=33110.5, ups=12.94, wpb=2558.3, bsz=128, num_updates=36600, lr=0.000231413, gnorm=0.939, loss_scale=32, train_wall=8, gb_free=21, wall=3037
2022-05-28 12:35:06 | INFO | train_inner | epoch 007:    578 / 6032 loss=4.977, nll_loss=3.311, ppl=9.92, wps=34053, ups=12.62, wpb=2698.7, bsz=128, num_updates=36700, lr=0.000231097, gnorm=0.907, loss_scale=32, train_wall=8, gb_free=18.3, wall=3045
2022-05-28 12:35:14 | INFO | train_inner | epoch 007:    678 / 6032 loss=4.81, nll_loss=3.121, ppl=8.7, wps=32845.5, ups=13.19, wpb=2489.9, bsz=128, num_updates=36800, lr=0.000230783, gnorm=0.92, loss_scale=32, train_wall=7, gb_free=20.1, wall=3053
2022-05-28 12:35:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-05-28 12:35:22 | INFO | train_inner | epoch 007:    779 / 6032 loss=4.991, nll_loss=3.328, ppl=10.04, wps=29512.5, ups=12.42, wpb=2375.9, bsz=128, num_updates=36900, lr=0.00023047, gnorm=0.986, loss_scale=16, train_wall=8, gb_free=19.2, wall=3061
2022-05-28 12:35:30 | INFO | train_inner | epoch 007:    879 / 6032 loss=4.924, nll_loss=3.25, ppl=9.52, wps=33681.4, ups=12.56, wpb=2682.4, bsz=128, num_updates=37000, lr=0.000230159, gnorm=0.918, loss_scale=16, train_wall=8, gb_free=19.4, wall=3069
2022-05-28 12:35:37 | INFO | train_inner | epoch 007:    979 / 6032 loss=4.939, nll_loss=3.268, ppl=9.64, wps=34422, ups=12.86, wpb=2676.5, bsz=128, num_updates=37100, lr=0.000229848, gnorm=0.915, loss_scale=16, train_wall=8, gb_free=20.4, wall=3076
2022-05-28 12:35:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.46 GiB (GPU 0; 23.70 GiB total capacity; 15.28 GiB already allocated; 3.38 GiB free; 18.65 GiB reserved in total by PyTorch)
2022-05-28 12:35:38 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 67           |        cudaMalloc retries: 158       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12108 MB |   15648 MB |  227008 GB |  226996 GB |
|       from large pool |   12069 MB |   15609 MB |  221949 GB |  221938 GB |
|       from small pool |      39 MB |      73 MB |    5058 GB |    5058 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12108 MB |   15648 MB |  227008 GB |  226996 GB |
|       from large pool |   12069 MB |   15609 MB |  221949 GB |  221938 GB |
|       from small pool |      39 MB |      73 MB |    5058 GB |    5058 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19102 MB |   19262 MB |     811 GB |     792 GB |
|       from large pool |   19060 MB |   19060 MB |     793 GB |     774 GB |
|       from small pool |      42 MB |     202 MB |      17 GB |      17 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6993 MB |    6993 MB |  295614 GB |  295608 GB |
|       from large pool |    6990 MB |    6990 MB |  289650 GB |  289643 GB |
|       from small pool |       2 MB |       5 MB |    5964 GB |    5964 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   59747 K  |   59747 K  |
|       from large pool |     271    |     275    |   28262 K  |   28262 K  |
|       from small pool |     321    |     466    |   31484 K  |   31484 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   59747 K  |   59747 K  |
|       from large pool |     271    |     275    |   28262 K  |   28262 K  |
|       from small pool |     321    |     466    |   31484 K  |   31484 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     126    |    9531    |    9485    |
|       from large pool |      25    |      25    |     414    |     389    |
|       from small pool |      21    |     101    |    9117    |    9096    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      33    |   30339 K  |   30339 K  |
|       from large pool |      18    |      18    |   15988 K  |   15988 K  |
|       from small pool |      10    |      20    |   14351 K  |   14351 K  |
|===========================================================================|

2022-05-28 12:35:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:35:46 | INFO | train_inner | epoch 007:   1080 / 6032 loss=5.046, nll_loss=3.389, ppl=10.48, wps=33499.8, ups=12.27, wpb=2730.3, bsz=128, num_updates=37200, lr=0.000229539, gnorm=0.945, loss_scale=16, train_wall=8, gb_free=20.4, wall=3085
2022-05-28 12:35:53 | INFO | train_inner | epoch 007:   1180 / 6032 loss=4.965, nll_loss=3.299, ppl=9.84, wps=33060.6, ups=12.94, wpb=2554.5, bsz=128, num_updates=37300, lr=0.000229231, gnorm=0.919, loss_scale=16, train_wall=7, gb_free=19, wall=3092
2022-05-28 12:36:01 | INFO | train_inner | epoch 007:   1280 / 6032 loss=5.016, nll_loss=3.356, ppl=10.24, wps=32726.9, ups=12.84, wpb=2548.4, bsz=128, num_updates=37400, lr=0.000228924, gnorm=0.93, loss_scale=16, train_wall=8, gb_free=18.3, wall=3100
2022-05-28 12:36:09 | INFO | train_inner | epoch 007:   1380 / 6032 loss=5.036, nll_loss=3.379, ppl=10.4, wps=35260.4, ups=12.69, wpb=2777.8, bsz=128, num_updates=37500, lr=0.000228619, gnorm=0.964, loss_scale=16, train_wall=8, gb_free=19.9, wall=3108
2022-05-28 12:36:17 | INFO | train_inner | epoch 007:   1480 / 6032 loss=4.993, nll_loss=3.329, ppl=10.05, wps=34114.3, ups=12.91, wpb=2642.2, bsz=128, num_updates=37600, lr=0.000228315, gnorm=0.936, loss_scale=16, train_wall=8, gb_free=15.9, wall=3116
2022-05-28 12:36:24 | INFO | train_inner | epoch 007:   1580 / 6032 loss=5.005, nll_loss=3.345, ppl=10.16, wps=34230.7, ups=12.78, wpb=2679, bsz=128, num_updates=37700, lr=0.000228012, gnorm=0.958, loss_scale=16, train_wall=8, gb_free=20.7, wall=3124
2022-05-28 12:36:32 | INFO | train_inner | epoch 007:   1680 / 6032 loss=5.094, nll_loss=3.445, ppl=10.89, wps=35411.3, ups=12.56, wpb=2819.7, bsz=128, num_updates=37800, lr=0.00022771, gnorm=0.929, loss_scale=16, train_wall=8, gb_free=13.3, wall=3132
2022-05-28 12:36:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.70 GiB total capacity; 15.42 GiB already allocated; 3.37 GiB free; 18.66 GiB reserved in total by PyTorch)
2022-05-28 12:36:39 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 68           |        cudaMalloc retries: 159       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12222 MB |   15787 MB |  231585 GB |  231573 GB |
|       from large pool |   12183 MB |   15748 MB |  226427 GB |  226415 GB |
|       from small pool |      39 MB |      73 MB |    5157 GB |    5157 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12222 MB |   15787 MB |  231585 GB |  231573 GB |
|       from large pool |   12183 MB |   15748 MB |  226427 GB |  226415 GB |
|       from small pool |      39 MB |      73 MB |    5157 GB |    5157 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19104 MB |   19262 MB |     811 GB |     792 GB |
|       from large pool |   19060 MB |   19060 MB |     793 GB |     774 GB |
|       from small pool |      44 MB |     202 MB |      17 GB |      17 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6881 MB |    6881 MB |  304388 GB |  304381 GB |
|       from large pool |    6876 MB |    6876 MB |  298305 GB |  298298 GB |
|       from small pool |       4 MB |      21 MB |    6082 GB |    6082 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   60965 K  |   60964 K  |
|       from large pool |     271    |     275    |   28840 K  |   28840 K  |
|       from small pool |     321    |     466    |   32124 K  |   32123 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   60965 K  |   60964 K  |
|       from large pool |     271    |     275    |   28840 K  |   28840 K  |
|       from small pool |     321    |     466    |   32124 K  |   32123 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     126    |    9611    |    9564    |
|       from large pool |      25    |      25    |     414    |     389    |
|       from small pool |      22    |     101    |    9197    |    9175    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      68    |   30960 K  |   30960 K  |
|       from large pool |      11    |      16    |   16315 K  |   16315 K  |
|       from small pool |      12    |      53    |   14644 K  |   14644 K  |
|===========================================================================|

2022-05-28 12:36:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:36:41 | INFO | train_inner | epoch 007:   1781 / 6032 loss=5.077, nll_loss=3.426, ppl=10.75, wps=34180.5, ups=12.31, wpb=2775.8, bsz=128, num_updates=37900, lr=0.000227409, gnorm=0.956, loss_scale=16, train_wall=8, gb_free=20.7, wall=3140
2022-05-28 12:36:48 | INFO | train_inner | epoch 007:   1881 / 6032 loss=5.061, nll_loss=3.408, ppl=10.61, wps=35025.7, ups=12.71, wpb=2756.2, bsz=128, num_updates=38000, lr=0.00022711, gnorm=0.936, loss_scale=16, train_wall=8, gb_free=18.8, wall=3148
2022-05-28 12:36:57 | INFO | train_inner | epoch 007:   1981 / 6032 loss=5.121, nll_loss=3.476, ppl=11.13, wps=33775.3, ups=12.36, wpb=2731.6, bsz=128, num_updates=38100, lr=0.000226812, gnorm=0.978, loss_scale=16, train_wall=8, gb_free=18, wall=3156
2022-05-28 12:37:04 | INFO | train_inner | epoch 007:   2081 / 6032 loss=4.956, nll_loss=3.289, ppl=9.77, wps=33683.2, ups=12.73, wpb=2645.5, bsz=128, num_updates=38200, lr=0.000226515, gnorm=0.96, loss_scale=16, train_wall=8, gb_free=21, wall=3164
2022-05-28 12:37:13 | INFO | train_inner | epoch 007:   2181 / 6032 loss=5.093, nll_loss=3.445, ppl=10.89, wps=34162.5, ups=12.06, wpb=2832.7, bsz=128, num_updates=38300, lr=0.000226219, gnorm=0.933, loss_scale=16, train_wall=8, gb_free=20.2, wall=3172
2022-05-28 12:37:21 | INFO | train_inner | epoch 007:   2281 / 6032 loss=4.884, nll_loss=3.207, ppl=9.23, wps=32874.7, ups=12.69, wpb=2591, bsz=128, num_updates=38400, lr=0.000225924, gnorm=0.933, loss_scale=16, train_wall=8, gb_free=20.7, wall=3180
2022-05-28 12:37:28 | INFO | train_inner | epoch 007:   2381 / 6032 loss=4.95, nll_loss=3.283, ppl=9.73, wps=33647.4, ups=12.63, wpb=2664.3, bsz=128, num_updates=38500, lr=0.00022563, gnorm=0.94, loss_scale=16, train_wall=8, gb_free=18.5, wall=3188
2022-05-28 12:37:36 | INFO | train_inner | epoch 007:   2481 / 6032 loss=4.883, nll_loss=3.206, ppl=9.23, wps=31882.6, ups=12.98, wpb=2456.4, bsz=128, num_updates=38600, lr=0.000225338, gnorm=0.95, loss_scale=16, train_wall=8, gb_free=19.6, wall=3195
2022-05-28 12:37:44 | INFO | train_inner | epoch 007:   2581 / 6032 loss=5.073, nll_loss=3.421, ppl=10.71, wps=34214.3, ups=12.83, wpb=2665.9, bsz=128, num_updates=38700, lr=0.000225047, gnorm=0.956, loss_scale=16, train_wall=8, gb_free=18.8, wall=3203
2022-05-28 12:37:52 | INFO | train_inner | epoch 007:   2681 / 6032 loss=5.09, nll_loss=3.442, ppl=10.87, wps=32729.9, ups=12.7, wpb=2576.5, bsz=128, num_updates=38800, lr=0.000224756, gnorm=0.969, loss_scale=16, train_wall=8, gb_free=19.4, wall=3211
2022-05-28 12:38:00 | INFO | train_inner | epoch 007:   2781 / 6032 loss=4.945, nll_loss=3.278, ppl=9.7, wps=31898, ups=12.71, wpb=2510.3, bsz=128, num_updates=38900, lr=0.000224467, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=20.7, wall=3219
2022-05-28 12:38:07 | INFO | train_inner | epoch 007:   2881 / 6032 loss=4.989, nll_loss=3.326, ppl=10.03, wps=34225.2, ups=12.9, wpb=2653.8, bsz=128, num_updates=39000, lr=0.000224179, gnorm=0.921, loss_scale=16, train_wall=8, gb_free=19.3, wall=3227
2022-05-28 12:38:16 | INFO | train_inner | epoch 007:   2981 / 6032 loss=5.005, nll_loss=3.345, ppl=10.16, wps=33722.2, ups=12.28, wpb=2745.4, bsz=128, num_updates=39100, lr=0.000223893, gnorm=0.932, loss_scale=16, train_wall=8, gb_free=19.2, wall=3235
2022-05-28 12:38:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.39 GiB (GPU 0; 23.70 GiB total capacity; 14.76 GiB already allocated; 3.38 GiB free; 18.65 GiB reserved in total by PyTorch)
2022-05-28 12:38:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 69           |        cudaMalloc retries: 160       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   11645 MB |   15112 MB |  239266 GB |  239255 GB |
|       from large pool |   11606 MB |   15073 MB |  233928 GB |  233917 GB |
|       from small pool |      38 MB |      73 MB |    5337 GB |    5337 GB |
|---------------------------------------------------------------------------|
| Active memory         |   11645 MB |   15112 MB |  239266 GB |  239255 GB |
|       from large pool |   11606 MB |   15073 MB |  233928 GB |  233917 GB |
|       from small pool |      38 MB |      73 MB |    5337 GB |    5337 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19102 MB |   19264 MB |     811 GB |     792 GB |
|       from large pool |   19060 MB |   19060 MB |     793 GB |     774 GB |
|       from small pool |      42 MB |     204 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    7456 MB |    7456 MB |  319041 GB |  319034 GB |
|       from large pool |    7453 MB |    7453 MB |  312747 GB |  312739 GB |
|       from small pool |       3 MB |       8 MB |    6294 GB |    6294 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   63002 K  |   63001 K  |
|       from large pool |     271    |     275    |   29792 K  |   29792 K  |
|       from small pool |     321    |     466    |   33209 K  |   33209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   63002 K  |   63001 K  |
|       from large pool |     271    |     275    |   29792 K  |   29792 K  |
|       from small pool |     321    |     466    |   33209 K  |   33209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     127    |    9691    |    9645    |
|       from large pool |      25    |      25    |     414    |     389    |
|       from small pool |      21    |     102    |    9277    |    9256    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      36    |   31998 K  |   31998 K  |
|       from large pool |      17    |      17    |   16853 K  |   16853 K  |
|       from small pool |      11    |      20    |   15144 K  |   15144 K  |
|===========================================================================|

2022-05-28 12:38:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:38:24 | INFO | train_inner | epoch 007:   3082 / 6032 loss=5.077, nll_loss=3.428, ppl=10.76, wps=33549.3, ups=11.99, wpb=2797, bsz=128, num_updates=39200, lr=0.000223607, gnorm=0.931, loss_scale=16, train_wall=8, gb_free=18.7, wall=3243
2022-05-28 12:38:32 | INFO | train_inner | epoch 007:   3182 / 6032 loss=4.844, nll_loss=3.163, ppl=8.96, wps=31167.4, ups=12.95, wpb=2407.1, bsz=128, num_updates=39300, lr=0.000223322, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=20.3, wall=3251
2022-05-28 12:38:40 | INFO | train_inner | epoch 007:   3282 / 6032 loss=5.167, nll_loss=3.53, ppl=11.55, wps=35290, ups=12.48, wpb=2827.7, bsz=128, num_updates=39400, lr=0.000223039, gnorm=0.953, loss_scale=16, train_wall=8, gb_free=20.5, wall=3259
2022-05-28 12:38:47 | INFO | train_inner | epoch 007:   3382 / 6032 loss=4.957, nll_loss=3.292, ppl=9.79, wps=31897.2, ups=13.26, wpb=2406.1, bsz=128, num_updates=39500, lr=0.000222756, gnorm=0.977, loss_scale=16, train_wall=7, gb_free=19, wall=3266
2022-05-28 12:38:55 | INFO | train_inner | epoch 007:   3482 / 6032 loss=5.133, nll_loss=3.492, ppl=11.25, wps=34360.2, ups=12.16, wpb=2825.4, bsz=128, num_updates=39600, lr=0.000222475, gnorm=0.94, loss_scale=16, train_wall=8, gb_free=19.9, wall=3275
2022-05-28 12:39:04 | INFO | train_inner | epoch 007:   3582 / 6032 loss=5.007, nll_loss=3.349, ppl=10.19, wps=32733.3, ups=12.31, wpb=2658.5, bsz=128, num_updates=39700, lr=0.000222194, gnorm=0.96, loss_scale=16, train_wall=8, gb_free=18.5, wall=3283
2022-05-28 12:39:12 | INFO | train_inner | epoch 007:   3682 / 6032 loss=4.984, nll_loss=3.323, ppl=10, wps=33919.3, ups=12.59, wpb=2694.4, bsz=128, num_updates=39800, lr=0.000221915, gnorm=0.954, loss_scale=16, train_wall=8, gb_free=20.5, wall=3291
2022-05-28 12:39:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.62 GiB (GPU 0; 23.70 GiB total capacity; 15.67 GiB already allocated; 3.38 GiB free; 18.65 GiB reserved in total by PyTorch)
2022-05-28 12:39:14 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 70           |        cudaMalloc retries: 161       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12329 MB |   16041 MB |  242948 GB |  242936 GB |
|       from large pool |   12290 MB |   16001 MB |  237528 GB |  237516 GB |
|       from small pool |      39 MB |      73 MB |    5419 GB |    5419 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12329 MB |   16041 MB |  242948 GB |  242936 GB |
|       from large pool |   12290 MB |   16001 MB |  237528 GB |  237516 GB |
|       from small pool |      39 MB |      73 MB |    5419 GB |    5419 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19102 MB |   19262 MB |     811 GB |     792 GB |
|       from large pool |   19060 MB |   19060 MB |     793 GB |     774 GB |
|       from small pool |      42 MB |     202 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6772 MB |    6772 MB |  326123 GB |  326117 GB |
|       from large pool |    6769 MB |    6769 MB |  319731 GB |  319724 GB |
|       from small pool |       2 MB |       6 MB |    6392 GB |    6392 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   63991 K  |   63990 K  |
|       from large pool |     271    |     275    |   30258 K  |   30258 K  |
|       from small pool |     321    |     466    |   33732 K  |   33731 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   63991 K  |   63990 K  |
|       from large pool |     271    |     275    |   30258 K  |   30258 K  |
|       from small pool |     321    |     466    |   33732 K  |   33731 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     126    |    9771    |    9725    |
|       from large pool |      25    |      25    |     414    |     389    |
|       from small pool |      21    |     101    |    9357    |    9336    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      38    |   32503 K  |   32502 K  |
|       from large pool |      14    |      17    |   17119 K  |   17119 K  |
|       from small pool |      10    |      24    |   15383 K  |   15383 K  |
|===========================================================================|

2022-05-28 12:39:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:39:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 13.63 GiB already allocated; 3.38 GiB free; 18.65 GiB reserved in total by PyTorch)
2022-05-28 12:39:16 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 71           |        cudaMalloc retries: 162       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13957 MB |   14005 MB |  243149 GB |  243136 GB |
|       from large pool |   13918 MB |   13965 MB |  237725 GB |  237712 GB |
|       from small pool |      39 MB |      73 MB |    5424 GB |    5424 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13957 MB |   14005 MB |  243149 GB |  243136 GB |
|       from large pool |   13918 MB |   13965 MB |  237725 GB |  237712 GB |
|       from small pool |      39 MB |      73 MB |    5424 GB |    5424 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19100 MB |   19244 MB |     811 GB |     792 GB |
|       from large pool |   19060 MB |   19060 MB |     793 GB |     774 GB |
|       from small pool |      40 MB |     184 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5142 MB |    5466 MB |  326506 GB |  326501 GB |
|       from large pool |    5141 MB |    5465 MB |  320108 GB |  320103 GB |
|       from small pool |       0 MB |      17 MB |    6397 GB |    6397 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   64042 K  |   64042 K  |
|       from large pool |     271    |     275    |   30283 K  |   30283 K  |
|       from small pool |     321    |     466    |   33759 K  |   33759 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   64042 K  |   64042 K  |
|       from large pool |     271    |     275    |   30283 K  |   30283 K  |
|       from small pool |     321    |     466    |   33759 K  |   33759 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     117    |    9842    |    9797    |
|       from large pool |      25    |      25    |     414    |     389    |
|       from small pool |      20    |      92    |    9428    |    9408    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      66    |   32529 K  |   32528 K  |
|       from large pool |      15    |      21    |   17133 K  |   17133 K  |
|       from small pool |       9    |      49    |   15395 K  |   15395 K  |
|===========================================================================|

2022-05-28 12:39:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:39:20 | INFO | train_inner | epoch 007:   3784 / 6032 loss=4.997, nll_loss=3.337, ppl=10.11, wps=31287.6, ups=12.2, wpb=2565.6, bsz=128, num_updates=39900, lr=0.000221637, gnorm=0.965, loss_scale=16, train_wall=8, gb_free=19.8, wall=3299
2022-05-28 12:39:28 | INFO | train_inner | epoch 007:   3884 / 6032 loss=5.105, nll_loss=3.461, ppl=11.01, wps=34129.6, ups=12.37, wpb=2759.7, bsz=128, num_updates=40000, lr=0.000221359, gnorm=0.946, loss_scale=16, train_wall=8, gb_free=20.2, wall=3307
2022-05-28 12:39:36 | INFO | train_inner | epoch 007:   3984 / 6032 loss=5.143, nll_loss=3.503, ppl=11.34, wps=36296.8, ups=12.15, wpb=2986.4, bsz=128, num_updates=40100, lr=0.000221083, gnorm=0.926, loss_scale=16, train_wall=8, gb_free=20.9, wall=3315
2022-05-28 12:39:44 | INFO | train_inner | epoch 007:   4084 / 6032 loss=5.034, nll_loss=3.38, ppl=10.41, wps=33346.1, ups=12.98, wpb=2568.6, bsz=128, num_updates=40200, lr=0.000220808, gnorm=0.958, loss_scale=16, train_wall=7, gb_free=19.1, wall=3323
2022-05-28 12:39:52 | INFO | train_inner | epoch 007:   4184 / 6032 loss=4.991, nll_loss=3.33, ppl=10.06, wps=32206.5, ups=12.82, wpb=2513.1, bsz=128, num_updates=40300, lr=0.000220534, gnorm=0.956, loss_scale=16, train_wall=8, gb_free=19.5, wall=3331
2022-05-28 12:39:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 12.94 GiB already allocated; 3.40 GiB free; 18.63 GiB reserved in total by PyTorch)
2022-05-28 12:39:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 72           |        cudaMalloc retries: 163       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13254 MB |   13286 MB |  245793 GB |  245780 GB |
|       from large pool |   13214 MB |   13245 MB |  240309 GB |  240297 GB |
|       from small pool |      40 MB |      73 MB |    5483 GB |    5483 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13254 MB |   13286 MB |  245793 GB |  245780 GB |
|       from large pool |   13214 MB |   13245 MB |  240309 GB |  240297 GB |
|       from small pool |      40 MB |      73 MB |    5483 GB |    5483 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19082 MB |   19262 MB |     811 GB |     793 GB |
|       from large pool |   19040 MB |   19060 MB |     793 GB |     774 GB |
|       from small pool |      42 MB |     202 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5827 MB |    5831 MB |  331474 GB |  331468 GB |
|       from large pool |    5825 MB |    5829 MB |  325006 GB |  325001 GB |
|       from small pool |       1 MB |      17 MB |    6467 GB |    6467 GB |
|---------------------------------------------------------------------------|
| Allocations           |     587    |     590    |   64728 K  |   64728 K  |
|       from large pool |     273    |     275    |   30606 K  |   30606 K  |
|       from small pool |     314    |     466    |   34122 K  |   34122 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     587    |     590    |   64728 K  |   64728 K  |
|       from large pool |     273    |     275    |   30606 K  |   30606 K  |
|       from small pool |     314    |     466    |   34122 K  |   34122 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     126    |    9923    |    9878    |
|       from large pool |      24    |      25    |     414    |     390    |
|       from small pool |      21    |     101    |    9509    |    9488    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      52    |   32878 K  |   32878 K  |
|       from large pool |      17    |      19    |   17315 K  |   17315 K  |
|       from small pool |      11    |      38    |   15563 K  |   15563 K  |
|===========================================================================|

2022-05-28 12:39:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:39:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.68 GiB already allocated; 131.69 MiB free; 21.90 GiB reserved in total by PyTorch)
2022-05-28 12:39:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 73           |        cudaMalloc retries: 165       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17084 MB |   17142 MB |  246139 GB |  246123 GB |
|       from large pool |   17044 MB |   17102 MB |  240648 GB |  240632 GB |
|       from small pool |      39 MB |      73 MB |    5491 GB |    5490 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17084 MB |   17142 MB |  246139 GB |  246123 GB |
|       from large pool |   17044 MB |   17102 MB |  240648 GB |  240632 GB |
|       from small pool |      39 MB |      73 MB |    5491 GB |    5490 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22428 MB |   22476 MB |     815 GB |     793 GB |
|       from large pool |   22386 MB |   22386 MB |     796 GB |     774 GB |
|       from small pool |      42 MB |      90 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5343 MB |    5486 MB |  332113 GB |  332108 GB |
|       from large pool |    5341 MB |    5486 MB |  325636 GB |  325631 GB |
|       from small pool |       2 MB |       4 MB |    6476 GB |    6476 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   64816 K  |   64816 K  |
|       from large pool |     271    |     275    |   30647 K  |   30647 K  |
|       from small pool |     321    |     466    |   34168 K  |   34168 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   64816 K  |   64816 K  |
|       from large pool |     271    |     275    |   30647 K  |   30647 K  |
|       from small pool |     321    |     466    |   34168 K  |   34168 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |      70    |   10024    |    9978    |
|       from large pool |      25    |      25    |     415    |     390    |
|       from small pool |      21    |      45    |    9609    |    9588    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      36    |   32923 K  |   32923 K  |
|       from large pool |      17    |      17    |   17338 K  |   17338 K  |
|       from small pool |      10    |      21    |   15584 K  |   15584 K  |
|===========================================================================|

2022-05-28 12:39:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:40:01 | INFO | train_inner | epoch 007:   4286 / 6032 loss=5.124, nll_loss=3.484, ppl=11.19, wps=28741.4, ups=10.72, wpb=2680.3, bsz=128, num_updates=40400, lr=0.000220261, gnorm=0.963, loss_scale=16, train_wall=9, gb_free=19, wall=3340
2022-05-28 12:40:09 | INFO | train_inner | epoch 007:   4386 / 6032 loss=5.004, nll_loss=3.347, ppl=10.17, wps=33730.2, ups=12.75, wpb=2646.3, bsz=128, num_updates=40500, lr=0.000219989, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=20.8, wall=3348
2022-05-28 12:40:16 | INFO | train_inner | epoch 007:   4486 / 6032 loss=5.073, nll_loss=3.426, ppl=10.74, wps=34866.4, ups=12.83, wpb=2717.6, bsz=128, num_updates=40600, lr=0.000219718, gnorm=0.948, loss_scale=16, train_wall=8, gb_free=20.4, wall=3356
2022-05-28 12:40:25 | INFO | train_inner | epoch 007:   4586 / 6032 loss=5.043, nll_loss=3.391, ppl=10.49, wps=33150.5, ups=12.35, wpb=2685.2, bsz=128, num_updates=40700, lr=0.000219448, gnorm=0.949, loss_scale=16, train_wall=8, gb_free=16.9, wall=3364
2022-05-28 12:40:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 365.69 MiB free; 21.67 GiB reserved in total by PyTorch)
2022-05-28 12:40:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 74           |        cudaMalloc retries: 171       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14033 MB |   18403 MB |  248285 GB |  248271 GB |
|       from large pool |   13994 MB |   18364 MB |  242746 GB |  242733 GB |
|       from small pool |      39 MB |      73 MB |    5538 GB |    5538 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14033 MB |   18403 MB |  248285 GB |  248271 GB |
|       from large pool |   13994 MB |   18364 MB |  242746 GB |  242733 GB |
|       from small pool |      39 MB |      73 MB |    5538 GB |    5538 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22194 MB |   22194 MB |     867 GB |     845 GB |
|       from large pool |   22154 MB |   22154 MB |     847 GB |     826 GB |
|       from small pool |      40 MB |      84 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3788 MB |    3790 MB |  334306 GB |  334302 GB |
|       from large pool |    3787 MB |    3789 MB |  327773 GB |  327769 GB |
|       from small pool |       0 MB |       8 MB |    6532 GB |    6532 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   65370 K  |   65369 K  |
|       from large pool |     271    |     275    |   30906 K  |   30906 K  |
|       from small pool |     321    |     466    |   34463 K  |   34463 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   65370 K  |   65369 K  |
|       from large pool |     271    |     275    |   30906 K  |   30906 K  |
|       from small pool |     321    |     466    |   34463 K  |   34463 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |      68    |   10308    |   10262    |
|       from large pool |      26    |      26    |     443    |     417    |
|       from small pool |      20    |      42    |    9865    |    9845    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      34    |   33201 K  |   33201 K  |
|       from large pool |      18    |      19    |   17482 K  |   17482 K  |
|       from small pool |       9    |      23    |   15718 K  |   15718 K  |
|===========================================================================|

2022-05-28 12:40:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:40:33 | INFO | train_inner | epoch 007:   4687 / 6032 loss=4.967, nll_loss=3.303, ppl=9.87, wps=29976.4, ups=11.98, wpb=2502.7, bsz=128, num_updates=40800, lr=0.000219179, gnorm=0.989, loss_scale=16, train_wall=8, gb_free=18.4, wall=3372
2022-05-28 12:40:41 | INFO | train_inner | epoch 007:   4787 / 6032 loss=5.03, nll_loss=3.378, ppl=10.4, wps=34584.6, ups=13.14, wpb=2632.9, bsz=128, num_updates=40900, lr=0.00021891, gnorm=0.974, loss_scale=16, train_wall=7, gb_free=20.3, wall=3380
2022-05-28 12:40:49 | INFO | train_inner | epoch 007:   4887 / 6032 loss=5.035, nll_loss=3.381, ppl=10.42, wps=36290, ups=12.53, wpb=2896.6, bsz=128, num_updates=41000, lr=0.000218643, gnorm=0.969, loss_scale=16, train_wall=8, gb_free=19.9, wall=3388
2022-05-28 12:40:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 633.69 MiB free; 21.41 GiB reserved in total by PyTorch)
2022-05-28 12:40:49 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 75           |        cudaMalloc retries: 172       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13821 MB |   17923 MB |  250022 GB |  250008 GB |
|       from large pool |   13781 MB |   17883 MB |  244443 GB |  244429 GB |
|       from small pool |      39 MB |      73 MB |    5579 GB |    5579 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13821 MB |   17923 MB |  250022 GB |  250008 GB |
|       from large pool |   13781 MB |   17883 MB |  244443 GB |  244429 GB |
|       from small pool |      39 MB |      73 MB |    5579 GB |    5579 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21926 MB |   22086 MB |     871 GB |     849 GB |
|       from large pool |   21884 MB |   21884 MB |     851 GB |     830 GB |
|       from small pool |      42 MB |     202 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4002 MB |    4002 MB |  336097 GB |  336093 GB |
|       from large pool |    4000 MB |    4000 MB |  329516 GB |  329512 GB |
|       from small pool |       2 MB |      15 MB |    6581 GB |    6581 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   65833 K  |   65832 K  |
|       from large pool |     271    |     275    |   31122 K  |   31122 K  |
|       from small pool |     321    |     466    |   34710 K  |   34710 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   65833 K  |   65832 K  |
|       from large pool |     271    |     275    |   31122 K  |   31122 K  |
|       from small pool |     321    |     466    |   34710 K  |   34710 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     127    |   10390    |   10343    |
|       from large pool |      26    |      26    |     444    |     418    |
|       from small pool |      21    |     101    |    9946    |    9925    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      65    |   33434 K  |   33434 K  |
|       from large pool |      20    |      20    |   17602 K  |   17602 K  |
|       from small pool |      10    |      51    |   15832 K  |   15832 K  |
|===========================================================================|

2022-05-28 12:40:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:40:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 1.62 GiB free; 20.41 GiB reserved in total by PyTorch)
2022-05-28 12:40:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 76           |        cudaMalloc retries: 174       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13632 MB |   17563 MB |  250491 GB |  250478 GB |
|       from large pool |   13593 MB |   17524 MB |  244903 GB |  244890 GB |
|       from small pool |      39 MB |      73 MB |    5587 GB |    5587 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13632 MB |   17563 MB |  250491 GB |  250478 GB |
|       from large pool |   13593 MB |   17524 MB |  244903 GB |  244890 GB |
|       from small pool |      39 MB |      73 MB |    5587 GB |    5587 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20904 MB |   21056 MB |     877 GB |     857 GB |
|       from large pool |   20860 MB |   20860 MB |     857 GB |     837 GB |
|       from small pool |      44 MB |     196 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2899 MB |    3340 MB |  336575 GB |  336572 GB |
|       from large pool |    2894 MB |    3335 MB |  329983 GB |  329980 GB |
|       from small pool |       4 MB |      11 MB |    6591 GB |    6591 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   65945 K  |   65945 K  |
|       from large pool |     271    |     275    |   31175 K  |   31175 K  |
|       from small pool |     321    |     466    |   34769 K  |   34769 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   65945 K  |   65945 K  |
|       from large pool |     271    |     275    |   31175 K  |   31175 K  |
|       from small pool |     321    |     466    |   34769 K  |   34769 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      48    |     124    |   10547    |   10499    |
|       from large pool |      26    |      26    |     446    |     420    |
|       from small pool |      22    |      98    |   10101    |   10079    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      59    |   33491 K  |   33491 K  |
|       from large pool |      20    |      21    |   17632 K  |   17632 K  |
|       from small pool |      15    |      46    |   15858 K  |   15858 K  |
|===========================================================================|

2022-05-28 12:40:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:40:57 | INFO | train_inner | epoch 007:   4989 / 6032 loss=5.138, nll_loss=3.5, ppl=11.32, wps=31234.9, ups=11.41, wpb=2737, bsz=128, num_updates=41100, lr=0.000218377, gnorm=0.96, loss_scale=16, train_wall=8, gb_free=19.6, wall=3396
2022-05-28 12:41:05 | INFO | train_inner | epoch 007:   5089 / 6032 loss=4.951, nll_loss=3.286, ppl=9.76, wps=33476.4, ups=12.93, wpb=2590, bsz=128, num_updates=41200, lr=0.000218112, gnorm=0.967, loss_scale=16, train_wall=8, gb_free=19.7, wall=3404
2022-05-28 12:41:13 | INFO | train_inner | epoch 007:   5189 / 6032 loss=4.939, nll_loss=3.273, ppl=9.67, wps=32791.9, ups=13.34, wpb=2458.5, bsz=128, num_updates=41300, lr=0.000217848, gnorm=0.97, loss_scale=16, train_wall=7, gb_free=17.1, wall=3412
2022-05-28 12:41:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.45 GiB already allocated; 457.69 MiB free; 21.58 GiB reserved in total by PyTorch)
2022-05-28 12:41:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 77           |        cudaMalloc retries: 176       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21960 MB |   22260 MB |  251760 GB |  251739 GB |
|       from large pool |   21918 MB |   22218 MB |  246143 GB |  246122 GB |
|       from small pool |      42 MB |      73 MB |    5617 GB |    5617 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21960 MB |   22260 MB |  251760 GB |  251739 GB |
|       from large pool |   21918 MB |   22218 MB |  246143 GB |  246122 GB |
|       from small pool |      42 MB |      73 MB |    5617 GB |    5617 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22102 MB |   22402 MB |     883 GB |     861 GB |
|       from large pool |   22058 MB |   22358 MB |     863 GB |     841 GB |
|       from small pool |      44 MB |     202 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  144509 KB |    4019 MB |  337875 GB |  337875 GB |
|       from large pool |  143092 KB |    4017 MB |  331249 GB |  331249 GB |
|       from small pool |    1417 KB |       4 MB |    6626 GB |    6626 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   66291 K  |   66291 K  |
|       from large pool |     214    |     215    |   31338 K  |   31338 K  |
|       from small pool |     302    |     466    |   34953 K  |   34953 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   66291 K  |   66291 K  |
|       from large pool |     214    |     215    |   31338 K  |   31338 K  |
|       from small pool |     302    |     466    |   34953 K  |   34953 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |     151    |   10652    |   10580    |
|       from large pool |      50    |      51    |     472    |     422    |
|       from small pool |      22    |     101    |   10180    |   10158    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      39    |      39    |   33667 K  |   33667 K  |
|       from large pool |      27    |      27    |   17723 K  |   17723 K  |
|       from small pool |      12    |      19    |   15943 K  |   15943 K  |
|===========================================================================|

2022-05-28 12:41:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:41:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 19.32 GiB already allocated; 1.97 GiB free; 20.06 GiB reserved in total by PyTorch)
2022-05-28 12:41:15 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 78           |        cudaMalloc retries: 178       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15214 MB |   19781 MB |  251936 GB |  251921 GB |
|       from large pool |   15175 MB |   19741 MB |  246316 GB |  246302 GB |
|       from small pool |      39 MB |      73 MB |    5619 GB |    5619 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15214 MB |   19781 MB |  251936 GB |  251921 GB |
|       from large pool |   15175 MB |   19741 MB |  246316 GB |  246302 GB |
|       from small pool |      39 MB |      73 MB |    5619 GB |    5619 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20538 MB |   22188 MB |     892 GB |     872 GB |
|       from large pool |   20498 MB |   22058 MB |     872 GB |     852 GB |
|       from small pool |      40 MB |     130 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  775242 KB |  775242 KB |  338034 GB |  338033 GB |
|       from large pool |  774812 KB |  774812 KB |  331405 GB |  331404 GB |
|       from small pool |     430 KB |    3202 KB |    6629 GB |    6629 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   66328 K  |   66327 K  |
|       from large pool |     271    |     275    |   31357 K  |   31356 K  |
|       from small pool |     321    |     466    |   34971 K  |   34971 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   66328 K  |   66327 K  |
|       from large pool |     271    |     275    |   31357 K  |   31356 K  |
|       from small pool |     321    |     466    |   34971 K  |   34971 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      66    |     115    |   10697    |   10631    |
|       from large pool |      46    |      50    |     474    |     428    |
|       from small pool |      20    |      65    |   10223    |   10203    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      38    |   33685 K  |   33685 K  |
|       from large pool |      28    |      28    |   17734 K  |   17734 K  |
|       from small pool |       8    |      17    |   15951 K  |   15951 K  |
|===========================================================================|

2022-05-28 12:41:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:41:22 | INFO | train_inner | epoch 007:   5291 / 6032 loss=5.175, nll_loss=3.542, ppl=11.65, wps=33174, ups=10.76, wpb=3082.9, bsz=128, num_updates=41400, lr=0.000217584, gnorm=0.962, loss_scale=16, train_wall=8, gb_free=20.7, wall=3421
2022-05-28 12:41:30 | INFO | train_inner | epoch 007:   5391 / 6032 loss=5.078, nll_loss=3.432, ppl=10.79, wps=33438, ups=12.73, wpb=2625.8, bsz=126.8, num_updates=41500, lr=0.000217322, gnorm=0.977, loss_scale=16, train_wall=8, gb_free=18.1, wall=3429
2022-05-28 12:41:38 | INFO | train_inner | epoch 007:   5491 / 6032 loss=5.247, nll_loss=3.624, ppl=12.33, wps=36866.8, ups=12.44, wpb=2962.4, bsz=128, num_updates=41600, lr=0.000217061, gnorm=0.984, loss_scale=16, train_wall=8, gb_free=20.7, wall=3437
2022-05-28 12:41:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 2.32 GiB free; 19.71 GiB reserved in total by PyTorch)
2022-05-28 12:41:46 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 79           |        cudaMalloc retries: 181       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13290 MB |   17221 MB |  254237 GB |  254224 GB |
|       from large pool |   13251 MB |   17181 MB |  248570 GB |  248557 GB |
|       from small pool |      39 MB |      73 MB |    5666 GB |    5666 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13290 MB |   17221 MB |  254237 GB |  254224 GB |
|       from large pool |   13251 MB |   17181 MB |  248570 GB |  248557 GB |
|       from small pool |      39 MB |      73 MB |    5666 GB |    5666 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20186 MB |   20186 MB |     907 GB |     887 GB |
|       from large pool |   20146 MB |   20146 MB |     887 GB |     867 GB |
|       from small pool |      40 MB |     196 MB |      20 GB |      20 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2963 MB |    2964 MB |  340253 GB |  340250 GB |
|       from large pool |    2962 MB |    2964 MB |  333567 GB |  333564 GB |
|       from small pool |       0 MB |      28 MB |    6685 GB |    6685 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   66909 K  |   66909 K  |
|       from large pool |     271    |     275    |   31632 K  |   31632 K  |
|       from small pool |     321    |     466    |   35276 K  |   35276 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   66909 K  |   66909 K  |
|       from large pool |     271    |     275    |   31632 K  |   31632 K  |
|       from small pool |     321    |     466    |   35276 K  |   35276 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      63    |     141    |   10859    |   10796    |
|       from large pool |      43    |      43    |     479    |     436    |
|       from small pool |      20    |      98    |   10380    |   10360    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      49    |   33984 K  |   33984 K  |
|       from large pool |      32    |      33    |   17893 K  |   17893 K  |
|       from small pool |       8    |      42    |   16091 K  |   16091 K  |
|===========================================================================|

2022-05-28 12:41:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:41:46 | INFO | train_inner | epoch 007:   5592 / 6032 loss=5.074, nll_loss=3.428, ppl=10.76, wps=32154.5, ups=12.01, wpb=2678, bsz=128, num_updates=41700, lr=0.0002168, gnorm=0.948, loss_scale=16, train_wall=8, gb_free=6.9, wall=3445
2022-05-28 12:41:54 | INFO | train_inner | epoch 007:   5692 / 6032 loss=5.065, nll_loss=3.419, ppl=10.69, wps=31295, ups=11.95, wpb=2618.2, bsz=128, num_updates=41800, lr=0.000216541, gnorm=0.946, loss_scale=16, train_wall=8, gb_free=18.4, wall=3454
2022-05-28 12:42:02 | INFO | train_inner | epoch 007:   5792 / 6032 loss=5.125, nll_loss=3.486, ppl=11.2, wps=34437.1, ups=12.37, wpb=2784.7, bsz=128, num_updates=41900, lr=0.000216282, gnorm=0.943, loss_scale=16, train_wall=8, gb_free=20.5, wall=3462
2022-05-28 12:42:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 2.65 GiB free; 19.38 GiB reserved in total by PyTorch)
2022-05-28 12:42:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 80           |        cudaMalloc retries: 185       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12955 MB |   16935 MB |  255613 GB |  255600 GB |
|       from large pool |   12915 MB |   16895 MB |  249918 GB |  249905 GB |
|       from small pool |      39 MB |      73 MB |    5695 GB |    5695 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12955 MB |   16935 MB |  255613 GB |  255600 GB |
|       from large pool |   12915 MB |   16895 MB |  249918 GB |  249905 GB |
|       from small pool |      39 MB |      73 MB |    5695 GB |    5695 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19846 MB |   19846 MB |     926 GB |     906 GB |
|       from large pool |   19806 MB |   19806 MB |     905 GB |     886 GB |
|       from small pool |      40 MB |     196 MB |      20 GB |      20 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2910 MB |    2910 MB |  342112 GB |  342109 GB |
|       from large pool |    2910 MB |    2910 MB |  335392 GB |  335389 GB |
|       from small pool |       0 MB |       4 MB |    6720 GB |    6720 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   67265 K  |   67264 K  |
|       from large pool |     271    |     275    |   31802 K  |   31802 K  |
|       from small pool |     321    |     466    |   35462 K  |   35462 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   67265 K  |   67264 K  |
|       from large pool |     271    |     275    |   31802 K  |   31802 K  |
|       from small pool |     321    |     466    |   35462 K  |   35462 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      62    |     140    |   11100    |   11038    |
|       from large pool |      42    |      42    |     485    |     443    |
|       from small pool |      20    |      98    |   10615    |   10595    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      43    |   34168 K  |   34168 K  |
|       from large pool |      33    |      33    |   17991 K  |   17991 K  |
|       from small pool |       8    |      21    |   16176 K  |   16176 K  |
|===========================================================================|

2022-05-28 12:42:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:42:11 | INFO | train_inner | epoch 007:   5893 / 6032 loss=5.005, nll_loss=3.349, ppl=10.19, wps=32433.9, ups=12.06, wpb=2688.6, bsz=128, num_updates=42000, lr=0.000216025, gnorm=0.917, loss_scale=16, train_wall=8, gb_free=18.7, wall=3470
2022-05-28 12:42:19 | INFO | train_inner | epoch 007:   5993 / 6032 loss=5.057, nll_loss=3.409, ppl=10.62, wps=34695.1, ups=12.79, wpb=2713.2, bsz=128, num_updates=42100, lr=0.000215768, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=18.8, wall=3478
2022-05-28 12:42:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:42:41 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.348 | nll_loss 3.69 | ppl 12.91 | wps 106333 | wpb 2687.4 | bsz 128 | num_updates 42139 | best_loss 5.348
2022-05-28 12:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 42139 updates
2022-05-28 12:42:41 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint7.pt
2022-05-28 12:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint7.pt
2022-05-28 12:42:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint7.pt (epoch 7 @ 42139 updates, score 5.348) (writing took 10.038785089273006 seconds)
2022-05-28 12:42:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-05-28 12:42:51 | INFO | train | epoch 007 | loss 5.025 | nll_loss 3.369 | ppl 10.33 | wps 31467.5 | ups 11.79 | wpb 2670.1 | bsz 128 | num_updates 42139 | lr 0.000215668 | gnorm 0.947 | loss_scale 16 | train_wall 466 | gb_free 19.1 | wall 3510
2022-05-28 12:42:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:42:51 | INFO | fairseq.trainer | begin training epoch 8
2022-05-28 12:42:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:42:56 | INFO | train_inner | epoch 008:     61 / 6032 loss=4.871, nll_loss=3.191, ppl=9.14, wps=7203.1, ups=2.67, wpb=2698.2, bsz=128, num_updates=42200, lr=0.000215512, gnorm=0.934, loss_scale=16, train_wall=8, gb_free=19, wall=3515
2022-05-28 12:43:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.45 GiB already allocated; 487.69 MiB free; 21.55 GiB reserved in total by PyTorch)
2022-05-28 12:43:03 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 81           |        cudaMalloc retries: 187       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21963 MB |   22263 MB |  259051 GB |  259030 GB |
|       from large pool |   21921 MB |   22221 MB |  253285 GB |  253264 GB |
|       from small pool |      42 MB |      73 MB |    5766 GB |    5765 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21963 MB |   22263 MB |  259051 GB |  259030 GB |
|       from large pool |   21921 MB |   22221 MB |  253285 GB |  253264 GB |
|       from small pool |      42 MB |      73 MB |    5766 GB |    5765 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22072 MB |   22530 MB |     940 GB |     919 GB |
|       from large pool |   22028 MB |   22328 MB |     919 GB |     898 GB |
|       from small pool |      44 MB |     202 MB |      21 GB |      20 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  110709 KB |    7503 MB |  347654 GB |  347654 GB |
|       from large pool |  109292 KB |    7501 MB |  340852 GB |  340852 GB |
|       from small pool |    1417 KB |       4 MB |    6801 GB |    6801 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   68149 K  |   68148 K  |
|       from large pool |     214    |     215    |   32245 K  |   32244 K  |
|       from small pool |     302    |     466    |   35903 K  |   35903 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   68149 K  |   68148 K  |
|       from large pool |     214    |     215    |   32245 K  |   32244 K  |
|       from small pool |     302    |     466    |   35903 K  |   35903 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     169    |   11287    |   11198    |
|       from large pool |      67    |      68    |     529    |     462    |
|       from small pool |      22    |     101    |   10758    |   10736    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      47    |   34612 K  |   34612 K  |
|       from large pool |      33    |      33    |   18245 K  |   18245 K  |
|       from small pool |      14    |      16    |   16366 K  |   16366 K  |
|===========================================================================|

2022-05-28 12:43:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:43:04 | INFO | train_inner | epoch 008:    162 / 6032 loss=4.855, nll_loss=3.17, ppl=9, wps=31372.9, ups=11.98, wpb=2618.8, bsz=128, num_updates=42300, lr=0.000215257, gnorm=0.924, loss_scale=16, train_wall=8, gb_free=20.4, wall=3524
2022-05-28 12:43:12 | INFO | train_inner | epoch 008:    262 / 6032 loss=4.816, nll_loss=3.124, ppl=8.72, wps=34255.4, ups=12.51, wpb=2738.2, bsz=128, num_updates=42400, lr=0.000215003, gnorm=0.928, loss_scale=16, train_wall=8, gb_free=20.8, wall=3532
2022-05-28 12:43:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 13.64 GiB already allocated; 2.38 GiB free; 19.65 GiB reserved in total by PyTorch)
2022-05-28 12:43:19 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 82           |        cudaMalloc retries: 188       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13962 MB |   14010 MB |  260204 GB |  260190 GB |
|       from large pool |   13922 MB |   13970 MB |  254412 GB |  254398 GB |
|       from small pool |      39 MB |      73 MB |    5791 GB |    5791 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13962 MB |   14010 MB |  260204 GB |  260190 GB |
|       from large pool |   13922 MB |   13970 MB |  254412 GB |  254398 GB |
|       from small pool |      39 MB |      73 MB |    5791 GB |    5791 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20118 MB |   22230 MB |     941 GB |     921 GB |
|       from large pool |   20078 MB |   22028 MB |     919 GB |     900 GB |
|       from small pool |      40 MB |     202 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6155 MB |    6155 MB |  348864 GB |  348858 GB |
|       from large pool |    6155 MB |    6155 MB |  342031 GB |  342025 GB |
|       from small pool |       0 MB |      10 MB |    6832 GB |    6832 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   68449 K  |   68449 K  |
|       from large pool |     271    |     275    |   32387 K  |   32386 K  |
|       from small pool |     321    |     466    |   36062 K  |   36062 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   68449 K  |   68449 K  |
|       from large pool |     271    |     275    |   32387 K  |   32386 K  |
|       from small pool |     321    |     466    |   36062 K  |   36062 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      84    |     168    |   11366    |   11282    |
|       from large pool |      64    |      67    |     529    |     465    |
|       from small pool |      20    |     101    |   10837    |   10817    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      55    |      57    |   34768 K  |   34768 K  |
|       from large pool |      43    |      43    |   18329 K  |   18329 K  |
|       from small pool |      12    |      39    |   16439 K  |   16439 K  |
|===========================================================================|

2022-05-28 12:43:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:43:20 | INFO | train_inner | epoch 008:    363 / 6032 loss=4.801, nll_loss=3.107, ppl=8.62, wps=33203.4, ups=12.39, wpb=2679.9, bsz=128, num_updates=42500, lr=0.00021475, gnorm=0.916, loss_scale=16, train_wall=8, gb_free=18.8, wall=3540
2022-05-28 12:43:29 | INFO | train_inner | epoch 008:    463 / 6032 loss=4.973, nll_loss=3.303, ppl=9.87, wps=34564.2, ups=11.97, wpb=2887.3, bsz=128, num_updates=42600, lr=0.000214498, gnorm=0.923, loss_scale=16, train_wall=8, gb_free=17.7, wall=3548
2022-05-28 12:43:37 | INFO | train_inner | epoch 008:    563 / 6032 loss=4.789, nll_loss=3.094, ppl=8.54, wps=31017.3, ups=13.02, wpb=2381.9, bsz=128, num_updates=42700, lr=0.000214247, gnorm=0.967, loss_scale=16, train_wall=7, gb_free=18.7, wall=3556
2022-05-28 12:43:44 | INFO | train_inner | epoch 008:    663 / 6032 loss=4.799, nll_loss=3.105, ppl=8.61, wps=33684.5, ups=12.62, wpb=2668.7, bsz=128, num_updates=42800, lr=0.000213996, gnorm=0.919, loss_scale=16, train_wall=8, gb_free=17.9, wall=3564
2022-05-28 12:43:52 | INFO | train_inner | epoch 008:    763 / 6032 loss=4.961, nll_loss=3.291, ppl=9.79, wps=35626.8, ups=12.63, wpb=2819.8, bsz=128, num_updates=42900, lr=0.000213747, gnorm=0.944, loss_scale=16, train_wall=8, gb_free=20.5, wall=3571
2022-05-28 12:44:00 | INFO | train_inner | epoch 008:    863 / 6032 loss=4.872, nll_loss=3.189, ppl=9.12, wps=34434.8, ups=12.82, wpb=2686.1, bsz=128, num_updates=43000, lr=0.000213498, gnorm=0.933, loss_scale=16, train_wall=8, gb_free=18.1, wall=3579
2022-05-28 12:44:08 | INFO | train_inner | epoch 008:    963 / 6032 loss=4.809, nll_loss=3.118, ppl=8.68, wps=34121.8, ups=12.73, wpb=2679.5, bsz=128, num_updates=43100, lr=0.00021325, gnorm=0.967, loss_scale=16, train_wall=8, gb_free=20.4, wall=3587
2022-05-28 12:44:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 1.08 GiB free; 20.95 GiB reserved in total by PyTorch)
2022-05-28 12:44:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 83           |        cudaMalloc retries: 190       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18613 MB |   18676 MB |  263935 GB |  263917 GB |
|       from large pool |   18573 MB |   18636 MB |  258063 GB |  258045 GB |
|       from small pool |      40 MB |      73 MB |    5871 GB |    5871 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18613 MB |   18676 MB |  263935 GB |  263917 GB |
|       from large pool |   18573 MB |   18636 MB |  258063 GB |  258045 GB |
|       from small pool |      40 MB |      73 MB |    5871 GB |    5871 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21450 MB |   21630 MB |     944 GB |     923 GB |
|       from large pool |   21408 MB |   21428 MB |     923 GB |     902 GB |
|       from small pool |      42 MB |     202 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2836 MB |    4843 MB |  355170 GB |  355167 GB |
|       from large pool |    2834 MB |    4842 MB |  348242 GB |  348239 GB |
|       from small pool |       1 MB |      16 MB |    6927 GB |    6927 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   69433 K  |   69432 K  |
|       from large pool |     271    |     275    |   32855 K  |   32854 K  |
|       from small pool |     321    |     466    |   36578 K  |   36578 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   69433 K  |   69432 K  |
|       from large pool |     271    |     275    |   32855 K  |   32854 K  |
|       from small pool |     321    |     466    |   36578 K  |   36578 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     162    |   11519    |   11438    |
|       from large pool |      60    |      61    |     530    |     470    |
|       from small pool |      21    |     101    |   10989    |   10968    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      65    |      67    |   35280 K  |   35280 K  |
|       from large pool |      52    |      52    |   18605 K  |   18605 K  |
|       from small pool |      13    |      48    |   16674 K  |   16674 K  |
|===========================================================================|

2022-05-28 12:44:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:44:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.86 GiB already allocated; 1.08 GiB free; 20.95 GiB reserved in total by PyTorch)
2022-05-28 12:44:14 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 84           |        cudaMalloc retries: 191       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15218 MB |   15270 MB |  264309 GB |  264294 GB |
|       from large pool |   15179 MB |   15230 MB |  258430 GB |  258416 GB |
|       from small pool |      39 MB |      73 MB |    5878 GB |    5878 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15218 MB |   15270 MB |  264309 GB |  264294 GB |
|       from large pool |   15179 MB |   15230 MB |  258430 GB |  258416 GB |
|       from small pool |      39 MB |      73 MB |    5878 GB |    5878 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21450 MB |   21604 MB |     944 GB |     923 GB |
|       from large pool |   21408 MB |   21408 MB |     923 GB |     902 GB |
|       from small pool |      42 MB |     196 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6231 MB |    6231 MB |  355816 GB |  355810 GB |
|       from large pool |    6228 MB |    6228 MB |  348879 GB |  348873 GB |
|       from small pool |       2 MB |      12 MB |    6936 GB |    6936 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   69529 K  |   69528 K  |
|       from large pool |     271    |     275    |   32901 K  |   32901 K  |
|       from small pool |     321    |     466    |   36627 K  |   36627 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   69529 K  |   69528 K  |
|       from large pool |     271    |     275    |   32901 K  |   32901 K  |
|       from small pool |     321    |     466    |   36627 K  |   36627 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     158    |   11596    |   11515    |
|       from large pool |      60    |      60    |     530    |     470    |
|       from small pool |      21    |      98    |   11066    |   11045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      45    |      55    |   35330 K  |   35330 K  |
|       from large pool |      33    |      33    |   18633 K  |   18633 K  |
|       from small pool |      12    |      40    |   16697 K  |   16697 K  |
|===========================================================================|

2022-05-28 12:44:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:44:16 | INFO | train_inner | epoch 008:   1065 / 6032 loss=4.843, nll_loss=3.156, ppl=8.91, wps=31674.3, ups=12.22, wpb=2591.1, bsz=128, num_updates=43200, lr=0.000213003, gnorm=0.947, loss_scale=16, train_wall=8, gb_free=19.7, wall=3595
2022-05-28 12:44:24 | INFO | train_inner | epoch 008:   1165 / 6032 loss=4.718, nll_loss=3.014, ppl=8.08, wps=31685.7, ups=12.87, wpb=2461.3, bsz=128, num_updates=43300, lr=0.000212757, gnorm=0.952, loss_scale=16, train_wall=8, gb_free=19.8, wall=3603
2022-05-28 12:44:32 | INFO | train_inner | epoch 008:   1265 / 6032 loss=4.819, nll_loss=3.129, ppl=8.75, wps=32818.3, ups=12.68, wpb=2587.9, bsz=128, num_updates=43400, lr=0.000212512, gnorm=0.975, loss_scale=16, train_wall=8, gb_free=20.5, wall=3611
2022-05-28 12:44:40 | INFO | train_inner | epoch 008:   1365 / 6032 loss=4.93, nll_loss=3.255, ppl=9.55, wps=36198.7, ups=12.34, wpb=2934.3, bsz=128, num_updates=43500, lr=0.000212267, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=14.3, wall=3619
2022-05-28 12:44:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.55 GiB already allocated; 1.08 GiB free; 20.95 GiB reserved in total by PyTorch)
2022-05-28 12:44:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 85           |        cudaMalloc retries: 192       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12962 MB |   16942 MB |  266380 GB |  266368 GB |
|       from large pool |   12923 MB |   16903 MB |  260453 GB |  260441 GB |
|       from small pool |      39 MB |      73 MB |    5926 GB |    5926 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12962 MB |   16942 MB |  266380 GB |  266368 GB |
|       from large pool |   12923 MB |   16903 MB |  260453 GB |  260441 GB |
|       from small pool |      39 MB |      73 MB |    5926 GB |    5926 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21450 MB |   21610 MB |     944 GB |     923 GB |
|       from large pool |   21408 MB |   21408 MB |     923 GB |     902 GB |
|       from small pool |      42 MB |     202 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1031 MB |    4507 MB |  359411 GB |  359410 GB |
|       from large pool |    1028 MB |    4504 MB |  352418 GB |  352417 GB |
|       from small pool |       2 MB |      17 MB |    6993 GB |    6993 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   70069 K  |   70068 K  |
|       from large pool |     271    |     275    |   33153 K  |   33153 K  |
|       from small pool |     321    |     466    |   36915 K  |   36915 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   70069 K  |   70068 K  |
|       from large pool |     271    |     275    |   33153 K  |   33153 K  |
|       from small pool |     321    |     466    |   36915 K  |   36915 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     161    |   11676    |   11595    |
|       from large pool |      60    |      60    |     530    |     470    |
|       from small pool |      21    |     101    |   11146    |   11125    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      73    |   35610 K  |   35610 K  |
|       from large pool |      39    |      40    |   18780 K  |   18780 K  |
|       from small pool |      10    |      55    |   16830 K  |   16830 K  |
|===========================================================================|

2022-05-28 12:44:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:44:48 | INFO | train_inner | epoch 008:   1466 / 6032 loss=4.866, nll_loss=3.185, ppl=9.09, wps=34343, ups=11.94, wpb=2877.2, bsz=128, num_updates=43600, lr=0.000212024, gnorm=0.972, loss_scale=16, train_wall=8, gb_free=18.2, wall=3627
2022-05-28 12:44:56 | INFO | train_inner | epoch 008:   1566 / 6032 loss=4.76, nll_loss=3.063, ppl=8.35, wps=32921.6, ups=13.02, wpb=2528.6, bsz=128, num_updates=43700, lr=0.000211781, gnorm=0.973, loss_scale=16, train_wall=7, gb_free=18.1, wall=3635
2022-05-28 12:45:04 | INFO | train_inner | epoch 008:   1666 / 6032 loss=4.965, nll_loss=3.298, ppl=9.83, wps=31892.7, ups=12.67, wpb=2517.6, bsz=126.8, num_updates=43800, lr=0.000211539, gnorm=1.047, loss_scale=16, train_wall=8, gb_free=18, wall=3643
2022-05-28 12:45:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 2.55 GiB free; 19.48 GiB reserved in total by PyTorch)
2022-05-28 12:45:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 86           |        cudaMalloc retries: 194       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13631 MB |   17562 MB |  268058 GB |  268045 GB |
|       from large pool |   13592 MB |   17522 MB |  262093 GB |  262080 GB |
|       from small pool |      39 MB |      73 MB |    5964 GB |    5964 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13631 MB |   17562 MB |  268058 GB |  268045 GB |
|       from large pool |   13592 MB |   17522 MB |  262093 GB |  262080 GB |
|       from small pool |      39 MB |      73 MB |    5964 GB |    5964 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19952 MB |   19952 MB |     953 GB |     934 GB |
|       from large pool |   19912 MB |   19912 MB |     931 GB |     912 GB |
|       from small pool |      40 MB |     202 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2388 MB |    2405 MB |  362126 GB |  362124 GB |
|       from large pool |    2387 MB |    2403 MB |  355088 GB |  355086 GB |
|       from small pool |       0 MB |       4 MB |    7037 GB |    7037 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   70519 K  |   70519 K  |
|       from large pool |     271    |     275    |   33365 K  |   33365 K  |
|       from small pool |     321    |     466    |   37154 K  |   37154 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   70519 K  |   70519 K  |
|       from large pool |     271    |     275    |   33365 K  |   33365 K  |
|       from small pool |     321    |     466    |   37154 K  |   37154 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     162    |   11759    |   11678    |
|       from large pool |      61    |      61    |     533    |     472    |
|       from small pool |      20    |     101    |   11226    |   11206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      66    |      68    |   35845 K  |   35845 K  |
|       from large pool |      54    |      55    |   18905 K  |   18905 K  |
|       from small pool |      12    |      21    |   16939 K  |   16939 K  |
|===========================================================================|

2022-05-28 12:45:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:45:12 | INFO | train_inner | epoch 008:   1767 / 6032 loss=5.091, nll_loss=3.44, ppl=10.85, wps=33533.2, ups=11.72, wpb=2861.2, bsz=128, num_updates=43900, lr=0.000211298, gnorm=0.986, loss_scale=16, train_wall=8, gb_free=18.3, wall=3652
2022-05-28 12:45:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 3.19 GiB free; 18.84 GiB reserved in total by PyTorch)
2022-05-28 12:45:16 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 87           |        cudaMalloc retries: 195       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12702 MB |   16633 MB |  268902 GB |  268889 GB |
|       from large pool |   12663 MB |   16593 MB |  262919 GB |  262906 GB |
|       from small pool |      39 MB |      73 MB |    5982 GB |    5982 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12702 MB |   16633 MB |  268902 GB |  268889 GB |
|       from large pool |   12663 MB |   16593 MB |  262919 GB |  262906 GB |
|       from small pool |      39 MB |      73 MB |    5982 GB |    5982 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19294 MB |   22532 MB |     960 GB |     941 GB |
|       from large pool |   19252 MB |   22330 MB |     938 GB |     919 GB |
|       from small pool |      42 MB |     202 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2659 MB |    2660 MB |  363336 GB |  363334 GB |
|       from large pool |    2656 MB |    2658 MB |  356277 GB |  356274 GB |
|       from small pool |       2 MB |      14 MB |    7059 GB |    7059 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   70733 K  |   70732 K  |
|       from large pool |     271    |     275    |   33465 K  |   33465 K  |
|       from small pool |     321    |     466    |   37268 K  |   37267 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   70733 K  |   70732 K  |
|       from large pool |     271    |     275    |   33465 K  |   33465 K  |
|       from small pool |     321    |     466    |   37268 K  |   37267 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     163    |   11842    |   11760    |
|       from large pool |      61    |      62    |     535    |     474    |
|       from small pool |      21    |     101    |   11307    |   11286    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      58    |      60    |   35956 K  |   35956 K  |
|       from large pool |      45    |      46    |   18964 K  |   18964 K  |
|       from small pool |      13    |      41    |   16992 K  |   16992 K  |
|===========================================================================|

2022-05-28 12:45:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:45:20 | INFO | train_inner | epoch 008:   1868 / 6032 loss=4.798, nll_loss=3.105, ppl=8.6, wps=32398.9, ups=12.45, wpb=2602.6, bsz=128, num_updates=44000, lr=0.000211058, gnorm=0.97, loss_scale=16, train_wall=8, gb_free=18.4, wall=3660
2022-05-28 12:45:29 | INFO | train_inner | epoch 008:   1968 / 6032 loss=5.062, nll_loss=3.407, ppl=10.6, wps=36380.9, ups=12.32, wpb=2954.1, bsz=128, num_updates=44100, lr=0.000210819, gnorm=0.926, loss_scale=16, train_wall=8, gb_free=20.3, wall=3668
2022-05-28 12:45:37 | INFO | train_inner | epoch 008:   2068 / 6032 loss=4.825, nll_loss=3.136, ppl=8.79, wps=33342.6, ups=12.59, wpb=2648.6, bsz=128, num_updates=44200, lr=0.00021058, gnorm=0.94, loss_scale=16, train_wall=8, gb_free=19.7, wall=3676
2022-05-28 12:45:44 | INFO | train_inner | epoch 008:   2168 / 6032 loss=4.815, nll_loss=3.126, ppl=8.73, wps=31326.8, ups=12.96, wpb=2416.6, bsz=128, num_updates=44300, lr=0.000210342, gnorm=0.973, loss_scale=16, train_wall=8, gb_free=19.8, wall=3683
2022-05-28 12:45:52 | INFO | train_inner | epoch 008:   2268 / 6032 loss=4.848, nll_loss=3.163, ppl=8.96, wps=33286, ups=12.79, wpb=2601.7, bsz=128, num_updates=44400, lr=0.000210105, gnorm=0.953, loss_scale=16, train_wall=8, gb_free=16.8, wall=3691
2022-05-28 12:45:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 3.03 GiB free; 19.01 GiB reserved in total by PyTorch)
2022-05-28 12:45:53 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 88           |        cudaMalloc retries: 197       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13822 MB |   17924 MB |  271581 GB |  271567 GB |
|       from large pool |   13783 MB |   17885 MB |  265537 GB |  265524 GB |
|       from small pool |      39 MB |      73 MB |    6043 GB |    6043 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13822 MB |   17924 MB |  271581 GB |  271567 GB |
|       from large pool |   13783 MB |   17885 MB |  265537 GB |  265524 GB |
|       from small pool |      39 MB |      73 MB |    6043 GB |    6043 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19462 MB |   21578 MB |     970 GB |     951 GB |
|       from large pool |   19422 MB |   21376 MB |     948 GB |     929 GB |
|       from small pool |      40 MB |     202 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1537 MB |    2360 MB |  367211 GB |  367209 GB |
|       from large pool |    1536 MB |    2359 MB |  360079 GB |  360077 GB |
|       from small pool |       0 MB |      15 MB |    7132 GB |    7132 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   71448 K  |   71447 K  |
|       from large pool |     271    |     275    |   33801 K  |   33801 K  |
|       from small pool |     321    |     466    |   37646 K  |   37646 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   71448 K  |   71447 K  |
|       from large pool |     271    |     275    |   33801 K  |   33801 K  |
|       from small pool |     321    |     466    |   37646 K  |   37646 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     163    |   11925    |   11844    |
|       from large pool |      61    |      62    |     538    |     477    |
|       from small pool |      20    |     101    |   11387    |   11367    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      65    |      70    |   36328 K  |   36328 K  |
|       from large pool |      53    |      53    |   19162 K  |   19162 K  |
|       from small pool |      12    |      51    |   17166 K  |   17166 K  |
|===========================================================================|

2022-05-28 12:45:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:46:00 | INFO | train_inner | epoch 008:   2369 / 6032 loss=4.968, nll_loss=3.302, ppl=9.86, wps=31959.8, ups=11.97, wpb=2670.1, bsz=128, num_updates=44500, lr=0.000209869, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=19.6, wall=3700
2022-05-28 12:46:09 | INFO | train_inner | epoch 008:   2469 / 6032 loss=4.956, nll_loss=3.286, ppl=9.75, wps=34825.6, ups=12.26, wpb=2841.2, bsz=128, num_updates=44600, lr=0.000209633, gnorm=0.947, loss_scale=16, train_wall=8, gb_free=16.5, wall=3708
2022-05-28 12:46:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 23.70 GiB total capacity; 15.64 GiB already allocated; 3.45 GiB free; 18.58 GiB reserved in total by PyTorch)
2022-05-28 12:46:16 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 89           |        cudaMalloc retries: 199       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12349 MB |   16011 MB |  273272 GB |  273260 GB |
|       from large pool |   12310 MB |   15972 MB |  267190 GB |  267178 GB |
|       from small pool |      39 MB |      73 MB |    6081 GB |    6081 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12349 MB |   16011 MB |  273272 GB |  273260 GB |
|       from large pool |   12310 MB |   15972 MB |  267190 GB |  267178 GB |
|       from small pool |      39 MB |      73 MB |    6081 GB |    6081 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19024 MB |   20798 MB |     979 GB |     960 GB |
|       from large pool |   18984 MB |   20596 MB |     956 GB |     938 GB |
|       from small pool |      40 MB |     202 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3010 MB |    3012 MB |  369641 GB |  369638 GB |
|       from large pool |    3009 MB |    3011 MB |  362464 GB |  362461 GB |
|       from small pool |       0 MB |      11 MB |    7177 GB |    7177 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   71900 K  |   71899 K  |
|       from large pool |     271    |     275    |   34013 K  |   34013 K  |
|       from small pool |     321    |     466    |   37886 K  |   37886 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   71900 K  |   71899 K  |
|       from large pool |     271    |     275    |   34013 K  |   34013 K  |
|       from small pool |     321    |     466    |   37886 K  |   37886 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     163    |   12009    |   11928    |
|       from large pool |      61    |      62    |     541    |     480    |
|       from small pool |      20    |     101    |   11468    |   11448    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      65    |      67    |   36564 K  |   36564 K  |
|       from large pool |      56    |      57    |   19287 K  |   19287 K  |
|       from small pool |       9    |      38    |   17276 K  |   17276 K  |
|===========================================================================|

2022-05-28 12:46:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:46:17 | INFO | train_inner | epoch 008:   2570 / 6032 loss=4.888, nll_loss=3.211, ppl=9.26, wps=31067, ups=12.26, wpb=2534.7, bsz=128, num_updates=44700, lr=0.000209399, gnorm=0.959, loss_scale=16, train_wall=8, gb_free=18.9, wall=3716
2022-05-28 12:46:25 | INFO | train_inner | epoch 008:   2670 / 6032 loss=4.835, nll_loss=3.148, ppl=8.86, wps=33811.8, ups=12.86, wpb=2629.4, bsz=128, num_updates=44800, lr=0.000209165, gnorm=0.985, loss_scale=16, train_wall=8, gb_free=20.8, wall=3724
2022-05-28 12:46:33 | INFO | train_inner | epoch 008:   2770 / 6032 loss=5.163, nll_loss=3.524, ppl=11.5, wps=34927.3, ups=11.4, wpb=3064.5, bsz=128, num_updates=44900, lr=0.000208932, gnorm=0.973, loss_scale=16, train_wall=9, gb_free=20.2, wall=3732
2022-05-28 12:46:41 | INFO | train_inner | epoch 008:   2870 / 6032 loss=4.975, nll_loss=3.309, ppl=9.91, wps=34999.4, ups=12.65, wpb=2766.2, bsz=128, num_updates=45000, lr=0.0002087, gnorm=0.962, loss_scale=16, train_wall=8, gb_free=17.7, wall=3740
2022-05-28 12:46:49 | INFO | train_inner | epoch 008:   2970 / 6032 loss=4.986, nll_loss=3.321, ppl=9.99, wps=34363.9, ups=12.72, wpb=2701.1, bsz=128, num_updates=45100, lr=0.000208468, gnorm=0.963, loss_scale=16, train_wall=8, gb_free=19.6, wall=3748
2022-05-28 12:46:57 | INFO | train_inner | epoch 008:   3070 / 6032 loss=4.954, nll_loss=3.286, ppl=9.75, wps=33537.7, ups=12.48, wpb=2686.3, bsz=128, num_updates=45200, lr=0.000208237, gnorm=0.98, loss_scale=16, train_wall=8, gb_free=20.2, wall=3756
2022-05-28 12:47:05 | INFO | train_inner | epoch 008:   3170 / 6032 loss=4.879, nll_loss=3.199, ppl=9.19, wps=32172.2, ups=12.13, wpb=2651.4, bsz=128, num_updates=45300, lr=0.000208007, gnorm=0.98, loss_scale=16, train_wall=8, gb_free=19.9, wall=3764
2022-05-28 12:47:13 | INFO | train_inner | epoch 008:   3270 / 6032 loss=4.962, nll_loss=3.295, ppl=9.81, wps=31756.2, ups=12.3, wpb=2581.9, bsz=128, num_updates=45400, lr=0.000207778, gnorm=0.988, loss_scale=16, train_wall=8, gb_free=19.1, wall=3773
2022-05-28 12:47:22 | INFO | train_inner | epoch 008:   3370 / 6032 loss=4.933, nll_loss=3.262, ppl=9.6, wps=30836.4, ups=12.19, wpb=2530.5, bsz=128, num_updates=45500, lr=0.00020755, gnorm=0.966, loss_scale=16, train_wall=8, gb_free=20.6, wall=3781
2022-05-28 12:47:30 | INFO | train_inner | epoch 008:   3470 / 6032 loss=4.948, nll_loss=3.278, ppl=9.7, wps=32044.1, ups=12.6, wpb=2543.8, bsz=128, num_updates=45600, lr=0.000207322, gnorm=0.987, loss_scale=16, train_wall=8, gb_free=20.4, wall=3789
2022-05-28 12:47:37 | INFO | train_inner | epoch 008:   3570 / 6032 loss=4.76, nll_loss=3.065, ppl=8.37, wps=31661, ups=13.29, wpb=2383.2, bsz=128, num_updates=45700, lr=0.000207095, gnorm=1.011, loss_scale=16, train_wall=7, gb_free=20.4, wall=3796
2022-05-28 12:47:45 | INFO | train_inner | epoch 008:   3670 / 6032 loss=5.157, nll_loss=3.517, ppl=11.45, wps=34987.8, ups=12.49, wpb=2800.8, bsz=128, num_updates=45800, lr=0.000206869, gnorm=0.985, loss_scale=16, train_wall=8, gb_free=15.9, wall=3804
2022-05-28 12:47:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.62 GiB (GPU 0; 23.70 GiB total capacity; 15.67 GiB already allocated; 2.28 GiB free; 19.75 GiB reserved in total by PyTorch)
2022-05-28 12:47:46 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 90           |        cudaMalloc retries: 204       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12334 MB |   16046 MB |  279859 GB |  279847 GB |
|       from large pool |   12295 MB |   16006 MB |  273631 GB |  273619 GB |
|       from small pool |      39 MB |      73 MB |    6228 GB |    6228 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12334 MB |   16046 MB |  279859 GB |  279847 GB |
|       from large pool |   12295 MB |   16006 MB |  273631 GB |  273619 GB |
|       from small pool |      39 MB |      73 MB |    6228 GB |    6228 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20220 MB |   20236 MB |    1010 GB |     990 GB |
|       from large pool |   20180 MB |   20180 MB |     987 GB |     967 GB |
|       from small pool |      40 MB |     202 MB |      23 GB |      22 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4173 MB |    4174 MB |  379256 GB |  379252 GB |
|       from large pool |    4172 MB |    4173 MB |  371905 GB |  371900 GB |
|       from small pool |       0 MB |      10 MB |    7351 GB |    7351 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   73641 K  |   73640 K  |
|       from large pool |     271    |     275    |   34832 K  |   34832 K  |
|       from small pool |     321    |     466    |   38808 K  |   38808 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   73641 K  |   73640 K  |
|       from large pool |     271    |     275    |   34832 K  |   34832 K  |
|       from small pool |     321    |     466    |   38808 K  |   38808 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     162    |   12340    |   12259    |
|       from large pool |      61    |      61    |     550    |     489    |
|       from small pool |      20    |     101    |   11790    |   11770    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      53    |      55    |   37472 K  |   37472 K  |
|       from large pool |      43    |      44    |   19771 K  |   19771 K  |
|       from small pool |      10    |      45    |   17700 K  |   17700 K  |
|===========================================================================|

2022-05-28 12:47:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:47:54 | INFO | train_inner | epoch 008:   3771 / 6032 loss=4.866, nll_loss=3.186, ppl=9.1, wps=30092, ups=11.84, wpb=2542.3, bsz=128, num_updates=45900, lr=0.000206643, gnorm=0.983, loss_scale=16, train_wall=8, gb_free=20.9, wall=3813
2022-05-28 12:48:02 | INFO | train_inner | epoch 008:   3871 / 6032 loss=4.884, nll_loss=3.207, ppl=9.23, wps=33356.4, ups=12.46, wpb=2677.2, bsz=128, num_updates=46000, lr=0.000206419, gnorm=0.967, loss_scale=16, train_wall=8, gb_free=19.6, wall=3821
2022-05-28 12:48:10 | INFO | train_inner | epoch 008:   3971 / 6032 loss=4.949, nll_loss=3.28, ppl=9.72, wps=32751.4, ups=12.17, wpb=2690.6, bsz=128, num_updates=46100, lr=0.000206195, gnorm=0.969, loss_scale=16, train_wall=8, gb_free=20.6, wall=3829
2022-05-28 12:48:18 | INFO | train_inner | epoch 008:   4071 / 6032 loss=4.991, nll_loss=3.328, ppl=10.05, wps=34998.7, ups=12.53, wpb=2792.5, bsz=128, num_updates=46200, lr=0.000205971, gnorm=0.975, loss_scale=16, train_wall=8, gb_free=20.4, wall=3837
2022-05-28 12:48:26 | INFO | train_inner | epoch 008:   4171 / 6032 loss=4.794, nll_loss=3.103, ppl=8.59, wps=31150.1, ups=12.85, wpb=2424.2, bsz=128, num_updates=46300, lr=0.000205749, gnorm=0.996, loss_scale=16, train_wall=8, gb_free=19.9, wall=3845
2022-05-28 12:48:34 | INFO | train_inner | epoch 008:   4271 / 6032 loss=5.004, nll_loss=3.343, ppl=10.14, wps=35322.4, ups=12.33, wpb=2865.2, bsz=128, num_updates=46400, lr=0.000205527, gnorm=0.961, loss_scale=16, train_wall=8, gb_free=18.6, wall=3853
2022-05-28 12:48:41 | INFO | train_inner | epoch 008:   4371 / 6032 loss=4.826, nll_loss=3.141, ppl=8.82, wps=32348, ups=13.19, wpb=2452.9, bsz=128, num_updates=46500, lr=0.000205306, gnorm=0.98, loss_scale=16, train_wall=7, gb_free=18.8, wall=3860
2022-05-28 12:48:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.27 GiB (GPU 0; 23.70 GiB total capacity; 14.69 GiB already allocated; 2.76 GiB free; 19.27 GiB reserved in total by PyTorch)
2022-05-28 12:48:43 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 91           |        cudaMalloc retries: 206       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   11701 MB |   15046 MB |  283987 GB |  283976 GB |
|       from large pool |   11662 MB |   15006 MB |  277662 GB |  277651 GB |
|       from small pool |      38 MB |      73 MB |    6325 GB |    6324 GB |
|---------------------------------------------------------------------------|
| Active memory         |   11701 MB |   15046 MB |  283987 GB |  283976 GB |
|       from large pool |   11662 MB |   15006 MB |  277662 GB |  277651 GB |
|       from small pool |      38 MB |      73 MB |    6325 GB |    6324 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19732 MB |   19894 MB |    1016 GB |     997 GB |
|       from large pool |   19692 MB |   19692 MB |     993 GB |     973 GB |
|       from small pool |      40 MB |     202 MB |      23 GB |      23 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3464 MB |    4686 MB |  385316 GB |  385313 GB |
|       from large pool |    3463 MB |    4685 MB |  377851 GB |  377847 GB |
|       from small pool |       1 MB |      10 MB |    7465 GB |    7465 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   74745 K  |   74744 K  |
|       from large pool |     271    |     275    |   35350 K  |   35350 K  |
|       from small pool |     321    |     466    |   39394 K  |   39394 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   74745 K  |   74744 K  |
|       from large pool |     271    |     275    |   35350 K  |   35350 K  |
|       from small pool |     321    |     466    |   39394 K  |   39394 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     162    |   12502    |   12421    |
|       from large pool |      61    |      61    |     552    |     491    |
|       from small pool |      20    |     101    |   11950    |   11930    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      60    |      62    |   38049 K  |   38049 K  |
|       from large pool |      49    |      50    |   20077 K  |   20077 K  |
|       from small pool |      11    |      43    |   17972 K  |   17972 K  |
|===========================================================================|

2022-05-28 12:48:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:48:49 | INFO | train_inner | epoch 008:   4472 / 6032 loss=4.874, nll_loss=3.197, ppl=9.17, wps=32745.8, ups=12.38, wpb=2646.1, bsz=128, num_updates=46600, lr=0.000205086, gnorm=0.947, loss_scale=16, train_wall=8, gb_free=20.8, wall=3868
2022-05-28 12:48:57 | INFO | train_inner | epoch 008:   4572 / 6032 loss=4.958, nll_loss=3.291, ppl=9.79, wps=36342.7, ups=12.52, wpb=2902.1, bsz=128, num_updates=46700, lr=0.000204866, gnorm=0.951, loss_scale=16, train_wall=8, gb_free=20.8, wall=3876
2022-05-28 12:49:06 | INFO | train_inner | epoch 008:   4672 / 6032 loss=4.997, nll_loss=3.336, ppl=10.1, wps=35018.3, ups=12.19, wpb=2873.7, bsz=128, num_updates=46800, lr=0.000204647, gnorm=0.964, loss_scale=16, train_wall=8, gb_free=18.9, wall=3885
2022-05-28 12:49:13 | INFO | train_inner | epoch 008:   4772 / 6032 loss=4.91, nll_loss=3.237, ppl=9.43, wps=34479.9, ups=12.63, wpb=2730.5, bsz=128, num_updates=46900, lr=0.000204429, gnorm=0.974, loss_scale=16, train_wall=8, gb_free=19.6, wall=3893
2022-05-28 12:49:21 | INFO | train_inner | epoch 008:   4872 / 6032 loss=4.967, nll_loss=3.303, ppl=9.87, wps=32988.8, ups=12.63, wpb=2611, bsz=128, num_updates=47000, lr=0.000204211, gnorm=0.968, loss_scale=16, train_wall=8, gb_free=19.5, wall=3901
2022-05-28 12:49:30 | INFO | train_inner | epoch 008:   4972 / 6032 loss=5.192, nll_loss=3.559, ppl=11.79, wps=35476.1, ups=12.05, wpb=2944.9, bsz=128, num_updates=47100, lr=0.000203994, gnorm=0.984, loss_scale=16, train_wall=8, gb_free=20.6, wall=3909
2022-05-28 12:49:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.69 GiB already allocated; 2.26 GiB free; 19.77 GiB reserved in total by PyTorch)
2022-05-28 12:49:31 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 92           |        cudaMalloc retries: 208       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17090 MB |   17148 MB |  287616 GB |  287599 GB |
|       from large pool |   17050 MB |   17108 MB |  281214 GB |  281198 GB |
|       from small pool |      39 MB |      73 MB |    6401 GB |    6401 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17090 MB |   17148 MB |  287616 GB |  287599 GB |
|       from large pool |   17050 MB |   17108 MB |  281214 GB |  281198 GB |
|       from small pool |      39 MB |      73 MB |    6401 GB |    6401 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20248 MB |   22412 MB |    1028 GB |    1008 GB |
|       from large pool |   20206 MB |   22210 MB |    1005 GB |     985 GB |
|       from small pool |      42 MB |     202 MB |      23 GB |      23 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3157 MB |    3425 MB |  390602 GB |  390599 GB |
|       from large pool |    3155 MB |    3425 MB |  383045 GB |  383042 GB |
|       from small pool |       2 MB |       5 MB |    7556 GB |    7556 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   75667 K  |   75667 K  |
|       from large pool |     271    |     275    |   35788 K  |   35788 K  |
|       from small pool |     321    |     466    |   39878 K  |   39878 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   75667 K  |   75667 K  |
|       from large pool |     271    |     275    |   35788 K  |   35788 K  |
|       from small pool |     321    |     466    |   39878 K  |   39878 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     163    |   12586    |   12504    |
|       from large pool |      61    |      62    |     555    |     494    |
|       from small pool |      21    |     101    |   12031    |   12010    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      56    |      57    |   38528 K  |   38528 K  |
|       from large pool |      45    |      45    |   20333 K  |   20333 K  |
|       from small pool |      11    |      18    |   18194 K  |   18194 K  |
|===========================================================================|

2022-05-28 12:49:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:49:38 | INFO | train_inner | epoch 008:   5073 / 6032 loss=4.868, nll_loss=3.189, ppl=9.12, wps=31835.8, ups=12.62, wpb=2523.1, bsz=128, num_updates=47200, lr=0.000203778, gnorm=0.969, loss_scale=16, train_wall=7, gb_free=18.4, wall=3917
2022-05-28 12:49:46 | INFO | train_inner | epoch 008:   5173 / 6032 loss=4.919, nll_loss=3.248, ppl=9.5, wps=32824, ups=12.62, wpb=2600.9, bsz=128, num_updates=47300, lr=0.000203562, gnorm=0.966, loss_scale=16, train_wall=8, gb_free=19.7, wall=3925
2022-05-28 12:49:53 | INFO | train_inner | epoch 008:   5273 / 6032 loss=4.973, nll_loss=3.308, ppl=9.91, wps=32078.9, ups=12.68, wpb=2529.3, bsz=128, num_updates=47400, lr=0.000203348, gnorm=0.977, loss_scale=16, train_wall=8, gb_free=19.8, wall=3933
2022-05-28 12:50:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 1.88 GiB free; 20.15 GiB reserved in total by PyTorch)
2022-05-28 12:50:00 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 93           |        cudaMalloc retries: 210       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13288 MB |   17219 MB |  289735 GB |  289722 GB |
|       from large pool |   13249 MB |   17179 MB |  283284 GB |  283271 GB |
|       from small pool |      39 MB |      73 MB |    6451 GB |    6450 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13288 MB |   17219 MB |  289735 GB |  289722 GB |
|       from large pool |   13249 MB |   17179 MB |  283284 GB |  283271 GB |
|       from small pool |      39 MB |      73 MB |    6451 GB |    6450 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20638 MB |   20638 MB |    1032 GB |    1012 GB |
|       from large pool |   20596 MB |   20596 MB |    1008 GB |     988 GB |
|       from small pool |      42 MB |     202 MB |      23 GB |      23 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3417 MB |    3418 MB |  393697 GB |  393694 GB |
|       from large pool |    3414 MB |    3416 MB |  386082 GB |  386079 GB |
|       from small pool |       2 MB |       8 MB |    7614 GB |    7614 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   76238 K  |   76237 K  |
|       from large pool |     271    |     275    |   36055 K  |   36055 K  |
|       from small pool |     321    |     466    |   40182 K  |   40182 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   76238 K  |   76237 K  |
|       from large pool |     271    |     275    |   36055 K  |   36055 K  |
|       from small pool |     321    |     466    |   40182 K  |   40182 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     162    |   12667    |   12585    |
|       from large pool |      61    |      61    |     556    |     495    |
|       from small pool |      21    |     101    |   12111    |   12090    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      64    |      66    |   38826 K  |   38826 K  |
|       from large pool |      53    |      54    |   20492 K  |   20492 K  |
|       from small pool |      11    |      23    |   18334 K  |   18334 K  |
|===========================================================================|

2022-05-28 12:50:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:50:02 | INFO | train_inner | epoch 008:   5374 / 6032 loss=4.946, nll_loss=3.278, ppl=9.7, wps=32639.2, ups=12.23, wpb=2668.6, bsz=128, num_updates=47500, lr=0.000203133, gnorm=0.991, loss_scale=16, train_wall=8, gb_free=20.2, wall=3941
2022-05-28 12:50:09 | INFO | train_inner | epoch 008:   5474 / 6032 loss=5.139, nll_loss=3.498, ppl=11.3, wps=34732.5, ups=12.68, wpb=2739.7, bsz=128, num_updates=47600, lr=0.00020292, gnorm=0.962, loss_scale=16, train_wall=8, gb_free=16.5, wall=3949
2022-05-28 12:50:18 | INFO | train_inner | epoch 008:   5574 / 6032 loss=5.048, nll_loss=3.396, ppl=10.53, wps=34424.7, ups=12.36, wpb=2784.3, bsz=128, num_updates=47700, lr=0.000202707, gnorm=0.976, loss_scale=16, train_wall=8, gb_free=20.8, wall=3957
2022-05-28 12:50:25 | INFO | train_inner | epoch 008:   5674 / 6032 loss=4.835, nll_loss=3.153, ppl=8.89, wps=31873, ups=13.07, wpb=2437.8, bsz=128, num_updates=47800, lr=0.000202495, gnorm=0.99, loss_scale=16, train_wall=7, gb_free=19.2, wall=3964
2022-05-28 12:50:33 | INFO | train_inner | epoch 008:   5774 / 6032 loss=4.89, nll_loss=3.215, ppl=9.29, wps=33449.9, ups=12.85, wpb=2603.2, bsz=128, num_updates=47900, lr=0.000202283, gnorm=1.004, loss_scale=16, train_wall=8, gb_free=19.7, wall=3972
2022-05-28 12:50:41 | INFO | train_inner | epoch 008:   5874 / 6032 loss=4.953, nll_loss=3.287, ppl=9.76, wps=34173.5, ups=12.5, wpb=2732.8, bsz=128, num_updates=48000, lr=0.000202073, gnorm=0.966, loss_scale=16, train_wall=8, gb_free=19.1, wall=3980
2022-05-28 12:50:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.70 GiB total capacity; 15.42 GiB already allocated; 2.31 GiB free; 19.72 GiB reserved in total by PyTorch)
2022-05-28 12:50:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 94           |        cudaMalloc retries: 212       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12228 MB |   15793 MB |  292884 GB |  292872 GB |
|       from large pool |   12189 MB |   15754 MB |  286359 GB |  286347 GB |
|       from small pool |      39 MB |      73 MB |    6524 GB |    6524 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12228 MB |   15793 MB |  292884 GB |  292872 GB |
|       from large pool |   12189 MB |   15754 MB |  286359 GB |  286347 GB |
|       from small pool |      39 MB |      73 MB |    6524 GB |    6524 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20196 MB |   20196 MB |    1039 GB |    1019 GB |
|       from large pool |   20156 MB |   20156 MB |    1015 GB |     995 GB |
|       from small pool |      40 MB |     202 MB |      23 GB |      23 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4401 MB |    4402 MB |  398338 GB |  398333 GB |
|       from large pool |    4400 MB |    4401 MB |  390636 GB |  390631 GB |
|       from small pool |       0 MB |      13 MB |    7702 GB |    7702 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   77076 K  |   77075 K  |
|       from large pool |     271    |     275    |   36446 K  |   36445 K  |
|       from small pool |     321    |     466    |   40630 K  |   40629 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   77076 K  |   77075 K  |
|       from large pool |     271    |     275    |   36446 K  |   36445 K  |
|       from small pool |     321    |     466    |   40630 K  |   40629 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     162    |   12749    |   12668    |
|       from large pool |      61    |      61    |     558    |     497    |
|       from small pool |      20    |     101    |   12191    |   12171    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      44    |      59    |   39262 K  |   39262 K  |
|       from large pool |      35    |      36    |   20722 K  |   20721 K  |
|       from small pool |       9    |      40    |   18540 K  |   18540 K  |
|===========================================================================|

2022-05-28 12:50:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:50:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.98 GiB already allocated; 1.52 GiB free; 20.51 GiB reserved in total by PyTorch)
2022-05-28 12:50:44 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 95           |        cudaMalloc retries: 213       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14039 MB |   18409 MB |  292990 GB |  292977 GB |
|       from large pool |   13999 MB |   18370 MB |  286464 GB |  286450 GB |
|       from small pool |      39 MB |      73 MB |    6526 GB |    6526 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14039 MB |   18409 MB |  292990 GB |  292977 GB |
|       from large pool |   13999 MB |   18370 MB |  286464 GB |  286450 GB |
|       from small pool |      39 MB |      73 MB |    6526 GB |    6526 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21002 MB |   21094 MB |    1043 GB |    1023 GB |
|       from large pool |   20962 MB |   20962 MB |    1019 GB |     999 GB |
|       from small pool |      40 MB |     132 MB |      23 GB |      23 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2590 MB |    2592 MB |  398476 GB |  398474 GB |
|       from large pool |    2590 MB |    2591 MB |  390773 GB |  390770 GB |
|       from small pool |       0 MB |      11 MB |    7703 GB |    7703 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   77098 K  |   77098 K  |
|       from large pool |     271    |     275    |   36456 K  |   36456 K  |
|       from small pool |     321    |     466    |   40641 K  |   40641 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   77098 K  |   77098 K  |
|       from large pool |     271    |     275    |   36456 K  |   36456 K  |
|       from small pool |     321    |     466    |   40641 K  |   40641 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     127    |   12796    |   12715    |
|       from large pool |      61    |      61    |     559    |     498    |
|       from small pool |      20    |      66    |   12237    |   12217    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      56    |   39274 K  |   39274 K  |
|       from large pool |      38    |      39    |   20728 K  |   20728 K  |
|       from small pool |      11    |      41    |   18545 K  |   18545 K  |
|===========================================================================|

2022-05-28 12:50:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:50:49 | INFO | train_inner | epoch 008:   5976 / 6032 loss=4.987, nll_loss=3.324, ppl=10.02, wps=32764.9, ups=11.83, wpb=2770.4, bsz=128, num_updates=48100, lr=0.000201862, gnorm=0.995, loss_scale=16, train_wall=8, gb_free=19.4, wall=3989
2022-05-28 12:50:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:51:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.326 | nll_loss 3.664 | ppl 12.68 | wps 105522 | wpb 2687.4 | bsz 128 | num_updates 48156 | best_loss 5.326
2022-05-28 12:51:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 48156 updates
2022-05-28 12:51:13 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint8.pt
2022-05-28 12:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint8.pt
2022-05-28 12:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint8.pt (epoch 8 @ 48156 updates, score 5.326) (writing took 10.792015574872494 seconds)
2022-05-28 12:51:24 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-05-28 12:51:24 | INFO | train | epoch 008 | loss 4.919 | nll_loss 3.246 | ppl 9.49 | wps 31305.5 | ups 11.72 | wpb 2670.2 | bsz 128 | num_updates 48156 | lr 0.000201745 | gnorm 0.967 | loss_scale 16 | train_wall 466 | gb_free 20.8 | wall 4023
2022-05-28 12:51:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:51:24 | INFO | fairseq.trainer | begin training epoch 9
2022-05-28 12:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:51:28 | INFO | train_inner | epoch 009:     44 / 6032 loss=4.763, nll_loss=3.067, ppl=8.38, wps=6570.3, ups=2.6, wpb=2524.9, bsz=128, num_updates=48200, lr=0.000201653, gnorm=0.957, loss_scale=16, train_wall=8, gb_free=17.6, wall=4027
2022-05-28 12:51:36 | INFO | train_inner | epoch 009:    144 / 6032 loss=4.652, nll_loss=2.934, ppl=7.64, wps=33904.9, ups=12.77, wpb=2655.3, bsz=128, num_updates=48300, lr=0.000201444, gnorm=0.924, loss_scale=16, train_wall=8, gb_free=15.7, wall=4035
2022-05-28 12:51:44 | INFO | train_inner | epoch 009:    244 / 6032 loss=4.716, nll_loss=3.009, ppl=8.05, wps=32737.8, ups=12.73, wpb=2571.7, bsz=128, num_updates=48400, lr=0.000201236, gnorm=0.974, loss_scale=16, train_wall=8, gb_free=18.5, wall=4043
2022-05-28 12:51:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 11.76 GiB already allocated; 4.10 GiB free; 17.93 GiB reserved in total by PyTorch)
2022-05-28 12:51:49 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 96           |        cudaMalloc retries: 216       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12040 MB |   12066 MB |  296920 GB |  296908 GB |
|       from large pool |   12000 MB |   12027 MB |  290313 GB |  290302 GB |
|       from small pool |      39 MB |      73 MB |    6606 GB |    6606 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12040 MB |   12066 MB |  296920 GB |  296908 GB |
|       from large pool |   12000 MB |   12027 MB |  290313 GB |  290302 GB |
|       from small pool |      39 MB |      73 MB |    6606 GB |    6606 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18358 MB |   18518 MB |    1060 GB |    1042 GB |
|       from large pool |   18316 MB |   18316 MB |    1036 GB |    1018 GB |
|       from small pool |      42 MB |     202 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6317 MB |    6317 MB |  405514 GB |  405508 GB |
|       from large pool |    6315 MB |    6315 MB |  397719 GB |  397713 GB |
|       from small pool |       2 MB |      14 MB |    7794 GB |    7794 GB |
|---------------------------------------------------------------------------|
| Allocations           |     590    |     593    |   78097 K  |   78096 K  |
|       from large pool |     273    |     275    |   36953 K  |   36952 K  |
|       from small pool |     317    |     466    |   41144 K  |   41144 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     590    |     593    |   78097 K  |   78096 K  |
|       from large pool |     273    |     275    |   36953 K  |   36952 K  |
|       from small pool |     317    |     466    |   41144 K  |   41144 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     126    |   13005    |   12959    |
|       from large pool |      25    |      25    |     563    |     538    |
|       from small pool |      21    |     101    |   12442    |   12421    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      44    |   39762 K  |   39762 K  |
|       from large pool |      15    |      16    |   21012 K  |   21012 K  |
|       from small pool |      12    |      34    |   18749 K  |   18749 K  |
|===========================================================================|

2022-05-28 12:51:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:51:52 | INFO | train_inner | epoch 009:    345 / 6032 loss=4.922, nll_loss=3.244, ppl=9.48, wps=35749.5, ups=11.94, wpb=2994.4, bsz=128, num_updates=48500, lr=0.000201028, gnorm=0.937, loss_scale=16, train_wall=8, gb_free=20, wall=4051
2022-05-28 12:51:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 269.69 MiB free; 21.77 GiB reserved in total by PyTorch)
2022-05-28 12:51:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 97           |        cudaMalloc retries: 217       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12698 MB |   16629 MB |  297273 GB |  297261 GB |
|       from large pool |   12659 MB |   16590 MB |  290660 GB |  290647 GB |
|       from small pool |      39 MB |      73 MB |    6613 GB |    6613 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12698 MB |   16629 MB |  297273 GB |  297261 GB |
|       from large pool |   12659 MB |   16590 MB |  290660 GB |  290647 GB |
|       from small pool |      39 MB |      73 MB |    6613 GB |    6613 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22290 MB |   22432 MB |    1064 GB |    1042 GB |
|       from large pool |   22248 MB |   22248 MB |    1040 GB |    1018 GB |
|       from small pool |      42 MB |     184 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5659 MB |    5660 MB |  406195 GB |  406189 GB |
|       from large pool |    5656 MB |    5657 MB |  398391 GB |  398386 GB |
|       from small pool |       2 MB |      24 MB |    7803 GB |    7803 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   78188 K  |   78187 K  |
|       from large pool |     271    |     275    |   36996 K  |   36996 K  |
|       from small pool |     321    |     466    |   41191 K  |   41191 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   78188 K  |   78187 K  |
|       from large pool |     271    |     275    |   36996 K  |   36996 K  |
|       from small pool |     321    |     466    |   41191 K  |   41191 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     118    |   13077    |   13030    |
|       from large pool |      26    |      26    |     564    |     538    |
|       from small pool |      21    |      92    |   12513    |   12492    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      68    |   39808 K  |   39808 K  |
|       from large pool |      20    |      21    |   21037 K  |   21037 K  |
|       from small pool |      11    |      54    |   18770 K  |   18770 K  |
|===========================================================================|

2022-05-28 12:51:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:52:00 | INFO | train_inner | epoch 009:    446 / 6032 loss=4.696, nll_loss=2.986, ppl=7.92, wps=31592.2, ups=12, wpb=2631.9, bsz=128, num_updates=48600, lr=0.000200821, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=20.3, wall=4059
2022-05-28 12:52:08 | INFO | train_inner | epoch 009:    546 / 6032 loss=4.707, nll_loss=2.997, ppl=7.99, wps=33583.5, ups=12.71, wpb=2641.3, bsz=128, num_updates=48700, lr=0.000200615, gnorm=0.942, loss_scale=16, train_wall=8, gb_free=19.8, wall=4067
2022-05-28 12:52:16 | INFO | train_inner | epoch 009:    646 / 6032 loss=4.802, nll_loss=3.107, ppl=8.61, wps=33686.2, ups=12.62, wpb=2668.4, bsz=128, num_updates=48800, lr=0.000200409, gnorm=0.955, loss_scale=16, train_wall=8, gb_free=20.6, wall=4075
2022-05-28 12:52:24 | INFO | train_inner | epoch 009:    746 / 6032 loss=4.928, nll_loss=3.25, ppl=9.52, wps=35132.9, ups=12.75, wpb=2756.5, bsz=128, num_updates=48900, lr=0.000200204, gnorm=0.97, loss_scale=16, train_wall=8, gb_free=20.1, wall=4083
2022-05-28 12:52:32 | INFO | train_inner | epoch 009:    846 / 6032 loss=4.744, nll_loss=3.04, ppl=8.23, wps=34368.1, ups=12.82, wpb=2681.7, bsz=128, num_updates=49000, lr=0.0002, gnorm=0.983, loss_scale=16, train_wall=8, gb_free=18.7, wall=4091
2022-05-28 12:52:39 | INFO | train_inner | epoch 009:    946 / 6032 loss=4.729, nll_loss=3.024, ppl=8.13, wps=32775.3, ups=13.09, wpb=2503.5, bsz=128, num_updates=49100, lr=0.000199796, gnorm=0.985, loss_scale=16, train_wall=7, gb_free=20.3, wall=4098
2022-05-28 12:52:47 | INFO | train_inner | epoch 009:   1046 / 6032 loss=4.812, nll_loss=3.118, ppl=8.68, wps=32899.8, ups=13.13, wpb=2505.2, bsz=128, num_updates=49200, lr=0.000199593, gnorm=0.99, loss_scale=16, train_wall=7, gb_free=19.7, wall=4106
2022-05-28 12:52:55 | INFO | train_inner | epoch 009:   1146 / 6032 loss=4.721, nll_loss=3.016, ppl=8.09, wps=32928.7, ups=12.9, wpb=2553.3, bsz=128, num_updates=49300, lr=0.000199391, gnorm=0.992, loss_scale=16, train_wall=8, gb_free=20.8, wall=4114
2022-05-28 12:53:03 | INFO | train_inner | epoch 009:   1246 / 6032 loss=4.771, nll_loss=3.071, ppl=8.4, wps=34138, ups=12.37, wpb=2760.3, bsz=128, num_updates=49400, lr=0.000199189, gnorm=0.967, loss_scale=16, train_wall=8, gb_free=19.3, wall=4122
2022-05-28 12:53:11 | INFO | train_inner | epoch 009:   1346 / 6032 loss=4.865, nll_loss=3.18, ppl=9.06, wps=34871.4, ups=12.62, wpb=2763.3, bsz=128, num_updates=49500, lr=0.000198987, gnorm=0.994, loss_scale=16, train_wall=8, gb_free=20.8, wall=4130
2022-05-28 12:53:19 | INFO | train_inner | epoch 009:   1446 / 6032 loss=4.755, nll_loss=3.053, ppl=8.3, wps=34388.7, ups=12.85, wpb=2676.2, bsz=128, num_updates=49600, lr=0.000198787, gnorm=0.969, loss_scale=16, train_wall=8, gb_free=19.1, wall=4138
2022-05-28 12:53:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-05-28 12:53:27 | INFO | train_inner | epoch 009:   1547 / 6032 loss=4.907, nll_loss=3.226, ppl=9.36, wps=32831.7, ups=12.4, wpb=2647.3, bsz=128, num_updates=49700, lr=0.000198587, gnorm=1.026, loss_scale=8, train_wall=8, gb_free=19.5, wall=4146
2022-05-28 12:53:34 | INFO | train_inner | epoch 009:   1647 / 6032 loss=4.69, nll_loss=2.98, ppl=7.89, wps=34250.6, ups=13.03, wpb=2628.5, bsz=128, num_updates=49800, lr=0.000198387, gnorm=0.977, loss_scale=8, train_wall=7, gb_free=20.1, wall=4153
2022-05-28 12:53:42 | INFO | train_inner | epoch 009:   1747 / 6032 loss=4.669, nll_loss=2.955, ppl=7.76, wps=32138.3, ups=13.21, wpb=2432.2, bsz=128, num_updates=49900, lr=0.000198188, gnorm=0.985, loss_scale=8, train_wall=7, gb_free=20.1, wall=4161
2022-05-28 12:53:50 | INFO | train_inner | epoch 009:   1847 / 6032 loss=4.802, nll_loss=3.108, ppl=8.62, wps=33509.2, ups=12.55, wpb=2670.4, bsz=128, num_updates=50000, lr=0.00019799, gnorm=1.026, loss_scale=8, train_wall=8, gb_free=19.6, wall=4169
2022-05-28 12:53:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 537.69 MiB free; 21.51 GiB reserved in total by PyTorch)
2022-05-28 12:53:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 98           |        cudaMalloc retries: 218       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   17220 MB |  306235 GB |  306222 GB |
|       from large pool |   13250 MB |   17180 MB |  299418 GB |  299405 GB |
|       from small pool |      39 MB |      73 MB |    6816 GB |    6816 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   17220 MB |  306235 GB |  306222 GB |
|       from large pool |   13250 MB |   17180 MB |  299418 GB |  299405 GB |
|       from small pool |      39 MB |      73 MB |    6816 GB |    6816 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22022 MB |   22182 MB |    1068 GB |    1046 GB |
|       from large pool |   21980 MB |   21980 MB |    1043 GB |    1022 GB |
|       from small pool |      42 MB |     202 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1276 MB |    4801 MB |  422950 GB |  422949 GB |
|       from large pool |    1273 MB |    4799 MB |  414907 GB |  414906 GB |
|       from small pool |       2 MB |      14 MB |    8043 GB |    8043 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   80571 K  |   80570 K  |
|       from large pool |     271    |     275    |   38114 K  |   38114 K  |
|       from small pool |     321    |     466    |   42456 K  |   42455 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   80571 K  |   80570 K  |
|       from large pool |     271    |     275    |   38114 K  |   38114 K  |
|       from small pool |     321    |     466    |   42456 K  |   42455 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     127    |   13158    |   13111    |
|       from large pool |      26    |      26    |     565    |     539    |
|       from small pool |      21    |     101    |   12593    |   12572    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      28    |      69    |   41022 K  |   41022 K  |
|       from large pool |      16    |      17    |   21672 K  |   21672 K  |
|       from small pool |      12    |      54    |   19350 K  |   19350 K  |
|===========================================================================|

2022-05-28 12:53:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:53:58 | INFO | train_inner | epoch 009:   1948 / 6032 loss=4.85, nll_loss=3.163, ppl=8.96, wps=31911.4, ups=12.12, wpb=2633.3, bsz=128, num_updates=50100, lr=0.000197792, gnorm=1.029, loss_scale=8, train_wall=8, gb_free=21, wall=4177
2022-05-28 12:54:06 | INFO | train_inner | epoch 009:   2048 / 6032 loss=4.838, nll_loss=3.148, ppl=8.87, wps=35476.1, ups=12.59, wpb=2818.8, bsz=128, num_updates=50200, lr=0.000197595, gnorm=0.968, loss_scale=8, train_wall=8, gb_free=20.7, wall=4185
2022-05-28 12:54:14 | INFO | train_inner | epoch 009:   2148 / 6032 loss=4.942, nll_loss=3.266, ppl=9.62, wps=33982.6, ups=12.14, wpb=2799, bsz=128, num_updates=50300, lr=0.000197399, gnorm=0.993, loss_scale=8, train_wall=8, gb_free=16.1, wall=4193
2022-05-28 12:54:22 | INFO | train_inner | epoch 009:   2248 / 6032 loss=4.765, nll_loss=3.068, ppl=8.38, wps=31959.3, ups=12.92, wpb=2473, bsz=128, num_updates=50400, lr=0.000197203, gnorm=0.997, loss_scale=8, train_wall=8, gb_free=19.4, wall=4201
2022-05-28 12:54:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 1.36 GiB free; 20.67 GiB reserved in total by PyTorch)
2022-05-28 12:54:22 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 99           |        cudaMalloc retries: 219       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13822 MB |   17925 MB |  308275 GB |  308262 GB |
|       from large pool |   13783 MB |   17885 MB |  301411 GB |  301397 GB |
|       from small pool |      39 MB |      73 MB |    6864 GB |    6864 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13822 MB |   17925 MB |  308275 GB |  308262 GB |
|       from large pool |   13783 MB |   17885 MB |  301411 GB |  301397 GB |
|       from small pool |      39 MB |      73 MB |    6864 GB |    6864 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21166 MB |   21316 MB |    1074 GB |    1054 GB |
|       from large pool |   21118 MB |   21118 MB |    1050 GB |    1029 GB |
|       from small pool |      48 MB |     198 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3241 MB |    3241 MB |  426738 GB |  426734 GB |
|       from large pool |    3232 MB |    3232 MB |  418638 GB |  418635 GB |
|       from small pool |       8 MB |      18 MB |    8099 GB |    8099 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   81120 K  |   81120 K  |
|       from large pool |     271    |     275    |   38371 K  |   38371 K  |
|       from small pool |     321    |     466    |   42748 K  |   42748 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   81120 K  |   81120 K  |
|       from large pool |     271    |     275    |   38371 K  |   38371 K  |
|       from small pool |     321    |     466    |   42748 K  |   42748 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     126    |   13238    |   13187    |
|       from large pool |      27    |      27    |     567    |     540    |
|       from small pool |      24    |      99    |   12671    |   12647    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      53    |   41301 K  |   41301 K  |
|       from large pool |      19    |      19    |   21817 K  |   21817 K  |
|       from small pool |      14    |      41    |   19484 K  |   19484 K  |
|===========================================================================|

2022-05-28 12:54:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:54:30 | INFO | train_inner | epoch 009:   2349 / 6032 loss=4.923, nll_loss=3.247, ppl=9.49, wps=32619.9, ups=12.32, wpb=2648.4, bsz=128, num_updates=50500, lr=0.000197007, gnorm=1.013, loss_scale=8, train_wall=8, gb_free=19, wall=4209
2022-05-28 12:54:38 | INFO | train_inner | epoch 009:   2449 / 6032 loss=4.679, nll_loss=2.969, ppl=7.83, wps=32401, ups=12.63, wpb=2565.6, bsz=128, num_updates=50600, lr=0.000196813, gnorm=0.968, loss_scale=8, train_wall=8, gb_free=19.3, wall=4217
2022-05-28 12:54:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 1.49 GiB free; 20.54 GiB reserved in total by PyTorch)
2022-05-28 12:54:39 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 100          |        cudaMalloc retries: 220       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12953 MB |   16933 MB |  309441 GB |  309428 GB |
|       from large pool |   12914 MB |   16894 MB |  302545 GB |  302533 GB |
|       from small pool |      39 MB |      73 MB |    6895 GB |    6895 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12953 MB |   16933 MB |  309441 GB |  309428 GB |
|       from large pool |   12914 MB |   16894 MB |  302545 GB |  302533 GB |
|       from small pool |      39 MB |      73 MB |    6895 GB |    6895 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21038 MB |   21200 MB |    1078 GB |    1058 GB |
|       from large pool |   20996 MB |   20996 MB |    1054 GB |    1033 GB |
|       from small pool |      42 MB |     204 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4104 MB |    4104 MB |  428769 GB |  428765 GB |
|       from large pool |    4101 MB |    4101 MB |  420634 GB |  420630 GB |
|       from small pool |       2 MB |      20 MB |    8135 GB |    8135 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   81433 K  |   81433 K  |
|       from large pool |     271    |     275    |   38513 K  |   38513 K  |
|       from small pool |     321    |     466    |   42919 K  |   42919 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   81433 K  |   81433 K  |
|       from large pool |     271    |     275    |   38513 K  |   38513 K  |
|       from small pool |     321    |     466    |   42919 K  |   42919 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      48    |     129    |   13317    |   13269    |
|       from large pool |      27    |      27    |     568    |     541    |
|       from small pool |      21    |     102    |   12749    |   12728    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      53    |   41462 K  |   41462 K  |
|       from large pool |      20    |      20    |   21897 K  |   21897 K  |
|       from small pool |      11    |      45    |   19564 K  |   19564 K  |
|===========================================================================|

2022-05-28 12:54:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:54:46 | INFO | train_inner | epoch 009:   2550 / 6032 loss=4.608, nll_loss=2.888, ppl=7.4, wps=31145.8, ups=12.75, wpb=2442.8, bsz=128, num_updates=50700, lr=0.000196618, gnorm=1.008, loss_scale=8, train_wall=7, gb_free=19.1, wall=4225
2022-05-28 12:54:54 | INFO | train_inner | epoch 009:   2650 / 6032 loss=4.933, nll_loss=3.257, ppl=9.56, wps=35300.9, ups=12.52, wpb=2819.9, bsz=128, num_updates=50800, lr=0.000196425, gnorm=0.967, loss_scale=8, train_wall=8, gb_free=20.7, wall=4233
2022-05-28 12:55:02 | INFO | train_inner | epoch 009:   2750 / 6032 loss=4.783, nll_loss=3.088, ppl=8.5, wps=33942.2, ups=12.56, wpb=2701.8, bsz=128, num_updates=50900, lr=0.000196232, gnorm=0.966, loss_scale=8, train_wall=8, gb_free=19.5, wall=4241
2022-05-28 12:55:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 393.69 MiB free; 21.65 GiB reserved in total by PyTorch)
2022-05-28 12:55:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 101          |        cudaMalloc retries: 222       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21958 MB |   22258 MB |  311772 GB |  311751 GB |
|       from large pool |   21916 MB |   22216 MB |  304826 GB |  304804 GB |
|       from small pool |      42 MB |      73 MB |    6946 GB |    6946 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21958 MB |   22258 MB |  311772 GB |  311751 GB |
|       from large pool |   21916 MB |   22216 MB |  304826 GB |  304804 GB |
|       from small pool |      42 MB |      73 MB |    6946 GB |    6946 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22166 MB |   22466 MB |    1084 GB |    1062 GB |
|       from large pool |   22122 MB |   22422 MB |    1059 GB |    1037 GB |
|       from small pool |      44 MB |     202 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  212217 KB |    5359 MB |  432721 GB |  432721 GB |
|       from large pool |  210800 KB |    5356 MB |  424526 GB |  424526 GB |
|       from small pool |    1417 KB |      20 MB |    8195 GB |    8195 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   82045 K  |   82045 K  |
|       from large pool |     214    |     215    |   38803 K  |   38802 K  |
|       from small pool |     302    |     466    |   43242 K  |   43242 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   82045 K  |   82045 K  |
|       from large pool |     214    |     215    |   38803 K  |   38802 K  |
|       from small pool |     302    |     466    |   43242 K  |   43242 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      65    |     144    |   13415    |   13350    |
|       from large pool |      43    |      44    |     586    |     543    |
|       from small pool |      22    |     101    |   12829    |   12807    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      38    |      52    |   41773 K  |   41773 K  |
|       from large pool |      25    |      25    |   22060 K  |   22060 K  |
|       from small pool |      13    |      43    |   19712 K  |   19712 K  |
|===========================================================================|

2022-05-28 12:55:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:55:10 | INFO | train_inner | epoch 009:   2851 / 6032 loss=4.906, nll_loss=3.229, ppl=9.38, wps=32703.7, ups=12.17, wpb=2686.3, bsz=128, num_updates=51000, lr=0.000196039, gnorm=1.003, loss_scale=8, train_wall=8, gb_free=2, wall=4249
2022-05-28 12:55:18 | INFO | train_inner | epoch 009:   2951 / 6032 loss=4.973, nll_loss=3.304, ppl=9.88, wps=33268.1, ups=11.83, wpb=2811.2, bsz=128, num_updates=51100, lr=0.000195847, gnorm=0.97, loss_scale=8, train_wall=8, gb_free=19.1, wall=4258
2022-05-28 12:55:27 | INFO | train_inner | epoch 009:   3051 / 6032 loss=5.001, nll_loss=3.335, ppl=10.09, wps=35629, ups=12.3, wpb=2896.8, bsz=128, num_updates=51200, lr=0.000195656, gnorm=0.967, loss_scale=8, train_wall=8, gb_free=20.1, wall=4266
2022-05-28 12:55:34 | INFO | train_inner | epoch 009:   3151 / 6032 loss=4.833, nll_loss=3.144, ppl=8.84, wps=32605.5, ups=12.91, wpb=2526.4, bsz=128, num_updates=51300, lr=0.000195465, gnorm=0.992, loss_scale=8, train_wall=8, gb_free=19.7, wall=4273
2022-05-28 12:55:42 | INFO | train_inner | epoch 009:   3251 / 6032 loss=4.785, nll_loss=3.091, ppl=8.52, wps=32711.5, ups=12.59, wpb=2598.6, bsz=128, num_updates=51400, lr=0.000195275, gnorm=0.995, loss_scale=8, train_wall=8, gb_free=19.8, wall=4281
2022-05-28 12:55:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.08 GiB (GPU 0; 23.70 GiB total capacity; 13.87 GiB already allocated; 2.89 GiB free; 19.14 GiB reserved in total by PyTorch)
2022-05-28 12:55:44 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 102          |        cudaMalloc retries: 224       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   11048 MB |   14198 MB |  314309 GB |  314298 GB |
|       from large pool |   11009 MB |   14159 MB |  307305 GB |  307294 GB |
|       from small pool |      38 MB |      73 MB |    7004 GB |    7003 GB |
|---------------------------------------------------------------------------|
| Active memory         |   11048 MB |   14198 MB |  314309 GB |  314298 GB |
|       from large pool |   11009 MB |   14159 MB |  307305 GB |  307294 GB |
|       from small pool |      38 MB |      73 MB |    7004 GB |    7003 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19600 MB |   19760 MB |    1087 GB |    1068 GB |
|       from large pool |   19558 MB |   19558 MB |    1062 GB |    1043 GB |
|       from small pool |      42 MB |     202 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3447 MB |    5401 MB |  435761 GB |  435757 GB |
|       from large pool |    3444 MB |    5398 MB |  427497 GB |  427494 GB |
|       from small pool |       3 MB |       8 MB |    8263 GB |    8263 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   82704 K  |   82704 K  |
|       from large pool |     271    |     275    |   39111 K  |   39111 K  |
|       from small pool |     321    |     466    |   43593 K  |   43592 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   82704 K  |   82704 K  |
|       from large pool |     271    |     275    |   39111 K  |   39111 K  |
|       from small pool |     321    |     466    |   43593 K  |   43592 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      63    |     143    |   13572    |   13509    |
|       from large pool |      42    |      42    |     587    |     545    |
|       from small pool |      21    |     101    |   12985    |   12964    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      49    |      51    |   42112 K  |   42112 K  |
|       from large pool |      34    |      35    |   22239 K  |   22239 K  |
|       from small pool |      15    |      28    |   19872 K  |   19872 K  |
|===========================================================================|

2022-05-28 12:55:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:55:50 | INFO | train_inner | epoch 009:   3352 / 6032 loss=4.858, nll_loss=3.173, ppl=9.02, wps=31970.9, ups=12.35, wpb=2589.8, bsz=128, num_updates=51500, lr=0.000195085, gnorm=0.987, loss_scale=8, train_wall=8, gb_free=19.9, wall=4290
2022-05-28 12:55:58 | INFO | train_inner | epoch 009:   3452 / 6032 loss=4.811, nll_loss=3.119, ppl=8.69, wps=32384.9, ups=12.85, wpb=2519.8, bsz=128, num_updates=51600, lr=0.000194896, gnorm=1.003, loss_scale=8, train_wall=8, gb_free=19.5, wall=4297
2022-05-28 12:56:06 | INFO | train_inner | epoch 009:   3552 / 6032 loss=4.935, nll_loss=3.262, ppl=9.59, wps=33817.6, ups=12.39, wpb=2730.2, bsz=128, num_updates=51700, lr=0.000194708, gnorm=0.962, loss_scale=8, train_wall=8, gb_free=20.3, wall=4305
2022-05-28 12:56:14 | INFO | train_inner | epoch 009:   3652 / 6032 loss=4.793, nll_loss=3.099, ppl=8.57, wps=33748, ups=12.93, wpb=2610.5, bsz=128, num_updates=51800, lr=0.00019452, gnorm=0.985, loss_scale=8, train_wall=8, gb_free=19.7, wall=4313
2022-05-28 12:56:22 | INFO | train_inner | epoch 009:   3752 / 6032 loss=4.752, nll_loss=3.053, ppl=8.3, wps=32520.9, ups=12.68, wpb=2564.6, bsz=128, num_updates=51900, lr=0.000194332, gnorm=0.978, loss_scale=8, train_wall=8, gb_free=19.5, wall=4321
2022-05-28 12:56:30 | INFO | train_inner | epoch 009:   3852 / 6032 loss=5.03, nll_loss=3.37, ppl=10.34, wps=32884.3, ups=11.57, wpb=2842.3, bsz=128, num_updates=52000, lr=0.000194145, gnorm=1.005, loss_scale=8, train_wall=8, gb_free=18.2, wall=4330
2022-05-28 12:56:38 | INFO | train_inner | epoch 009:   3952 / 6032 loss=4.789, nll_loss=3.096, ppl=8.55, wps=33513.1, ups=12.9, wpb=2597.2, bsz=126.8, num_updates=52100, lr=0.000193959, gnorm=1.016, loss_scale=8, train_wall=8, gb_free=20.6, wall=4337
2022-05-28 12:56:46 | INFO | train_inner | epoch 009:   4052 / 6032 loss=4.776, nll_loss=3.081, ppl=8.46, wps=32145.9, ups=12.44, wpb=2583.1, bsz=128, num_updates=52200, lr=0.000193773, gnorm=0.991, loss_scale=8, train_wall=8, gb_free=19.6, wall=4345
2022-05-28 12:56:54 | INFO | train_inner | epoch 009:   4152 / 6032 loss=4.97, nll_loss=3.302, ppl=9.86, wps=34693.9, ups=12.39, wpb=2799.8, bsz=128, num_updates=52300, lr=0.000193587, gnorm=0.985, loss_scale=8, train_wall=8, gb_free=20.2, wall=4353
2022-05-28 12:57:02 | INFO | train_inner | epoch 009:   4252 / 6032 loss=4.811, nll_loss=3.122, ppl=8.7, wps=33682.6, ups=12.74, wpb=2643.2, bsz=128, num_updates=52400, lr=0.000193403, gnorm=0.984, loss_scale=8, train_wall=8, gb_free=21, wall=4361
2022-05-28 12:57:10 | INFO | train_inner | epoch 009:   4352 / 6032 loss=4.783, nll_loss=3.088, ppl=8.5, wps=33828.4, ups=13.07, wpb=2588.8, bsz=128, num_updates=52500, lr=0.000193218, gnorm=1.009, loss_scale=8, train_wall=7, gb_free=20.3, wall=4369
2022-05-28 12:57:18 | INFO | train_inner | epoch 009:   4452 / 6032 loss=4.891, nll_loss=3.211, ppl=9.26, wps=35183.8, ups=12.63, wpb=2786.1, bsz=128, num_updates=52600, lr=0.000193035, gnorm=0.976, loss_scale=8, train_wall=8, gb_free=20.1, wall=4377
2022-05-28 12:57:26 | INFO | train_inner | epoch 009:   4552 / 6032 loss=4.934, nll_loss=3.262, ppl=9.59, wps=35111.3, ups=12.37, wpb=2837.3, bsz=128, num_updates=52700, lr=0.000192851, gnorm=0.981, loss_scale=8, train_wall=8, gb_free=20, wall=4385
2022-05-28 12:57:34 | INFO | train_inner | epoch 009:   4652 / 6032 loss=4.887, nll_loss=3.209, ppl=9.24, wps=36445.9, ups=12.1, wpb=3013.1, bsz=128, num_updates=52800, lr=0.000192669, gnorm=0.986, loss_scale=8, train_wall=8, gb_free=19.8, wall=4393
2022-05-28 12:57:42 | INFO | train_inner | epoch 009:   4752 / 6032 loss=4.866, nll_loss=3.184, ppl=9.09, wps=33720.3, ups=12.2, wpb=2765, bsz=128, num_updates=52900, lr=0.000192486, gnorm=0.977, loss_scale=8, train_wall=8, gb_free=18.9, wall=4401
2022-05-28 12:57:50 | INFO | train_inner | epoch 009:   4852 / 6032 loss=4.871, nll_loss=3.19, ppl=9.13, wps=34605.5, ups=12.39, wpb=2792.2, bsz=128, num_updates=53000, lr=0.000192305, gnorm=0.979, loss_scale=8, train_wall=8, gb_free=20.7, wall=4410
2022-05-28 12:57:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 113.69 MiB free; 21.92 GiB reserved in total by PyTorch)
2022-05-28 12:57:58 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 103          |        cudaMalloc retries: 227       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18611 MB |   18674 MB |  324163 GB |  324144 GB |
|       from large pool |   18571 MB |   18634 MB |  316944 GB |  316925 GB |
|       from small pool |      40 MB |      73 MB |    7218 GB |    7218 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18611 MB |   18674 MB |  324163 GB |  324144 GB |
|       from large pool |   18571 MB |   18634 MB |  316944 GB |  316925 GB |
|       from small pool |      40 MB |      73 MB |    7218 GB |    7218 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22446 MB |   22446 MB |    1106 GB |    1084 GB |
|       from large pool |   22404 MB |   22404 MB |    1080 GB |    1058 GB |
|       from small pool |      42 MB |     202 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3834 MB |    3889 MB |  447006 GB |  447002 GB |
|       from large pool |    3832 MB |    3887 MB |  438488 GB |  438485 GB |
|       from small pool |       1 MB |      12 MB |    8517 GB |    8517 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   85306 K  |   85305 K  |
|       from large pool |     271    |     275    |   40345 K  |   40345 K  |
|       from small pool |     321    |     466    |   44960 K  |   44960 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   85306 K  |   85305 K  |
|       from large pool |     271    |     275    |   40345 K  |   40345 K  |
|       from small pool |     321    |     466    |   44960 K  |   44960 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      63    |     144    |   13733    |   13670    |
|       from large pool |      42    |      43    |     592    |     550    |
|       from small pool |      21    |     101    |   13141    |   13120    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      47    |      60    |   43456 K  |   43456 K  |
|       from large pool |      36    |      36    |   22955 K  |   22955 K  |
|       from small pool |      11    |      45    |   20500 K  |   20500 K  |
|===========================================================================|

2022-05-28 12:57:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:57:59 | INFO | train_inner | epoch 009:   4953 / 6032 loss=4.806, nll_loss=3.115, ppl=8.66, wps=33048.1, ups=12.16, wpb=2717.9, bsz=128, num_updates=53100, lr=0.000192124, gnorm=0.989, loss_scale=8, train_wall=8, gb_free=18.2, wall=4418
2022-05-28 12:58:07 | INFO | train_inner | epoch 009:   5053 / 6032 loss=4.845, nll_loss=3.162, ppl=8.95, wps=31232.4, ups=11.54, wpb=2706.6, bsz=128, num_updates=53200, lr=0.000191943, gnorm=0.969, loss_scale=8, train_wall=8, gb_free=15.8, wall=4426
2022-05-28 12:58:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 19.31 GiB already allocated; 413.69 MiB free; 21.63 GiB reserved in total by PyTorch)
2022-05-28 12:58:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 104          |        cudaMalloc retries: 231       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15208 MB |   19774 MB |  324938 GB |  324923 GB |
|       from large pool |   15169 MB |   19735 MB |  317701 GB |  317686 GB |
|       from small pool |      39 MB |      73 MB |    7237 GB |    7236 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15208 MB |   19774 MB |  324938 GB |  324923 GB |
|       from large pool |   15169 MB |   19735 MB |  317701 GB |  317686 GB |
|       from small pool |      39 MB |      73 MB |    7237 GB |    7236 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22146 MB |   22146 MB |    1140 GB |    1118 GB |
|       from large pool |   22102 MB |   22102 MB |    1114 GB |    1092 GB |
|       from small pool |      44 MB |     196 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2371 MB |    2772 MB |  447750 GB |  447748 GB |
|       from large pool |    2366 MB |    2767 MB |  439212 GB |  439209 GB |
|       from small pool |       4 MB |      31 MB |    8538 GB |    8538 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   85500 K  |   85499 K  |
|       from large pool |     271    |     275    |   40435 K  |   40435 K  |
|       from small pool |     321    |     466    |   45064 K  |   45064 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   85500 K  |   85499 K  |
|       from large pool |     271    |     275    |   40435 K  |   40435 K  |
|       from small pool |     321    |     466    |   45064 K  |   45064 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     125    |   13917    |   13868    |
|       from large pool |      27    |      27    |     619    |     592    |
|       from small pool |      22    |      98    |   13298    |   13276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      79    |   43555 K  |   43555 K  |
|       from large pool |      16    |      16    |   23005 K  |   23005 K  |
|       from small pool |      11    |      75    |   20550 K  |   20550 K  |
|===========================================================================|

2022-05-28 12:58:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:58:16 | INFO | train_inner | epoch 009:   5154 / 6032 loss=5.018, nll_loss=3.359, ppl=10.26, wps=30886.8, ups=11.68, wpb=2644.2, bsz=128, num_updates=53300, lr=0.000191763, gnorm=1.032, loss_scale=8, train_wall=8, gb_free=18.8, wall=4435
2022-05-28 12:58:24 | INFO | train_inner | epoch 009:   5254 / 6032 loss=5.048, nll_loss=3.392, ppl=10.5, wps=34567, ups=11.8, wpb=2929.5, bsz=128, num_updates=53400, lr=0.000191583, gnorm=1.003, loss_scale=8, train_wall=8, gb_free=19.1, wall=4443
2022-05-28 12:58:32 | INFO | train_inner | epoch 009:   5354 / 6032 loss=4.774, nll_loss=3.079, ppl=8.45, wps=32745.2, ups=12.82, wpb=2555.1, bsz=128, num_updates=53500, lr=0.000191404, gnorm=0.989, loss_scale=8, train_wall=8, gb_free=20, wall=4451
2022-05-28 12:58:40 | INFO | train_inner | epoch 009:   5454 / 6032 loss=4.873, nll_loss=3.192, ppl=9.14, wps=34741.4, ups=12.71, wpb=2732.4, bsz=128, num_updates=53600, lr=0.000191225, gnorm=0.98, loss_scale=8, train_wall=8, gb_free=20.6, wall=4459
2022-05-28 12:58:48 | INFO | train_inner | epoch 009:   5554 / 6032 loss=4.701, nll_loss=2.997, ppl=7.98, wps=32434.7, ups=13.04, wpb=2487.8, bsz=128, num_updates=53700, lr=0.000191047, gnorm=1.003, loss_scale=8, train_wall=7, gb_free=19, wall=4467
2022-05-28 12:58:55 | INFO | train_inner | epoch 009:   5654 / 6032 loss=4.844, nll_loss=3.161, ppl=8.94, wps=31816.3, ups=12.76, wpb=2493.5, bsz=128, num_updates=53800, lr=0.00019087, gnorm=1.02, loss_scale=8, train_wall=8, gb_free=21, wall=4475
2022-05-28 12:58:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 3.72 GiB free; 18.31 GiB reserved in total by PyTorch)
2022-05-28 12:58:59 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 105          |        cudaMalloc retries: 234       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13632 MB |   17563 MB |  328622 GB |  328608 GB |
|       from large pool |   13593 MB |   17523 MB |  321304 GB |  321290 GB |
|       from small pool |      39 MB |      73 MB |    7318 GB |    7318 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13632 MB |   17563 MB |  328622 GB |  328608 GB |
|       from large pool |   13593 MB |   17523 MB |  321304 GB |  321290 GB |
|       from small pool |      39 MB |      73 MB |    7318 GB |    7318 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18754 MB |   22110 MB |    1151 GB |    1133 GB |
|       from large pool |   18708 MB |   21908 MB |    1125 GB |    1107 GB |
|       from small pool |      46 MB |     202 MB |      26 GB |      26 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1189 MB |    2828 MB |  451570 GB |  451569 GB |
|       from large pool |    1182 MB |    2821 MB |  442935 GB |  442934 GB |
|       from small pool |       6 MB |       9 MB |    8634 GB |    8634 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   86474 K  |   86473 K  |
|       from large pool |     271    |     275    |   40895 K  |   40895 K  |
|       from small pool |     321    |     466    |   45578 K  |   45578 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   86474 K  |   86473 K  |
|       from large pool |     271    |     275    |   40895 K  |   40895 K  |
|       from small pool |     321    |     466    |   45578 K  |   45578 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     128    |   14075    |   14026    |
|       from large pool |      26    |      27    |     622    |     596    |
|       from small pool |      23    |     101    |   13453    |   13430    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      34    |   44046 K  |   44046 K  |
|       from large pool |      20    |      21    |   23261 K  |   23261 K  |
|       from small pool |      12    |      20    |   20784 K  |   20784 K  |
|===========================================================================|

2022-05-28 12:58:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:59:04 | INFO | train_inner | epoch 009:   5755 / 6032 loss=4.792, nll_loss=3.101, ppl=8.58, wps=31113, ups=11.99, wpb=2594.2, bsz=128, num_updates=53900, lr=0.000190693, gnorm=0.992, loss_scale=8, train_wall=8, gb_free=18.8, wall=4483
2022-05-28 12:59:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 3.34 GiB free; 18.69 GiB reserved in total by PyTorch)
2022-05-28 12:59:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 106          |        cudaMalloc retries: 235       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13956 MB |   18278 MB |  329219 GB |  329205 GB |
|       from large pool |   13917 MB |   18239 MB |  321885 GB |  321871 GB |
|       from small pool |      39 MB |      73 MB |    7334 GB |    7334 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13956 MB |   18278 MB |  329219 GB |  329205 GB |
|       from large pool |   13917 MB |   18239 MB |  321885 GB |  321871 GB |
|       from small pool |      39 MB |      73 MB |    7334 GB |    7334 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19142 MB |   19300 MB |    1155 GB |    1137 GB |
|       from large pool |   19098 MB |   19098 MB |    1129 GB |    1110 GB |
|       from small pool |      44 MB |     202 MB |      26 GB |      26 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     863 MB |    2772 MB |  452189 GB |  452188 GB |
|       from large pool |     858 MB |    2768 MB |  443536 GB |  443535 GB |
|       from small pool |       4 MB |      15 MB |    8653 GB |    8653 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   86642 K  |   86641 K  |
|       from large pool |     271    |     275    |   40972 K  |   40972 K  |
|       from small pool |     321    |     466    |   45669 K  |   45669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   86642 K  |   86641 K  |
|       from large pool |     271    |     275    |   40972 K  |   40972 K  |
|       from small pool |     321    |     466    |   45669 K  |   45669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      48    |     127    |   14154    |   14106    |
|       from large pool |      26    |      26    |     623    |     597    |
|       from small pool |      22    |     101    |   13531    |   13509    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      60    |   44131 K  |   44131 K  |
|       from large pool |      19    |      19    |   23304 K  |   23304 K  |
|       from small pool |      12    |      44    |   20827 K  |   20827 K  |
|===========================================================================|

2022-05-28 12:59:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:59:12 | INFO | train_inner | epoch 009:   5856 / 6032 loss=4.837, nll_loss=3.152, ppl=8.89, wps=34033.6, ups=12.11, wpb=2810.3, bsz=128, num_updates=54000, lr=0.000190516, gnorm=0.969, loss_scale=8, train_wall=8, gb_free=17.6, wall=4491
2022-05-28 12:59:20 | INFO | train_inner | epoch 009:   5956 / 6032 loss=4.832, nll_loss=3.146, ppl=8.85, wps=35178.2, ups=12.71, wpb=2766.9, bsz=128, num_updates=54100, lr=0.00019034, gnorm=0.959, loss_scale=8, train_wall=8, gb_free=21, wall=4499
2022-05-28 12:59:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 3.29 GiB free; 18.74 GiB reserved in total by PyTorch)
2022-05-28 12:59:22 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 107          |        cudaMalloc retries: 237       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14033 MB |   18403 MB |  330308 GB |  330294 GB |
|       from large pool |   13994 MB |   18364 MB |  322954 GB |  322940 GB |
|       from small pool |      39 MB |      73 MB |    7354 GB |    7354 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14033 MB |   18403 MB |  330308 GB |  330294 GB |
|       from large pool |   13994 MB |   18364 MB |  322954 GB |  322940 GB |
|       from small pool |      39 MB |      73 MB |    7354 GB |    7354 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19190 MB |   21278 MB |    1166 GB |    1147 GB |
|       from large pool |   19148 MB |   21076 MB |    1139 GB |    1121 GB |
|       from small pool |      42 MB |     202 MB |      26 GB |      26 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     784 MB |    2767 MB |  453348 GB |  453348 GB |
|       from large pool |     781 MB |    2765 MB |  444671 GB |  444670 GB |
|       from small pool |       2 MB |       5 MB |    8677 GB |    8677 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   86917 K  |   86917 K  |
|       from large pool |     271    |     275    |   41106 K  |   41106 K  |
|       from small pool |     321    |     466    |   45810 K  |   45810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   86917 K  |   86917 K  |
|       from large pool |     271    |     275    |   41106 K  |   41106 K  |
|       from small pool |     321    |     466    |   45810 K  |   45810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     128    |   14236    |   14189    |
|       from large pool |      26    |      27    |     626    |     600    |
|       from small pool |      21    |     101    |   13610    |   13589    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      37    |   44270 K  |   44270 K  |
|       from large pool |      17    |      18    |   23378 K  |   23378 K  |
|       from small pool |       9    |      22    |   20891 K  |   20891 K  |
|===========================================================================|

2022-05-28 12:59:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 12:59:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 12:59:46 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.315 | nll_loss 3.65 | ppl 12.56 | wps 103187 | wpb 2687.4 | bsz 128 | num_updates 54175 | best_loss 5.315
2022-05-28 12:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 54175 updates
2022-05-28 12:59:46 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint9.pt
2022-05-28 12:59:50 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint9.pt
2022-05-28 12:59:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint9.pt (epoch 9 @ 54175 updates, score 5.315) (writing took 10.329120363108814 seconds)
2022-05-28 12:59:57 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-05-28 12:59:57 | INFO | train | epoch 009 | loss 4.831 | nll_loss 3.142 | ppl 8.83 | wps 31390.2 | ups 11.74 | wpb 2673 | bsz 128 | num_updates 54175 | lr 0.000190208 | gnorm 0.984 | loss_scale 8 | train_wall 467 | gb_free 20.3 | wall 4536
2022-05-28 12:59:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 12:59:57 | INFO | fairseq.trainer | begin training epoch 10
2022-05-28 12:59:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 12:59:59 | INFO | train_inner | epoch 010:     25 / 6032 loss=4.829, nll_loss=3.142, ppl=8.83, wps=6670.2, ups=2.57, wpb=2597.4, bsz=128, num_updates=54200, lr=0.000190164, gnorm=0.986, loss_scale=8, train_wall=8, gb_free=20.2, wall=4538
2022-05-28 13:00:07 | INFO | train_inner | epoch 010:    125 / 6032 loss=4.694, nll_loss=2.979, ppl=7.89, wps=33369, ups=12.63, wpb=2642.1, bsz=128, num_updates=54300, lr=0.000189989, gnorm=0.973, loss_scale=8, train_wall=8, gb_free=18.1, wall=4546
2022-05-28 13:00:15 | INFO | train_inner | epoch 010:    225 / 6032 loss=4.882, nll_loss=3.197, ppl=9.17, wps=35002.2, ups=11.89, wpb=2944.5, bsz=128, num_updates=54400, lr=0.000189814, gnorm=0.969, loss_scale=8, train_wall=8, gb_free=20.8, wall=4554
2022-05-28 13:00:23 | INFO | train_inner | epoch 010:    325 / 6032 loss=4.703, nll_loss=2.99, ppl=7.94, wps=36720.1, ups=12.46, wpb=2946.1, bsz=128, num_updates=54500, lr=0.00018964, gnorm=0.938, loss_scale=8, train_wall=8, gb_free=19.6, wall=4562
2022-05-28 13:00:31 | INFO | train_inner | epoch 010:    425 / 6032 loss=4.565, nll_loss=2.834, ppl=7.13, wps=30655.4, ups=12.95, wpb=2367.5, bsz=128, num_updates=54600, lr=0.000189466, gnorm=1.006, loss_scale=8, train_wall=8, gb_free=19.5, wall=4570
2022-05-28 13:00:39 | INFO | train_inner | epoch 010:    525 / 6032 loss=4.794, nll_loss=3.097, ppl=8.56, wps=34673.5, ups=12.3, wpb=2818.8, bsz=128, num_updates=54700, lr=0.000189293, gnorm=0.991, loss_scale=8, train_wall=8, gb_free=20.8, wall=4578
2022-05-28 13:00:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 161.69 MiB free; 21.87 GiB reserved in total by PyTorch)
2022-05-28 13:00:44 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 108          |        cudaMalloc retries: 240       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14032 MB |   18402 MB |  335487 GB |  335473 GB |
|       from large pool |   13993 MB |   18363 MB |  328021 GB |  328007 GB |
|       from small pool |      39 MB |      73 MB |    7465 GB |    7465 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14032 MB |   18402 MB |  335487 GB |  335473 GB |
|       from large pool |   13993 MB |   18363 MB |  328021 GB |  328007 GB |
|       from small pool |      39 MB |      73 MB |    7465 GB |    7465 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22398 MB |   22556 MB |    1189 GB |    1167 GB |
|       from large pool |   22354 MB |   22354 MB |    1162 GB |    1140 GB |
|       from small pool |      44 MB |     202 MB |      26 GB |      26 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     909 MB |    3995 MB |  461338 GB |  461337 GB |
|       from large pool |     904 MB |    3990 MB |  452530 GB |  452529 GB |
|       from small pool |       4 MB |      10 MB |    8807 GB |    8807 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   88223 K  |   88222 K  |
|       from large pool |     271    |     275    |   41741 K  |   41741 K  |
|       from small pool |     321    |     466    |   46481 K  |   46481 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   88223 K  |   88222 K  |
|       from large pool |     271    |     275    |   41741 K  |   41741 K  |
|       from small pool |     321    |     466    |   46481 K  |   46481 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     124    |   14425    |   14380    |
|       from large pool |      23    |      23    |     632    |     609    |
|       from small pool |      22    |     101    |   13793    |   13771    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      32    |   44917 K  |   44917 K  |
|       from large pool |      16    |      17    |   23728 K  |   23728 K  |
|       from small pool |      14    |      22    |   21189 K  |   21189 K  |
|===========================================================================|

2022-05-28 13:00:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:00:47 | INFO | train_inner | epoch 010:    626 / 6032 loss=4.731, nll_loss=3.021, ppl=8.12, wps=33857.1, ups=12.35, wpb=2741.2, bsz=128, num_updates=54800, lr=0.00018912, gnorm=0.98, loss_scale=8, train_wall=8, gb_free=20.5, wall=4586
2022-05-28 13:00:55 | INFO | train_inner | epoch 010:    726 / 6032 loss=4.656, nll_loss=2.937, ppl=7.66, wps=32473.8, ups=13.19, wpb=2462.4, bsz=128, num_updates=54900, lr=0.000188948, gnorm=1.015, loss_scale=8, train_wall=7, gb_free=17.2, wall=4594
2022-05-28 13:01:03 | INFO | train_inner | epoch 010:    826 / 6032 loss=4.703, nll_loss=2.991, ppl=7.95, wps=33078.4, ups=12.71, wpb=2603.2, bsz=128, num_updates=55000, lr=0.000188776, gnorm=1.008, loss_scale=8, train_wall=8, gb_free=19.6, wall=4602
2022-05-28 13:01:11 | INFO | train_inner | epoch 010:    926 / 6032 loss=4.843, nll_loss=3.15, ppl=8.88, wps=31299, ups=11.41, wpb=2742.5, bsz=128, num_updates=55100, lr=0.000188605, gnorm=1.001, loss_scale=8, train_wall=9, gb_free=19.8, wall=4611
2022-05-28 13:01:20 | INFO | train_inner | epoch 010:   1026 / 6032 loss=4.885, nll_loss=3.2, ppl=9.19, wps=33445.7, ups=11.99, wpb=2788.5, bsz=128, num_updates=55200, lr=0.000188434, gnorm=0.986, loss_scale=8, train_wall=8, gb_free=20.4, wall=4619
2022-05-28 13:01:28 | INFO | train_inner | epoch 010:   1126 / 6032 loss=4.615, nll_loss=2.891, ppl=7.42, wps=31886.9, ups=12.67, wpb=2516.7, bsz=128, num_updates=55300, lr=0.000188263, gnorm=1, loss_scale=8, train_wall=8, gb_free=17.1, wall=4627
2022-05-28 13:01:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 3.65 GiB free; 18.38 GiB reserved in total by PyTorch)
2022-05-28 13:01:32 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 109          |        cudaMalloc retries: 244       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13821 MB |   17923 MB |  338947 GB |  338933 GB |
|       from large pool |   13781 MB |   17883 MB |  331399 GB |  331386 GB |
|       from small pool |      39 MB |      73 MB |    7547 GB |    7547 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13821 MB |   17923 MB |  338947 GB |  338933 GB |
|       from large pool |   13781 MB |   17883 MB |  331399 GB |  331386 GB |
|       from small pool |      39 MB |      73 MB |    7547 GB |    7547 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18820 MB |   22208 MB |    1210 GB |    1192 GB |
|       from large pool |   18780 MB |   22006 MB |    1183 GB |    1165 GB |
|       from small pool |      40 MB |     202 MB |      27 GB |      27 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     896 MB |    3390 MB |  467605 GB |  467604 GB |
|       from large pool |     896 MB |    3389 MB |  458700 GB |  458699 GB |
|       from small pool |       0 MB |       4 MB |    8904 GB |    8904 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   89156 K  |   89156 K  |
|       from large pool |     271    |     275    |   42175 K  |   42175 K  |
|       from small pool |     321    |     466    |   46980 K  |   46980 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   89156 K  |   89156 K  |
|       from large pool |     271    |     275    |   42175 K  |   42175 K  |
|       from small pool |     321    |     466    |   46980 K  |   46980 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     125    |   14611    |   14568    |
|       from large pool |      23    |      24    |     638    |     615    |
|       from small pool |      20    |     101    |   13973    |   13953    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      28    |   45390 K  |   45390 K  |
|       from large pool |      17    |      17    |   23971 K  |   23971 K  |
|       from small pool |       8    |      20    |   21418 K  |   21418 K  |
|===========================================================================|

2022-05-28 13:01:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:01:36 | INFO | train_inner | epoch 010:   1227 / 6032 loss=4.766, nll_loss=3.063, ppl=8.36, wps=31458.9, ups=11.69, wpb=2691.4, bsz=128, num_updates=55400, lr=0.000188093, gnorm=1.001, loss_scale=8, train_wall=8, gb_free=17, wall=4635
2022-05-28 13:01:44 | INFO | train_inner | epoch 010:   1327 / 6032 loss=4.837, nll_loss=3.144, ppl=8.84, wps=34486.7, ups=12.48, wpb=2763.5, bsz=128, num_updates=55500, lr=0.000187924, gnorm=1.001, loss_scale=8, train_wall=8, gb_free=19.2, wall=4643
2022-05-28 13:01:52 | INFO | train_inner | epoch 010:   1427 / 6032 loss=4.736, nll_loss=3.028, ppl=8.16, wps=34032.1, ups=12.71, wpb=2677.3, bsz=128, num_updates=55600, lr=0.000187755, gnorm=1.016, loss_scale=8, train_wall=8, gb_free=20.5, wall=4651
2022-05-28 13:01:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.45 GiB already allocated; 445.69 MiB free; 21.60 GiB reserved in total by PyTorch)
2022-05-28 13:01:57 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 110          |        cudaMalloc retries: 246       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21961 MB |   22261 MB |  340780 GB |  340758 GB |
|       from large pool |   21919 MB |   22219 MB |  333192 GB |  333171 GB |
|       from small pool |      42 MB |      73 MB |    7587 GB |    7587 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21961 MB |   22261 MB |  340780 GB |  340758 GB |
|       from large pool |   21919 MB |   22219 MB |  333192 GB |  333171 GB |
|       from small pool |      42 MB |      73 MB |    7587 GB |    7587 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22114 MB |   22414 MB |    1218 GB |    1196 GB |
|       from large pool |   22070 MB |   22370 MB |    1191 GB |    1169 GB |
|       from small pool |      44 MB |     202 MB |      27 GB |      27 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  155957 KB |    5017 MB |  470723 GB |  470722 GB |
|       from large pool |  154540 KB |    5015 MB |  461771 GB |  461770 GB |
|       from small pool |    1417 KB |       4 MB |    8952 GB |    8952 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |   89629 K  |   89628 K  |
|       from large pool |     214    |     215    |   42399 K  |   42399 K  |
|       from small pool |     302    |     466    |   47230 K  |   47229 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |   89629 K  |   89628 K  |
|       from large pool |     214    |     215    |   42399 K  |   42399 K  |
|       from small pool |     302    |     466    |   47230 K  |   47229 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     153    |   14723    |   14649    |
|       from large pool |      52    |      53    |     669    |     617    |
|       from small pool |      22    |     101    |   14054    |   14032    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      40    |   45628 K  |   45628 K  |
|       from large pool |      26    |      26    |   24096 K  |   24096 K  |
|       from small pool |      14    |      22    |   21532 K  |   21532 K  |
|===========================================================================|

2022-05-28 13:01:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:02:00 | INFO | train_inner | epoch 010:   1528 / 6032 loss=4.764, nll_loss=3.062, ppl=8.35, wps=32827, ups=12.22, wpb=2687.3, bsz=128, num_updates=55700, lr=0.000187586, gnorm=1.004, loss_scale=8, train_wall=8, gb_free=19.2, wall=4659
2022-05-28 13:02:09 | INFO | train_inner | epoch 010:   1628 / 6032 loss=4.852, nll_loss=3.163, ppl=8.96, wps=33865.7, ups=11.9, wpb=2845.6, bsz=128, num_updates=55800, lr=0.000187418, gnorm=1.008, loss_scale=8, train_wall=8, gb_free=20.5, wall=4668
2022-05-28 13:02:16 | INFO | train_inner | epoch 010:   1728 / 6032 loss=4.698, nll_loss=2.985, ppl=7.92, wps=34378.2, ups=12.81, wpb=2684.1, bsz=128, num_updates=55900, lr=0.00018725, gnorm=0.995, loss_scale=8, train_wall=8, gb_free=20.4, wall=4676
2022-05-28 13:02:24 | INFO | train_inner | epoch 010:   1828 / 6032 loss=4.506, nll_loss=2.768, ppl=6.81, wps=32024.4, ups=13.18, wpb=2430.5, bsz=128, num_updates=56000, lr=0.000187083, gnorm=1.017, loss_scale=8, train_wall=7, gb_free=20.8, wall=4683
2022-05-28 13:02:33 | INFO | train_inner | epoch 010:   1928 / 6032 loss=4.903, nll_loss=3.222, ppl=9.33, wps=32857.7, ups=11.69, wpb=2811.9, bsz=128, num_updates=56100, lr=0.000186916, gnorm=1.016, loss_scale=8, train_wall=8, gb_free=19.3, wall=4692
2022-05-28 13:02:40 | INFO | train_inner | epoch 010:   2028 / 6032 loss=4.585, nll_loss=2.858, ppl=7.25, wps=32058.2, ups=12.9, wpb=2484.4, bsz=128, num_updates=56200, lr=0.00018675, gnorm=1.017, loss_scale=8, train_wall=8, gb_free=19.9, wall=4700
2022-05-28 13:02:49 | INFO | train_inner | epoch 010:   2128 / 6032 loss=4.792, nll_loss=3.096, ppl=8.55, wps=30805.6, ups=11.59, wpb=2657.4, bsz=128, num_updates=56300, lr=0.000186584, gnorm=1.046, loss_scale=8, train_wall=8, gb_free=18.1, wall=4708
2022-05-28 13:02:57 | INFO | train_inner | epoch 010:   2228 / 6032 loss=4.721, nll_loss=3.013, ppl=8.07, wps=34710, ups=12.57, wpb=2761.1, bsz=128, num_updates=56400, lr=0.000186418, gnorm=0.987, loss_scale=8, train_wall=8, gb_free=20.3, wall=4716
2022-05-28 13:03:06 | INFO | train_inner | epoch 010:   2328 / 6032 loss=4.911, nll_loss=3.23, ppl=9.38, wps=35017.6, ups=11.67, wpb=2999.8, bsz=128, num_updates=56500, lr=0.000186253, gnorm=0.998, loss_scale=8, train_wall=8, gb_free=14, wall=4725
2022-05-28 13:03:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 931.69 MiB free; 21.12 GiB reserved in total by PyTorch)
2022-05-28 13:03:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 111          |        cudaMalloc retries: 250       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13288 MB |   17218 MB |  346062 GB |  346049 GB |
|       from large pool |   13248 MB |   17179 MB |  338357 GB |  338344 GB |
|       from small pool |      39 MB |      73 MB |    7704 GB |    7704 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13288 MB |   17218 MB |  346062 GB |  346049 GB |
|       from large pool |   13248 MB |   17179 MB |  338357 GB |  338344 GB |
|       from small pool |      39 MB |      73 MB |    7704 GB |    7704 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21628 MB |   21790 MB |    1233 GB |    1211 GB |
|       from large pool |   21586 MB |   21586 MB |    1204 GB |    1183 GB |
|       from small pool |      42 MB |     204 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3773 MB |    4409 MB |  477732 GB |  477728 GB |
|       from large pool |    3771 MB |    4406 MB |  468641 GB |  468638 GB |
|       from small pool |       2 MB |      16 MB |    9090 GB |    9090 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   90999 K  |   90999 K  |
|       from large pool |     271    |     275    |   43042 K  |   43042 K  |
|       from small pool |     321    |     466    |   47957 K  |   47957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   90999 K  |   90999 K  |
|       from large pool |     271    |     275    |   43042 K  |   43042 K  |
|       from small pool |     321    |     466    |   47957 K  |   47957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      70    |     151    |   15042    |   14972    |
|       from large pool |      49    |      49    |     673    |     624    |
|       from small pool |      21    |     102    |   14369    |   14348    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      50    |      63    |   46339 K  |   46339 K  |
|       from large pool |      41    |      42    |   24472 K  |   24472 K  |
|       from small pool |       9    |      40    |   21867 K  |   21867 K  |
|===========================================================================|

2022-05-28 13:03:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:03:13 | INFO | train_inner | epoch 010:   2429 / 6032 loss=4.571, nll_loss=2.843, ppl=7.18, wps=31957, ups=12.85, wpb=2487.2, bsz=128, num_updates=56600, lr=0.000186089, gnorm=1.01, loss_scale=8, train_wall=7, gb_free=20.1, wall=4732
2022-05-28 13:03:21 | INFO | train_inner | epoch 010:   2529 / 6032 loss=4.54, nll_loss=2.806, ppl=6.99, wps=32204.9, ups=13.47, wpb=2391.1, bsz=128, num_updates=56700, lr=0.000185924, gnorm=1.015, loss_scale=8, train_wall=7, gb_free=20.5, wall=4740
2022-05-28 13:03:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 2.70 GiB free; 19.33 GiB reserved in total by PyTorch)
2022-05-28 13:03:26 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 112          |        cudaMalloc retries: 251       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13632 MB |   17563 MB |  347321 GB |  347307 GB |
|       from large pool |   13593 MB |   17523 MB |  339583 GB |  339570 GB |
|       from small pool |      39 MB |      73 MB |    7737 GB |    7737 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13632 MB |   17563 MB |  347321 GB |  347307 GB |
|       from large pool |   13593 MB |   17523 MB |  339583 GB |  339570 GB |
|       from small pool |      39 MB |      73 MB |    7737 GB |    7737 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19796 MB |   19958 MB |    1235 GB |    1216 GB |
|       from large pool |   19756 MB |   19756 MB |    1207 GB |    1188 GB |
|       from small pool |      40 MB |     202 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2231 MB |    2814 MB |  479673 GB |  479671 GB |
|       from large pool |    2230 MB |    2813 MB |  470543 GB |  470541 GB |
|       from small pool |       0 MB |      26 MB |    9129 GB |    9129 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   91353 K  |   91353 K  |
|       from large pool |     271    |     275    |   43204 K  |   43204 K  |
|       from small pool |     321    |     466    |   48149 K  |   48148 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   91353 K  |   91353 K  |
|       from large pool |     271    |     275    |   43204 K  |   43204 K  |
|       from small pool |     321    |     466    |   48149 K  |   48148 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     150    |   15123    |   15054    |
|       from large pool |      49    |      49    |     674    |     625    |
|       from small pool |      20    |     101    |   14449    |   14429    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      51    |      53    |   46524 K  |   46524 K  |
|       from large pool |      43    |      44    |   24567 K  |   24567 K  |
|       from small pool |       8    |      42    |   21956 K  |   21956 K  |
|===========================================================================|

2022-05-28 13:03:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:03:29 | INFO | train_inner | epoch 010:   2630 / 6032 loss=4.678, nll_loss=2.965, ppl=7.81, wps=32281.3, ups=12.81, wpb=2519.6, bsz=128, num_updates=56800, lr=0.000185761, gnorm=1.021, loss_scale=8, train_wall=7, gb_free=19.4, wall=4748
2022-05-28 13:03:36 | INFO | train_inner | epoch 010:   2730 / 6032 loss=4.523, nll_loss=2.787, ppl=6.9, wps=32927.9, ups=13.26, wpb=2483.1, bsz=128, num_updates=56900, lr=0.000185597, gnorm=1.015, loss_scale=8, train_wall=7, gb_free=19.8, wall=4755
2022-05-28 13:03:44 | INFO | train_inner | epoch 010:   2830 / 6032 loss=4.851, nll_loss=3.164, ppl=8.96, wps=35822.2, ups=12.64, wpb=2835, bsz=128, num_updates=57000, lr=0.000185435, gnorm=1.012, loss_scale=8, train_wall=8, gb_free=20.1, wall=4763
2022-05-28 13:03:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 19.32 GiB already allocated; 1.46 GiB free; 20.57 GiB reserved in total by PyTorch)
2022-05-28 13:03:52 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 113          |        cudaMalloc retries: 253       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15218 MB |   19784 MB |  349258 GB |  349243 GB |
|       from large pool |   15178 MB |   19744 MB |  341481 GB |  341466 GB |
|       from small pool |      39 MB |      73 MB |    7777 GB |    7776 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15218 MB |   19784 MB |  349258 GB |  349243 GB |
|       from large pool |   15178 MB |   19744 MB |  341481 GB |  341466 GB |
|       from small pool |      39 MB |      73 MB |    7777 GB |    7776 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21064 MB |   21064 MB |    1244 GB |    1224 GB |
|       from large pool |   21024 MB |   21024 MB |    1216 GB |    1196 GB |
|       from small pool |      40 MB |     198 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1279 MB |    2936 MB |  482616 GB |  482615 GB |
|       from large pool |    1279 MB |    2935 MB |  473439 GB |  473438 GB |
|       from small pool |       0 MB |       4 MB |    9176 GB |    9176 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   91872 K  |   91871 K  |
|       from large pool |     271    |     275    |   43454 K  |   43454 K  |
|       from small pool |     321    |     466    |   48417 K  |   48417 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   91872 K  |   91871 K  |
|       from large pool |     271    |     275    |   43454 K  |   43454 K  |
|       from small pool |     321    |     466    |   48417 K  |   48417 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     148    |   15204    |   15135    |
|       from large pool |      49    |      49    |     676    |     627    |
|       from small pool |      20    |      99    |   14528    |   14508    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      38    |   46793 K  |   46793 K  |
|       from large pool |      28    |      28    |   24715 K  |   24714 K  |
|       from small pool |       8    |      22    |   22078 K  |   22078 K  |
|===========================================================================|

2022-05-28 13:03:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:03:52 | INFO | train_inner | epoch 010:   2931 / 6032 loss=4.654, nll_loss=2.938, ppl=7.66, wps=32593.3, ups=12.4, wpb=2628.2, bsz=128, num_updates=57100, lr=0.000185272, gnorm=1.008, loss_scale=8, train_wall=8, gb_free=16.7, wall=4771
2022-05-28 13:04:00 | INFO | train_inner | epoch 010:   3031 / 6032 loss=4.755, nll_loss=3.052, ppl=8.29, wps=35173.3, ups=12.98, wpb=2709.8, bsz=128, num_updates=57200, lr=0.00018511, gnorm=1.026, loss_scale=8, train_wall=8, gb_free=15.1, wall=4779
2022-05-28 13:04:07 | INFO | train_inner | epoch 010:   3131 / 6032 loss=4.757, nll_loss=3.054, ppl=8.31, wps=33725.5, ups=13.21, wpb=2554, bsz=128, num_updates=57300, lr=0.000184948, gnorm=1.03, loss_scale=8, train_wall=7, gb_free=19.6, wall=4786
2022-05-28 13:04:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 901.69 MiB free; 21.15 GiB reserved in total by PyTorch)
2022-05-28 13:04:16 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 114          |        cudaMalloc retries: 256       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18612 MB |   18675 MB |  351043 GB |  351025 GB |
|       from large pool |   18572 MB |   18634 MB |  343226 GB |  343207 GB |
|       from small pool |      40 MB |      73 MB |    7817 GB |    7817 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18612 MB |   18675 MB |  351043 GB |  351025 GB |
|       from large pool |   18572 MB |   18634 MB |  343226 GB |  343207 GB |
|       from small pool |      40 MB |      73 MB |    7817 GB |    7817 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21658 MB |   21658 MB |    1257 GB |    1236 GB |
|       from large pool |   21616 MB |   21616 MB |    1228 GB |    1207 GB |
|       from small pool |      42 MB |     202 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3045 MB |    3772 MB |  485321 GB |  485318 GB |
|       from large pool |    3043 MB |    3770 MB |  476097 GB |  476094 GB |
|       from small pool |       1 MB |      19 MB |    9224 GB |    9224 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   92339 K  |   92339 K  |
|       from large pool |     271    |     275    |   43674 K  |   43674 K  |
|       from small pool |     321    |     466    |   48665 K  |   48664 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   92339 K  |   92339 K  |
|       from large pool |     271    |     275    |   43674 K  |   43674 K  |
|       from small pool |     321    |     466    |   48665 K  |   48664 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      69    |     150    |   15369    |   15300    |
|       from large pool |      48    |      49    |     679    |     631    |
|       from small pool |      21    |     101    |   14690    |   14669    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      50    |      62    |   47037 K  |   47037 K  |
|       from large pool |      42    |      42    |   24844 K  |   24844 K  |
|       from small pool |       8    |      49    |   22193 K  |   22193 K  |
|===========================================================================|

2022-05-28 13:04:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:04:16 | INFO | train_inner | epoch 010:   3232 / 6032 loss=4.693, nll_loss=2.983, ppl=7.91, wps=33118.9, ups=11.81, wpb=2805, bsz=128, num_updates=57400, lr=0.000184787, gnorm=0.982, loss_scale=8, train_wall=8, gb_free=20.6, wall=4795
2022-05-28 13:04:24 | INFO | train_inner | epoch 010:   3332 / 6032 loss=4.827, nll_loss=3.135, ppl=8.78, wps=35430.7, ups=12.38, wpb=2860.8, bsz=128, num_updates=57500, lr=0.000184627, gnorm=1.012, loss_scale=8, train_wall=8, gb_free=19, wall=4803
2022-05-28 13:04:32 | INFO | train_inner | epoch 010:   3432 / 6032 loss=4.77, nll_loss=3.072, ppl=8.41, wps=34600.9, ups=12.24, wpb=2827.8, bsz=128, num_updates=57600, lr=0.000184466, gnorm=1, loss_scale=8, train_wall=8, gb_free=19.5, wall=4811
2022-05-28 13:04:40 | INFO | train_inner | epoch 010:   3532 / 6032 loss=4.671, nll_loss=2.956, ppl=7.76, wps=32437.4, ups=13.19, wpb=2459.8, bsz=128, num_updates=57700, lr=0.000184306, gnorm=1.027, loss_scale=8, train_wall=7, gb_free=19.8, wall=4819
2022-05-28 13:04:47 | INFO | train_inner | epoch 010:   3632 / 6032 loss=4.705, nll_loss=2.997, ppl=7.99, wps=34759.6, ups=12.92, wpb=2690.8, bsz=128, num_updates=57800, lr=0.000184147, gnorm=0.973, loss_scale=8, train_wall=8, gb_free=18.2, wall=4827
2022-05-28 13:04:55 | INFO | train_inner | epoch 010:   3732 / 6032 loss=4.933, nll_loss=3.257, ppl=9.56, wps=35086.9, ups=12.52, wpb=2802.8, bsz=128, num_updates=57900, lr=0.000183988, gnorm=1.009, loss_scale=8, train_wall=8, gb_free=20.3, wall=4835
2022-05-28 13:05:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.69 GiB already allocated; 4.36 GiB free; 17.67 GiB reserved in total by PyTorch)
2022-05-28 13:05:00 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 115          |        cudaMalloc retries: 257       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17088 MB |   17145 MB |  354385 GB |  354368 GB |
|       from large pool |   17048 MB |   17105 MB |  346498 GB |  346482 GB |
|       from small pool |      39 MB |      73 MB |    7886 GB |    7886 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17088 MB |   17145 MB |  354385 GB |  354368 GB |
|       from large pool |   17048 MB |   17105 MB |  346498 GB |  346482 GB |
|       from small pool |      39 MB |      73 MB |    7886 GB |    7886 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18092 MB |   21818 MB |    1257 GB |    1240 GB |
|       from large pool |   18050 MB |   21616 MB |    1228 GB |    1211 GB |
|       from small pool |      42 MB |     202 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1003 MB |    3698 MB |  490393 GB |  490392 GB |
|       from large pool |    1001 MB |    3695 MB |  481085 GB |  481084 GB |
|       from small pool |       2 MB |      13 MB |    9307 GB |    9307 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   93210 K  |   93210 K  |
|       from large pool |     271    |     275    |   44091 K  |   44090 K  |
|       from small pool |     321    |     466    |   49119 K  |   49119 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   93210 K  |   93210 K  |
|       from large pool |     271    |     275    |   44091 K  |   44090 K  |
|       from small pool |     321    |     466    |   49119 K  |   49119 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |     149    |   15449    |   15381    |
|       from large pool |      47    |      48    |     679    |     632    |
|       from small pool |      21    |     101    |   14770    |   14749    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      46    |      61    |   47491 K  |   47491 K  |
|       from large pool |      35    |      35    |   25088 K  |   25088 K  |
|       from small pool |      11    |      48    |   22403 K  |   22403 K  |
|===========================================================================|

2022-05-28 13:05:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:05:03 | INFO | train_inner | epoch 010:   3833 / 6032 loss=4.715, nll_loss=3.008, ppl=8.04, wps=32750, ups=12.33, wpb=2656.9, bsz=128, num_updates=58000, lr=0.000183829, gnorm=1.012, loss_scale=8, train_wall=8, gb_free=17.9, wall=4843
2022-05-28 13:05:11 | INFO | train_inner | epoch 010:   3933 / 6032 loss=4.832, nll_loss=3.143, ppl=8.83, wps=34910.8, ups=12.68, wpb=2752.8, bsz=128, num_updates=58100, lr=0.000183671, gnorm=0.994, loss_scale=8, train_wall=8, gb_free=19.9, wall=4851
2022-05-28 13:05:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 855.69 MiB free; 21.20 GiB reserved in total by PyTorch)
2022-05-28 13:05:19 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 116          |        cudaMalloc retries: 259       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12959 MB |   16939 MB |  355793 GB |  355780 GB |
|       from large pool |   12920 MB |   16900 MB |  347877 GB |  347865 GB |
|       from small pool |      39 MB |      73 MB |    7915 GB |    7915 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12959 MB |   16939 MB |  355793 GB |  355780 GB |
|       from large pool |   12920 MB |   16900 MB |  347877 GB |  347865 GB |
|       from small pool |      39 MB |      73 MB |    7915 GB |    7915 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21704 MB |   21704 MB |    1264 GB |    1243 GB |
|       from large pool |   21664 MB |   21664 MB |    1235 GB |    1214 GB |
|       from small pool |      40 MB |     198 MB |      29 GB |      28 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4764 MB |    4764 MB |  492555 GB |  492550 GB |
|       from large pool |    4763 MB |    4763 MB |  483212 GB |  483208 GB |
|       from small pool |       0 MB |       3 MB |    9342 GB |    9342 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   93579 K  |   93578 K  |
|       from large pool |     271    |     275    |   44268 K  |   44267 K  |
|       from small pool |     321    |     466    |   49311 K  |   49311 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   93579 K  |   93578 K  |
|       from large pool |     271    |     275    |   44268 K  |   44267 K  |
|       from small pool |     321    |     466    |   49311 K  |   49311 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      68    |     147    |   15529    |   15461    |
|       from large pool |      48    |      48    |     681    |     633    |
|       from small pool |      20    |      99    |   14848    |   14828    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      41    |      43    |   47681 K  |   47681 K  |
|       from large pool |      32    |      32    |   25191 K  |   25191 K  |
|       from small pool |       9    |      16    |   22490 K  |   22490 K  |
|===========================================================================|

2022-05-28 13:05:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:05:20 | INFO | train_inner | epoch 010:   4034 / 6032 loss=4.792, nll_loss=3.096, ppl=8.55, wps=32333.5, ups=12.21, wpb=2647.1, bsz=128, num_updates=58200, lr=0.000183513, gnorm=1.024, loss_scale=8, train_wall=8, gb_free=16.7, wall=4859
2022-05-28 13:05:28 | INFO | train_inner | epoch 010:   4134 / 6032 loss=4.779, nll_loss=3.082, ppl=8.47, wps=32496.7, ups=12.31, wpb=2639.7, bsz=128, num_updates=58300, lr=0.000183355, gnorm=1.049, loss_scale=8, train_wall=8, gb_free=20.4, wall=4867
2022-05-28 13:05:36 | INFO | train_inner | epoch 010:   4234 / 6032 loss=4.75, nll_loss=3.05, ppl=8.28, wps=34005.4, ups=12.61, wpb=2697.4, bsz=128, num_updates=58400, lr=0.000183198, gnorm=1.013, loss_scale=8, train_wall=8, gb_free=19.3, wall=4875
2022-05-28 13:05:44 | INFO | train_inner | epoch 010:   4334 / 6032 loss=4.733, nll_loss=3.03, ppl=8.17, wps=33875.8, ups=12.66, wpb=2676.4, bsz=128, num_updates=58500, lr=0.000183042, gnorm=1.011, loss_scale=8, train_wall=8, gb_free=18.9, wall=4883
2022-05-28 13:05:52 | INFO | train_inner | epoch 010:   4434 / 6032 loss=4.851, nll_loss=3.163, ppl=8.96, wps=35342.4, ups=12.37, wpb=2857.4, bsz=128, num_updates=58600, lr=0.000182885, gnorm=1.031, loss_scale=8, train_wall=8, gb_free=17.4, wall=4891
2022-05-28 13:05:59 | INFO | train_inner | epoch 010:   4534 / 6032 loss=4.678, nll_loss=2.967, ppl=7.82, wps=31006.2, ups=12.89, wpb=2404.7, bsz=128, num_updates=58700, lr=0.00018273, gnorm=1.027, loss_scale=8, train_wall=8, gb_free=17.7, wall=4898
2022-05-28 13:06:07 | INFO | train_inner | epoch 010:   4634 / 6032 loss=4.836, nll_loss=3.146, ppl=8.85, wps=35366, ups=12.4, wpb=2851.2, bsz=128, num_updates=58800, lr=0.000182574, gnorm=0.997, loss_scale=8, train_wall=8, gb_free=19.8, wall=4907
2022-05-28 13:06:16 | INFO | train_inner | epoch 010:   4734 / 6032 loss=4.903, nll_loss=3.223, ppl=9.34, wps=33452.5, ups=11.57, wpb=2890.9, bsz=128, num_updates=58900, lr=0.000182419, gnorm=1.029, loss_scale=8, train_wall=8, gb_free=20.3, wall=4915
2022-05-28 13:06:24 | INFO | train_inner | epoch 010:   4834 / 6032 loss=4.873, nll_loss=3.189, ppl=9.12, wps=32653, ups=12.86, wpb=2539.9, bsz=128, num_updates=59000, lr=0.000182264, gnorm=1.09, loss_scale=8, train_wall=8, gb_free=20.2, wall=4923
2022-05-28 13:06:32 | INFO | train_inner | epoch 010:   4934 / 6032 loss=4.781, nll_loss=3.086, ppl=8.49, wps=32232, ups=13.03, wpb=2473.4, bsz=128, num_updates=59100, lr=0.00018211, gnorm=1.035, loss_scale=8, train_wall=7, gb_free=19.6, wall=4931
2022-05-28 13:06:39 | INFO | train_inner | epoch 010:   5034 / 6032 loss=4.78, nll_loss=3.083, ppl=8.48, wps=32129.5, ups=12.75, wpb=2519.7, bsz=128, num_updates=59200, lr=0.000181956, gnorm=1.042, loss_scale=8, train_wall=8, gb_free=19, wall=4938
2022-05-28 13:06:48 | INFO | train_inner | epoch 010:   5134 / 6032 loss=4.769, nll_loss=3.072, ppl=8.41, wps=32832.9, ups=12.23, wpb=2684.1, bsz=128, num_updates=59300, lr=0.000181803, gnorm=1.016, loss_scale=8, train_wall=8, gb_free=19.5, wall=4947
2022-05-28 13:06:56 | INFO | train_inner | epoch 010:   5234 / 6032 loss=4.801, nll_loss=3.107, ppl=8.61, wps=35007.5, ups=12.4, wpb=2823.4, bsz=128, num_updates=59400, lr=0.00018165, gnorm=1.024, loss_scale=8, train_wall=8, gb_free=19.9, wall=4955
2022-05-28 13:07:04 | INFO | train_inner | epoch 010:   5334 / 6032 loss=4.7, nll_loss=2.993, ppl=7.96, wps=33044.7, ups=12.57, wpb=2628.1, bsz=128, num_updates=59500, lr=0.000181497, gnorm=1.017, loss_scale=8, train_wall=8, gb_free=20.5, wall=4963
2022-05-28 13:07:11 | INFO | train_inner | epoch 010:   5434 / 6032 loss=4.688, nll_loss=2.982, ppl=7.9, wps=34140.5, ups=12.71, wpb=2686.9, bsz=128, num_updates=59600, lr=0.000181345, gnorm=1.002, loss_scale=8, train_wall=8, gb_free=16.3, wall=4971
2022-05-28 13:07:20 | INFO | train_inner | epoch 010:   5534 / 6032 loss=4.915, nll_loss=3.239, ppl=9.44, wps=35857.1, ups=12.34, wpb=2906.9, bsz=128, num_updates=59700, lr=0.000181193, gnorm=0.974, loss_scale=8, train_wall=8, gb_free=18.5, wall=4979
2022-05-28 13:07:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 361.69 MiB free; 21.68 GiB reserved in total by PyTorch)
2022-05-28 13:07:22 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 117          |        cudaMalloc retries: 263       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13958 MB |   18281 MB |  364877 GB |  364863 GB |
|       from large pool |   13919 MB |   18241 MB |  356754 GB |  356740 GB |
|       from small pool |      39 MB |      73 MB |    8122 GB |    8122 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13958 MB |   18281 MB |  364877 GB |  364863 GB |
|       from large pool |   13919 MB |   18241 MB |  356754 GB |  356740 GB |
|       from small pool |      39 MB |      73 MB |    8122 GB |    8122 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22198 MB |   22198 MB |    1285 GB |    1263 GB |
|       from large pool |   22152 MB |   22152 MB |    1255 GB |    1234 GB |
|       from small pool |      46 MB |     202 MB |      29 GB |      29 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3917 MB |    3917 MB |  506400 GB |  506397 GB |
|       from large pool |    3910 MB |    3910 MB |  496812 GB |  496809 GB |
|       from small pool |       6 MB |      10 MB |    9588 GB |    9588 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   95983 K  |   95982 K  |
|       from large pool |     271    |     275    |   45394 K  |   45394 K  |
|       from small pool |     321    |     466    |   50588 K  |   50588 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   95983 K  |   95982 K  |
|       from large pool |     271    |     275    |   45394 K  |   45394 K  |
|       from small pool |     321    |     466    |   50588 K  |   50588 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      71    |     149    |   15777    |   15706    |
|       from large pool |      48    |      48    |     687    |     639    |
|       from small pool |      23    |     101    |   15090    |   15067    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      43    |      44    |   48930 K  |   48930 K  |
|       from large pool |      32    |      32    |   25850 K  |   25850 K  |
|       from small pool |      11    |      23    |   23079 K  |   23079 K  |
|===========================================================================|

2022-05-28 13:07:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:07:28 | INFO | train_inner | epoch 010:   5635 / 6032 loss=4.864, nll_loss=3.18, ppl=9.06, wps=33427.7, ups=11.97, wpb=2791.9, bsz=128, num_updates=59800, lr=0.000181041, gnorm=1.009, loss_scale=8, train_wall=8, gb_free=20.7, wall=4987
2022-05-28 13:07:36 | INFO | train_inner | epoch 010:   5735 / 6032 loss=4.808, nll_loss=3.118, ppl=8.68, wps=32650.4, ups=12.21, wpb=2675.1, bsz=128, num_updates=59900, lr=0.00018089, gnorm=1.037, loss_scale=8, train_wall=8, gb_free=20.6, wall=4995
2022-05-28 13:07:44 | INFO | train_inner | epoch 010:   5835 / 6032 loss=4.647, nll_loss=2.932, ppl=7.63, wps=31568.4, ups=13.03, wpb=2422, bsz=128, num_updates=60000, lr=0.000180739, gnorm=1.029, loss_scale=8, train_wall=7, gb_free=20.4, wall=5003
2022-05-28 13:07:51 | INFO | train_inner | epoch 010:   5935 / 6032 loss=4.736, nll_loss=3.033, ppl=8.18, wps=34313.5, ups=13.05, wpb=2630.1, bsz=128, num_updates=60100, lr=0.000180589, gnorm=1.001, loss_scale=8, train_wall=7, gb_free=18.8, wall=5011
2022-05-28 13:07:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 13:08:19 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.332 | nll_loss 3.658 | ppl 12.63 | wps 104761 | wpb 2687.4 | bsz 128 | num_updates 60197 | best_loss 5.315
2022-05-28 13:08:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 60197 updates
2022-05-28 13:08:19 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint10.pt
2022-05-28 13:08:22 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint10.pt
2022-05-28 13:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint10.pt (epoch 10 @ 60197 updates, score 5.332) (writing took 7.339703879784793 seconds)
2022-05-28 13:08:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-05-28 13:08:26 | INFO | train | epoch 010 | loss 4.754 | nll_loss 3.052 | ppl 8.29 | wps 31659 | ups 11.83 | wpb 2676.7 | bsz 128 | num_updates 60197 | lr 0.000180443 | gnorm 1.01 | loss_scale 8 | train_wall 467 | gb_free 20.2 | wall 5045
2022-05-28 13:08:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 13:08:26 | INFO | fairseq.trainer | begin training epoch 11
2022-05-28 13:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 13:08:26 | INFO | train_inner | epoch 011:      3 / 6032 loss=4.7, nll_loss=2.993, ppl=7.96, wps=7442.8, ups=2.87, wpb=2595, bsz=126.8, num_updates=60200, lr=0.000180439, gnorm=1.05, loss_scale=8, train_wall=8, gb_free=19.1, wall=5045
2022-05-28 13:08:34 | INFO | train_inner | epoch 011:    103 / 6032 loss=4.579, nll_loss=2.847, ppl=7.19, wps=31562.8, ups=12.63, wpb=2498.4, bsz=128, num_updates=60300, lr=0.000180289, gnorm=1.01, loss_scale=8, train_wall=8, gb_free=18.8, wall=5053
2022-05-28 13:08:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.70 GiB total capacity; 15.42 GiB already allocated; 3.03 GiB free; 19.00 GiB reserved in total by PyTorch)
2022-05-28 13:08:36 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 118          |        cudaMalloc retries: 266       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12224 MB |   15788 MB |  369629 GB |  369617 GB |
|       from large pool |   12185 MB |   15749 MB |  361399 GB |  361387 GB |
|       from small pool |      39 MB |      73 MB |    8229 GB |    8229 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12224 MB |   15788 MB |  369629 GB |  369617 GB |
|       from large pool |   12185 MB |   15749 MB |  361399 GB |  361387 GB |
|       from small pool |      39 MB |      73 MB |    8229 GB |    8229 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19452 MB |   19606 MB |    1306 GB |    1287 GB |
|       from large pool |   19404 MB |   19404 MB |    1276 GB |    1257 GB |
|       from small pool |      48 MB |     202 MB |      29 GB |      29 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3661 MB |    7186 MB |  514165 GB |  514161 GB |
|       from large pool |    3652 MB |    7177 MB |  504454 GB |  504450 GB |
|       from small pool |       8 MB |      32 MB |    9710 GB |    9710 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   97224 K  |   97223 K  |
|       from large pool |     271    |     275    |   45997 K  |   45997 K  |
|       from small pool |     321    |     466    |   51226 K  |   51226 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   97224 K  |   97223 K  |
|       from large pool |     271    |     275    |   45997 K  |   45997 K  |
|       from small pool |     321    |     466    |   51226 K  |   51226 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     129    |   16000    |   15948    |
|       from large pool |      28    |      28    |     692    |     664    |
|       from small pool |      24    |     101    |   15308    |   15284    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      78    |   49559 K  |   49559 K  |
|       from large pool |      12    |      13    |   26197 K  |   26197 K  |
|       from small pool |      14    |      73    |   23361 K  |   23361 K  |
|===========================================================================|

2022-05-28 13:08:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:08:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 2.63 GiB free; 19.40 GiB reserved in total by PyTorch)
2022-05-28 13:08:40 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 119          |        cudaMalloc retries: 267       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12953 MB |   16933 MB |  369877 GB |  369864 GB |
|       from large pool |   12914 MB |   16894 MB |  361642 GB |  361629 GB |
|       from small pool |      39 MB |      73 MB |    8234 GB |    8234 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12953 MB |   16933 MB |  369877 GB |  369864 GB |
|       from large pool |   12914 MB |   16894 MB |  361642 GB |  361629 GB |
|       from small pool |      39 MB |      73 MB |    8234 GB |    8234 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19862 MB |   20004 MB |    1310 GB |    1291 GB |
|       from large pool |   19818 MB |   19818 MB |    1280 GB |    1261 GB |
|       from small pool |      44 MB |     186 MB |      30 GB |      29 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2928 MB |    6864 MB |  514836 GB |  514833 GB |
|       from large pool |    2923 MB |    6859 MB |  505119 GB |  505116 GB |
|       from small pool |       4 MB |      17 MB |    9717 GB |    9717 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |   97290 K  |   97289 K  |
|       from large pool |     271    |     275    |   46029 K  |   46029 K  |
|       from small pool |     321    |     466    |   51260 K  |   51260 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |   97290 K  |   97289 K  |
|       from large pool |     271    |     275    |   46029 K  |   46029 K  |
|       from small pool |     321    |     466    |   51260 K  |   51260 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     121    |   16070    |   16020    |
|       from large pool |      28    |      28    |     693    |     665    |
|       from small pool |      22    |      93    |   15377    |   15355    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      67    |   49592 K  |   49592 K  |
|       from large pool |      21    |      21    |   26215 K  |   26215 K  |
|       from small pool |      12    |      50    |   23377 K  |   23377 K  |
|===========================================================================|

2022-05-28 13:08:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:08:42 | INFO | train_inner | epoch 011:    205 / 6032 loss=4.442, nll_loss=2.688, ppl=6.44, wps=30086.9, ups=12.2, wpb=2465.4, bsz=128, num_updates=60400, lr=0.00018014, gnorm=0.982, loss_scale=8, train_wall=8, gb_free=19.2, wall=5062
2022-05-28 13:08:51 | INFO | train_inner | epoch 011:    305 / 6032 loss=4.567, nll_loss=2.833, ppl=7.12, wps=35006.3, ups=12.12, wpb=2888, bsz=128, num_updates=60500, lr=0.000179991, gnorm=0.958, loss_scale=8, train_wall=8, gb_free=20.7, wall=5070
2022-05-28 13:08:59 | INFO | train_inner | epoch 011:    405 / 6032 loss=4.622, nll_loss=2.896, ppl=7.45, wps=33208.8, ups=12.39, wpb=2680.7, bsz=128, num_updates=60600, lr=0.000179842, gnorm=1.018, loss_scale=8, train_wall=8, gb_free=21, wall=5078
2022-05-28 13:09:07 | INFO | train_inner | epoch 011:    505 / 6032 loss=4.623, nll_loss=2.898, ppl=7.45, wps=34028.5, ups=12.46, wpb=2731, bsz=128, num_updates=60700, lr=0.000179694, gnorm=1.001, loss_scale=8, train_wall=8, gb_free=20, wall=5086
2022-05-28 13:09:15 | INFO | train_inner | epoch 011:    605 / 6032 loss=4.69, nll_loss=2.973, ppl=7.85, wps=34965.1, ups=12.31, wpb=2840.9, bsz=128, num_updates=60800, lr=0.000179546, gnorm=1, loss_scale=8, train_wall=8, gb_free=18.6, wall=5094
2022-05-28 13:09:23 | INFO | train_inner | epoch 011:    705 / 6032 loss=4.559, nll_loss=2.824, ppl=7.08, wps=33056, ups=12.94, wpb=2554.2, bsz=128, num_updates=60900, lr=0.000179399, gnorm=1.022, loss_scale=8, train_wall=8, gb_free=16.8, wall=5102
2022-05-28 13:09:30 | INFO | train_inner | epoch 011:    805 / 6032 loss=4.517, nll_loss=2.775, ppl=6.84, wps=32274.1, ups=12.81, wpb=2519.5, bsz=128, num_updates=61000, lr=0.000179252, gnorm=1.025, loss_scale=8, train_wall=8, gb_free=20.3, wall=5110
2022-05-28 13:09:39 | INFO | train_inner | epoch 011:    905 / 6032 loss=4.716, nll_loss=3.002, ppl=8.01, wps=35896.7, ups=12.3, wpb=2918.2, bsz=128, num_updates=61100, lr=0.000179105, gnorm=1.013, loss_scale=8, train_wall=8, gb_free=19.7, wall=5118
2022-05-28 13:09:46 | INFO | train_inner | epoch 011:   1005 / 6032 loss=4.611, nll_loss=2.883, ppl=7.38, wps=34595.5, ups=12.66, wpb=2732.7, bsz=126.8, num_updates=61200, lr=0.000178958, gnorm=1.082, loss_scale=8, train_wall=8, gb_free=19.4, wall=5126
2022-05-28 13:09:55 | INFO | train_inner | epoch 011:   1105 / 6032 loss=4.712, nll_loss=3, ppl=8, wps=33823.4, ups=12.22, wpb=2767.7, bsz=128, num_updates=61300, lr=0.000178812, gnorm=1.066, loss_scale=8, train_wall=8, gb_free=20.5, wall=5134
2022-05-28 13:10:03 | INFO | train_inner | epoch 011:   1205 / 6032 loss=4.665, nll_loss=2.947, ppl=7.71, wps=34391.7, ups=12.31, wpb=2793.3, bsz=128, num_updates=61400, lr=0.000178667, gnorm=1.002, loss_scale=8, train_wall=8, gb_free=19.5, wall=5142
2022-05-28 13:10:11 | INFO | train_inner | epoch 011:   1305 / 6032 loss=4.596, nll_loss=2.869, ppl=7.31, wps=32055.1, ups=12.71, wpb=2521.4, bsz=128, num_updates=61500, lr=0.000178521, gnorm=1.045, loss_scale=8, train_wall=8, gb_free=20.9, wall=5150
2022-05-28 13:10:19 | INFO | train_inner | epoch 011:   1405 / 6032 loss=4.941, nll_loss=3.261, ppl=9.59, wps=34175, ups=11.83, wpb=2889.4, bsz=128, num_updates=61600, lr=0.000178377, gnorm=1.041, loss_scale=8, train_wall=8, gb_free=20.3, wall=5158
2022-05-28 13:10:27 | INFO | train_inner | epoch 011:   1505 / 6032 loss=4.728, nll_loss=3.019, ppl=8.11, wps=35069.2, ups=12.76, wpb=2749.3, bsz=128, num_updates=61700, lr=0.000178232, gnorm=1.015, loss_scale=8, train_wall=8, gb_free=20.6, wall=5166
2022-05-28 13:10:35 | INFO | train_inner | epoch 011:   1605 / 6032 loss=4.614, nll_loss=2.887, ppl=7.4, wps=32301.5, ups=12.71, wpb=2541.6, bsz=128, num_updates=61800, lr=0.000178088, gnorm=1.025, loss_scale=8, train_wall=8, gb_free=19.8, wall=5174
2022-05-28 13:10:43 | INFO | train_inner | epoch 011:   1705 / 6032 loss=4.72, nll_loss=3.01, ppl=8.06, wps=33132.9, ups=12.48, wpb=2655.6, bsz=128, num_updates=61900, lr=0.000177944, gnorm=1.051, loss_scale=8, train_wall=8, gb_free=19.1, wall=5182
2022-05-28 13:10:51 | INFO | train_inner | epoch 011:   1805 / 6032 loss=4.767, nll_loss=3.063, ppl=8.36, wps=37384.6, ups=12.15, wpb=3077.3, bsz=128, num_updates=62000, lr=0.0001778, gnorm=0.99, loss_scale=8, train_wall=8, gb_free=17.3, wall=5190
2022-05-28 13:10:59 | INFO | train_inner | epoch 011:   1905 / 6032 loss=4.61, nll_loss=2.884, ppl=7.38, wps=31346.2, ups=12.78, wpb=2452.7, bsz=128, num_updates=62100, lr=0.000177657, gnorm=1.035, loss_scale=8, train_wall=8, gb_free=20.4, wall=5198
2022-05-28 13:11:07 | INFO | train_inner | epoch 011:   2005 / 6032 loss=4.648, nll_loss=2.928, ppl=7.61, wps=33239.5, ups=12.5, wpb=2660.2, bsz=128, num_updates=62200, lr=0.000177514, gnorm=1.035, loss_scale=8, train_wall=8, gb_free=20.6, wall=5206
2022-05-28 13:11:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 13.50 GiB already allocated; 405.69 MiB free; 21.63 GiB reserved in total by PyTorch)
2022-05-28 13:11:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 120          |        cudaMalloc retries: 268       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13823 MB |   13868 MB |  381003 GB |  380990 GB |
|       from large pool |   13783 MB |   13829 MB |  372521 GB |  372507 GB |
|       from small pool |      39 MB |      73 MB |    8482 GB |    8482 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13823 MB |   13868 MB |  381003 GB |  380990 GB |
|       from large pool |   13783 MB |   13829 MB |  372521 GB |  372507 GB |
|       from small pool |      39 MB |      73 MB |    8482 GB |    8482 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22154 MB |   22316 MB |    1316 GB |    1295 GB |
|       from large pool |   22114 MB |   22114 MB |    1286 GB |    1265 GB |
|       from small pool |      40 MB |     202 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    8330 MB |    8330 MB |  535853 GB |  535845 GB |
|       from large pool |    8330 MB |    8330 MB |  525843 GB |  525835 GB |
|       from small pool |       0 MB |       3 MB |   10010 GB |   10010 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  100195 K  |  100195 K  |
|       from large pool |     271    |     275    |   47398 K  |   47398 K  |
|       from small pool |     321    |     466    |   52797 K  |   52796 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  100195 K  |  100195 K  |
|       from large pool |     271    |     275    |   47398 K  |   47398 K  |
|       from small pool |     321    |     466    |   52797 K  |   52796 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     130    |   16151    |   16102    |
|       from large pool |      29    |      29    |     695    |     666    |
|       from small pool |      20    |     101    |   15456    |   15436    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      35    |   51078 K  |   51078 K  |
|       from large pool |      22    |      22    |   26995 K  |   26995 K  |
|       from small pool |      11    |      17    |   24082 K  |   24082 K  |
|===========================================================================|

2022-05-28 13:11:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:11:15 | INFO | train_inner | epoch 011:   2106 / 6032 loss=4.527, nll_loss=2.789, ppl=6.91, wps=32043.4, ups=12.74, wpb=2514.3, bsz=128, num_updates=62300, lr=0.000177372, gnorm=1.03, loss_scale=8, train_wall=8, gb_free=19, wall=5214
2022-05-28 13:11:22 | INFO | train_inner | epoch 011:   2206 / 6032 loss=4.665, nll_loss=2.945, ppl=7.7, wps=34488.1, ups=12.85, wpb=2683.7, bsz=128, num_updates=62400, lr=0.000177229, gnorm=1.021, loss_scale=8, train_wall=8, gb_free=19.3, wall=5222
2022-05-28 13:11:30 | INFO | train_inner | epoch 011:   2306 / 6032 loss=4.62, nll_loss=2.894, ppl=7.43, wps=33836.2, ups=12.81, wpb=2640.6, bsz=128, num_updates=62500, lr=0.000177088, gnorm=1.032, loss_scale=8, train_wall=8, gb_free=17.2, wall=5229
2022-05-28 13:11:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.69 GiB already allocated; 403.69 MiB free; 21.64 GiB reserved in total by PyTorch)
2022-05-28 13:11:31 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 121          |        cudaMalloc retries: 269       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17085 MB |   17143 MB |  382578 GB |  382562 GB |
|       from large pool |   17045 MB |   17103 MB |  374063 GB |  374046 GB |
|       from small pool |      39 MB |      73 MB |    8515 GB |    8515 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17085 MB |   17143 MB |  382578 GB |  382562 GB |
|       from large pool |   17045 MB |   17103 MB |  374063 GB |  374046 GB |
|       from small pool |      39 MB |      73 MB |    8515 GB |    8515 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22156 MB |   22316 MB |    1317 GB |    1295 GB |
|       from large pool |   22114 MB |   22114 MB |    1286 GB |    1265 GB |
|       from small pool |      42 MB |     202 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5068 MB |    5488 MB |  538743 GB |  538738 GB |
|       from large pool |    5068 MB |    5488 MB |  528693 GB |  528688 GB |
|       from small pool |       0 MB |       5 MB |   10050 GB |   10050 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  100620 K  |  100620 K  |
|       from large pool |     271    |     275    |   47602 K  |   47602 K  |
|       from small pool |     321    |     466    |   53018 K  |   53017 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  100620 K  |  100620 K  |
|       from large pool |     271    |     275    |   47602 K  |   47602 K  |
|       from small pool |     321    |     466    |   53018 K  |   53017 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     130    |   16232    |   16182    |
|       from large pool |      29    |      29    |     695    |     666    |
|       from small pool |      21    |     101    |   15537    |   15516    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      35    |   51295 K  |   51295 K  |
|       from large pool |      21    |      21    |   27112 K  |   27112 K  |
|       from small pool |       9    |      23    |   24183 K  |   24183 K  |
|===========================================================================|

2022-05-28 13:11:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:11:38 | INFO | train_inner | epoch 011:   2407 / 6032 loss=4.689, nll_loss=2.974, ppl=7.86, wps=34198.3, ups=12.25, wpb=2792.4, bsz=128, num_updates=62600, lr=0.000176946, gnorm=1.012, loss_scale=8, train_wall=8, gb_free=18.6, wall=5238
2022-05-28 13:11:47 | INFO | train_inner | epoch 011:   2507 / 6032 loss=4.726, nll_loss=3.018, ppl=8.1, wps=35137.1, ups=12.05, wpb=2917, bsz=128, num_updates=62700, lr=0.000176805, gnorm=1.003, loss_scale=8, train_wall=8, gb_free=8.3, wall=5246
2022-05-28 13:11:55 | INFO | train_inner | epoch 011:   2607 / 6032 loss=4.701, nll_loss=2.99, ppl=7.94, wps=33400.2, ups=12.55, wpb=2660.6, bsz=128, num_updates=62800, lr=0.000176664, gnorm=1.05, loss_scale=8, train_wall=8, gb_free=20.3, wall=5254
2022-05-28 13:12:03 | INFO | train_inner | epoch 011:   2707 / 6032 loss=4.751, nll_loss=3.044, ppl=8.25, wps=36976.5, ups=12.15, wpb=3043.1, bsz=128, num_updates=62900, lr=0.000176524, gnorm=0.986, loss_scale=8, train_wall=8, gb_free=16.2, wall=5262
2022-05-28 13:12:11 | INFO | train_inner | epoch 011:   2807 / 6032 loss=4.836, nll_loss=3.144, ppl=8.84, wps=37017.6, ups=12.3, wpb=3008.8, bsz=128, num_updates=63000, lr=0.000176383, gnorm=1.011, loss_scale=8, train_wall=8, gb_free=20.6, wall=5270
2022-05-28 13:12:19 | INFO | train_inner | epoch 011:   2907 / 6032 loss=4.538, nll_loss=2.803, ppl=6.98, wps=31820.6, ups=13.02, wpb=2444.3, bsz=128, num_updates=63100, lr=0.000176244, gnorm=1.053, loss_scale=8, train_wall=7, gb_free=16.7, wall=5278
2022-05-28 13:12:27 | INFO | train_inner | epoch 011:   3007 / 6032 loss=4.972, nll_loss=3.298, ppl=9.84, wps=35134.6, ups=11.82, wpb=2972.5, bsz=128, num_updates=63200, lr=0.000176104, gnorm=1.051, loss_scale=8, train_wall=8, gb_free=16.9, wall=5286
2022-05-28 13:12:35 | INFO | train_inner | epoch 011:   3107 / 6032 loss=4.536, nll_loss=2.8, ppl=6.96, wps=32592.6, ups=12.91, wpb=2524.9, bsz=128, num_updates=63300, lr=0.000175965, gnorm=1.027, loss_scale=8, train_wall=8, gb_free=19.1, wall=5294
2022-05-28 13:12:43 | INFO | train_inner | epoch 011:   3207 / 6032 loss=4.496, nll_loss=2.756, ppl=6.76, wps=32501, ups=13.13, wpb=2475.2, bsz=128, num_updates=63400, lr=0.000175826, gnorm=1.026, loss_scale=8, train_wall=7, gb_free=19.8, wall=5302
2022-05-28 13:12:51 | INFO | train_inner | epoch 011:   3307 / 6032 loss=4.796, nll_loss=3.099, ppl=8.57, wps=35614.7, ups=12.5, wpb=2849.8, bsz=128, num_updates=63500, lr=0.000175688, gnorm=1.024, loss_scale=8, train_wall=8, gb_free=20.1, wall=5310
2022-05-28 13:12:59 | INFO | train_inner | epoch 011:   3407 / 6032 loss=4.657, nll_loss=2.938, ppl=7.67, wps=34186.5, ups=12.54, wpb=2726.4, bsz=128, num_updates=63600, lr=0.000175549, gnorm=1.019, loss_scale=8, train_wall=8, gb_free=18.9, wall=5318
2022-05-28 13:13:06 | INFO | train_inner | epoch 011:   3507 / 6032 loss=4.747, nll_loss=3.044, ppl=8.25, wps=35072.7, ups=12.66, wpb=2769.3, bsz=128, num_updates=63700, lr=0.000175412, gnorm=1.017, loss_scale=8, train_wall=8, gb_free=20.3, wall=5326
2022-05-28 13:13:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 403.69 MiB free; 21.64 GiB reserved in total by PyTorch)
2022-05-28 13:13:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 122          |        cudaMalloc retries: 270       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12699 MB |   16630 MB |  389886 GB |  389873 GB |
|       from large pool |   12660 MB |   16591 MB |  381217 GB |  381205 GB |
|       from small pool |      39 MB |      73 MB |    8668 GB |    8668 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12699 MB |   16630 MB |  389886 GB |  389873 GB |
|       from large pool |   12660 MB |   16591 MB |  381217 GB |  381205 GB |
|       from small pool |      39 MB |      73 MB |    8668 GB |    8668 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22156 MB |   22316 MB |    1317 GB |    1295 GB |
|       from large pool |   22114 MB |   22114 MB |    1286 GB |    1265 GB |
|       from small pool |      42 MB |     202 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2000 MB |    5525 MB |  552129 GB |  552127 GB |
|       from large pool |    1997 MB |    5522 MB |  541897 GB |  541895 GB |
|       from small pool |       2 MB |      67 MB |   10232 GB |   10232 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  102491 K  |  102490 K  |
|       from large pool |     271    |     275    |   48492 K  |   48492 K  |
|       from small pool |     321    |     466    |   53998 K  |   53998 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  102491 K  |  102490 K  |
|       from large pool |     271    |     275    |   48492 K  |   48492 K  |
|       from small pool |     321    |     466    |   53998 K  |   53998 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     130    |   16313    |   16263    |
|       from large pool |      29    |      29    |     695    |     666    |
|       from small pool |      21    |     101    |   15618    |   15597    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      77    |   52248 K  |   52248 K  |
|       from large pool |      23    |      24    |   27617 K  |   27617 K  |
|       from small pool |      10    |      72    |   24630 K  |   24630 K  |
|===========================================================================|

2022-05-28 13:13:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:13:14 | INFO | train_inner | epoch 011:   3608 / 6032 loss=4.675, nll_loss=2.959, ppl=7.78, wps=31240.7, ups=12.47, wpb=2505.4, bsz=128, num_updates=63800, lr=0.000175274, gnorm=1.068, loss_scale=8, train_wall=8, gb_free=19.2, wall=5334
2022-05-28 13:13:22 | INFO | train_inner | epoch 011:   3708 / 6032 loss=4.732, nll_loss=3.025, ppl=8.14, wps=34372.9, ups=12.79, wpb=2686.6, bsz=128, num_updates=63900, lr=0.000175137, gnorm=1.035, loss_scale=8, train_wall=8, gb_free=20, wall=5341
2022-05-28 13:13:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 435.69 MiB free; 21.61 GiB reserved in total by PyTorch)
2022-05-28 13:13:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 123          |        cudaMalloc retries: 272       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21956 MB |   22256 MB |  391053 GB |  391031 GB |
|       from large pool |   21913 MB |   22213 MB |  382358 GB |  382336 GB |
|       from small pool |      42 MB |      73 MB |    8695 GB |    8695 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21956 MB |   22256 MB |  391053 GB |  391031 GB |
|       from large pool |   21913 MB |   22213 MB |  382358 GB |  382336 GB |
|       from small pool |      42 MB |      73 MB |    8695 GB |    8695 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22124 MB |   22424 MB |    1324 GB |    1303 GB |
|       from large pool |   22080 MB |   22380 MB |    1294 GB |    1272 GB |
|       from small pool |      44 MB |     202 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  171449 KB |    5113 MB |  554293 GB |  554293 GB |
|       from large pool |  170032 KB |    5113 MB |  544029 GB |  544029 GB |
|       from small pool |    1417 KB |      12 MB |   10263 GB |   10263 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |  102801 K  |  102800 K  |
|       from large pool |     214    |     215    |   48638 K  |   48637 K  |
|       from small pool |     302    |     466    |   54163 K  |   54162 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |  102801 K  |  102800 K  |
|       from large pool |     214    |     215    |   48638 K  |   48637 K  |
|       from small pool |     302    |     466    |   54163 K  |   54162 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      56    |     135    |   16400    |   16344    |
|       from large pool |      34    |      35    |     702    |     668    |
|       from small pool |      22    |     101    |   15698    |   15676    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      52    |   52406 K  |   52406 K  |
|       from large pool |      24    |      24    |   27700 K  |   27700 K  |
|       from small pool |      10    |      39    |   24705 K  |   24705 K  |
|===========================================================================|

2022-05-28 13:13:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:13:30 | INFO | train_inner | epoch 011:   3809 / 6032 loss=4.607, nll_loss=2.884, ppl=7.38, wps=33390.8, ups=12.29, wpb=2717.3, bsz=128, num_updates=64000, lr=0.000175, gnorm=1.033, loss_scale=8, train_wall=8, gb_free=19.7, wall=5350
2022-05-28 13:13:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 3.41 GiB free; 18.62 GiB reserved in total by PyTorch)
2022-05-28 13:13:31 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 124          |        cudaMalloc retries: 274       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13288 MB |   17219 MB |  391687 GB |  391674 GB |
|       from large pool |   13249 MB |   17179 MB |  382981 GB |  382968 GB |
|       from small pool |      39 MB |      73 MB |    8706 GB |    8706 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13288 MB |   17219 MB |  391687 GB |  391674 GB |
|       from large pool |   13249 MB |   17179 MB |  382981 GB |  382968 GB |
|       from small pool |      39 MB |      73 MB |    8706 GB |    8706 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19068 MB |   22260 MB |    1328 GB |    1310 GB |
|       from large pool |   19028 MB |   22080 MB |    1298 GB |    1279 GB |
|       from small pool |      40 MB |     180 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1847 MB |    3402 MB |  555477 GB |  555475 GB |
|       from large pool |    1846 MB |    3400 MB |  545200 GB |  545198 GB |
|       from small pool |       0 MB |       3 MB |   10277 GB |   10277 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  102964 K  |  102963 K  |
|       from large pool |     271    |     275    |   48718 K  |   48718 K  |
|       from small pool |     321    |     466    |   54245 K  |   54245 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  102964 K  |  102963 K  |
|       from large pool |     271    |     275    |   48718 K  |   48718 K  |
|       from small pool |     321    |     466    |   54245 K  |   54245 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      53    |     124    |   16469    |   16416    |
|       from large pool |      33    |      34    |     703    |     670    |
|       from small pool |      20    |      90    |   15766    |   15746    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      35    |   52489 K  |   52489 K  |
|       from large pool |      25    |      26    |   27747 K  |   27747 K  |
|       from small pool |       8    |      15    |   24742 K  |   24742 K  |
|===========================================================================|

2022-05-28 13:13:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:13:38 | INFO | train_inner | epoch 011:   3910 / 6032 loss=4.524, nll_loss=2.786, ppl=6.9, wps=29678.4, ups=12.71, wpb=2334.4, bsz=128, num_updates=64100, lr=0.000174863, gnorm=1.07, loss_scale=8, train_wall=7, gb_free=18.9, wall=5357
2022-05-28 13:13:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.62 GiB (GPU 0; 23.70 GiB total capacity; 15.67 GiB already allocated; 3.62 GiB free; 18.41 GiB reserved in total by PyTorch)
2022-05-28 13:13:42 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 125          |        cudaMalloc retries: 275       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12332 MB |   16043 MB |  392379 GB |  392367 GB |
|       from large pool |   12293 MB |   16004 MB |  383653 GB |  383641 GB |
|       from small pool |      39 MB |      73 MB |    8725 GB |    8725 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12332 MB |   16043 MB |  392379 GB |  392367 GB |
|       from large pool |   12293 MB |   16004 MB |  383653 GB |  383641 GB |
|       from small pool |      39 MB |      73 MB |    8725 GB |    8725 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18848 MB |   19006 MB |    1332 GB |    1314 GB |
|       from large pool |   18808 MB |   18808 MB |    1301 GB |    1283 GB |
|       from small pool |      40 MB |     198 MB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2803 MB |    3390 MB |  556803 GB |  556801 GB |
|       from large pool |    2802 MB |    3389 MB |  546503 GB |  546501 GB |
|       from small pool |       0 MB |      22 MB |   10300 GB |   10300 GB |
|---------------------------------------------------------------------------|
| Allocations           |     593    |     597    |  103168 K  |  103167 K  |
|       from large pool |     271    |     275    |   48810 K  |   48810 K  |
|       from small pool |     322    |     466    |   54357 K  |   54357 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     593    |     597    |  103168 K  |  103167 K  |
|       from large pool |     271    |     275    |   48810 K  |   48810 K  |
|       from small pool |     322    |     466    |   54357 K  |   54357 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      53    |     132    |   16549    |   16496    |
|       from large pool |      33    |      33    |     704    |     671    |
|       from small pool |      20    |      99    |   15845    |   15825    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      50    |   52594 K  |   52594 K  |
|       from large pool |      20    |      21    |   27800 K  |   27800 K  |
|       from small pool |       9    |      34    |   24793 K  |   24793 K  |
|===========================================================================|

2022-05-28 13:13:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:13:47 | INFO | train_inner | epoch 011:   4011 / 6032 loss=4.819, nll_loss=3.127, ppl=8.74, wps=32906, ups=11.59, wpb=2838.6, bsz=128, num_updates=64200, lr=0.000174727, gnorm=1.029, loss_scale=8, train_wall=8, gb_free=20.4, wall=5366
2022-05-28 13:13:55 | INFO | train_inner | epoch 011:   4111 / 6032 loss=4.568, nll_loss=2.838, ppl=7.15, wps=32031.8, ups=13.05, wpb=2454.2, bsz=128, num_updates=64300, lr=0.000174591, gnorm=1.059, loss_scale=8, train_wall=7, gb_free=19.5, wall=5374
2022-05-28 13:14:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 243.69 MiB free; 21.79 GiB reserved in total by PyTorch)
2022-05-28 13:14:02 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 126          |        cudaMalloc retries: 277       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13632 MB |   17563 MB |  393867 GB |  393853 GB |
|       from large pool |   13592 MB |   17523 MB |  385105 GB |  385092 GB |
|       from small pool |      39 MB |      73 MB |    8761 GB |    8761 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13632 MB |   17563 MB |  393867 GB |  393853 GB |
|       from large pool |   13592 MB |   17523 MB |  385105 GB |  385092 GB |
|       from small pool |      39 MB |      73 MB |    8761 GB |    8761 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22316 MB |   22316 MB |    1343 GB |    1321 GB |
|       from large pool |   22274 MB |   22274 MB |    1311 GB |    1290 GB |
|       from small pool |      42 MB |     202 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4751 MB |    4752 MB |  559499 GB |  559494 GB |
|       from large pool |    4749 MB |    4750 MB |  549157 GB |  549153 GB |
|       from small pool |       2 MB |      13 MB |   10341 GB |   10341 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  103564 K  |  103563 K  |
|       from large pool |     271    |     275    |   48994 K  |   48994 K  |
|       from small pool |     321    |     466    |   54569 K  |   54569 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  103564 K  |  103563 K  |
|       from large pool |     271    |     275    |   48994 K  |   48994 K  |
|       from small pool |     321    |     466    |   54569 K  |   54569 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      55    |     135    |   16633    |   16578    |
|       from large pool |      34    |      34    |     707    |     673    |
|       from small pool |      21    |     101    |   15926    |   15905    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      60    |   52799 K  |   52799 K  |
|       from large pool |      27    |      28    |   27907 K  |   27907 K  |
|       from small pool |      10    |      44    |   24892 K  |   24892 K  |
|===========================================================================|

2022-05-28 13:14:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:14:03 | INFO | train_inner | epoch 011:   4212 / 6032 loss=4.621, nll_loss=2.898, ppl=7.46, wps=32519.1, ups=12.44, wpb=2613.8, bsz=128, num_updates=64400, lr=0.000174456, gnorm=1.05, loss_scale=8, train_wall=8, gb_free=20.7, wall=5382
2022-05-28 13:14:10 | INFO | train_inner | epoch 011:   4312 / 6032 loss=4.659, nll_loss=2.943, ppl=7.69, wps=33866.3, ups=12.71, wpb=2665.2, bsz=128, num_updates=64500, lr=0.00017432, gnorm=1.039, loss_scale=8, train_wall=8, gb_free=20.5, wall=5390
2022-05-28 13:14:18 | INFO | train_inner | epoch 011:   4412 / 6032 loss=4.409, nll_loss=2.657, ppl=6.31, wps=30908, ups=13.39, wpb=2309, bsz=128, num_updates=64600, lr=0.000174185, gnorm=1.045, loss_scale=8, train_wall=7, gb_free=17.3, wall=5397
2022-05-28 13:14:26 | INFO | train_inner | epoch 011:   4512 / 6032 loss=4.744, nll_loss=3.041, ppl=8.23, wps=33828.8, ups=12.85, wpb=2633.5, bsz=128, num_updates=64700, lr=0.000174051, gnorm=1.057, loss_scale=8, train_wall=8, gb_free=18.6, wall=5405
2022-05-28 13:14:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.86 GiB already allocated; 4.18 GiB free; 17.85 GiB reserved in total by PyTorch)
2022-05-28 13:14:30 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 127          |        cudaMalloc retries: 278       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15212 MB |   15263 MB |  395856 GB |  395841 GB |
|       from large pool |   15173 MB |   15224 MB |  387047 GB |  387032 GB |
|       from small pool |      39 MB |      73 MB |    8808 GB |    8808 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15212 MB |   15263 MB |  395856 GB |  395841 GB |
|       from large pool |   15173 MB |   15224 MB |  387047 GB |  387032 GB |
|       from small pool |      39 MB |      73 MB |    8808 GB |    8808 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18276 MB |   21718 MB |    1346 GB |    1328 GB |
|       from large pool |   18220 MB |   21516 MB |    1315 GB |    1297 GB |
|       from small pool |      56 MB |     202 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3063 MB |    3250 MB |  563178 GB |  563175 GB |
|       from large pool |    3046 MB |    3233 MB |  552780 GB |  552777 GB |
|       from small pool |      16 MB |      68 MB |   10398 GB |   10398 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  104119 K  |  104118 K  |
|       from large pool |     271    |     275    |   49253 K  |   49253 K  |
|       from small pool |     321    |     466    |   54865 K  |   54865 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  104119 K  |  104118 K  |
|       from large pool |     271    |     275    |   49253 K  |   49253 K  |
|       from small pool |     321    |     466    |   54865 K  |   54865 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |     135    |   16714    |   16653    |
|       from large pool |      33    |      34    |     708    |     675    |
|       from small pool |      28    |     101    |   16006    |   15978    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |     104    |   53087 K  |   53087 K  |
|       from large pool |      20    |      20    |   28059 K  |   28059 K  |
|       from small pool |      17    |      98    |   25027 K  |   25027 K  |
|===========================================================================|

2022-05-28 13:14:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:14:34 | INFO | train_inner | epoch 011:   4613 / 6032 loss=4.688, nll_loss=2.976, ppl=7.87, wps=32168.5, ups=12.78, wpb=2517.1, bsz=128, num_updates=64800, lr=0.000173916, gnorm=1.068, loss_scale=8, train_wall=7, gb_free=19.9, wall=5413
2022-05-28 13:14:41 | INFO | train_inner | epoch 011:   4713 / 6032 loss=4.672, nll_loss=2.957, ppl=7.76, wps=34387.1, ups=12.86, wpb=2673.7, bsz=128, num_updates=64900, lr=0.000173782, gnorm=1.04, loss_scale=8, train_wall=8, gb_free=17.9, wall=5420
2022-05-28 13:14:49 | INFO | train_inner | epoch 011:   4813 / 6032 loss=4.624, nll_loss=2.903, ppl=7.48, wps=33744.8, ups=12.9, wpb=2615.3, bsz=128, num_updates=65000, lr=0.000173649, gnorm=1.055, loss_scale=8, train_wall=8, gb_free=20.3, wall=5428
2022-05-28 13:14:58 | INFO | train_inner | epoch 011:   4913 / 6032 loss=4.785, nll_loss=3.089, ppl=8.51, wps=32345.8, ups=11.84, wpb=2731.3, bsz=128, num_updates=65100, lr=0.000173515, gnorm=1.04, loss_scale=8, train_wall=8, gb_free=16, wall=5437
2022-05-28 13:14:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 13.63 GiB already allocated; 3.98 GiB free; 18.05 GiB reserved in total by PyTorch)
2022-05-28 13:14:58 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 128          |        cudaMalloc retries: 280       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13957 MB |   14005 MB |  397950 GB |  397936 GB |
|       from large pool |   13918 MB |   13966 MB |  389093 GB |  389079 GB |
|       from small pool |      39 MB |      73 MB |    8857 GB |    8857 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13957 MB |   14005 MB |  397950 GB |  397936 GB |
|       from large pool |   13918 MB |   13966 MB |  389093 GB |  389079 GB |
|       from small pool |      39 MB |      73 MB |    8857 GB |    8857 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18484 MB |   22168 MB |    1356 GB |    1338 GB |
|       from large pool |   18442 MB |   21984 MB |    1325 GB |    1307 GB |
|       from small pool |      42 MB |     184 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4526 MB |    4526 MB |  566998 GB |  566993 GB |
|       from large pool |    4523 MB |    4523 MB |  556542 GB |  556538 GB |
|       from small pool |       2 MB |      17 MB |   10455 GB |   10455 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  104667 K  |  104666 K  |
|       from large pool |     271    |     275    |   49509 K  |   49509 K  |
|       from small pool |     321    |     466    |   55157 K  |   55157 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  104667 K  |  104666 K  |
|       from large pool |     271    |     275    |   49509 K  |   49509 K  |
|       from small pool |     321    |     466    |   55157 K  |   55157 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      54    |     126    |   16861    |   16807    |
|       from large pool |      33    |      34    |     711    |     678    |
|       from small pool |      21    |      92    |   16150    |   16129    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      63    |   53370 K  |   53370 K  |
|       from large pool |      19    |      19    |   28207 K  |   28207 K  |
|       from small pool |       8    |      51    |   25162 K  |   25162 K  |
|===========================================================================|

2022-05-28 13:14:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:15:06 | INFO | train_inner | epoch 011:   5014 / 6032 loss=4.712, nll_loss=3.004, ppl=8.02, wps=34612.9, ups=11.98, wpb=2888.7, bsz=128, num_updates=65200, lr=0.000173382, gnorm=1.011, loss_scale=8, train_wall=8, gb_free=19.9, wall=5445
2022-05-28 13:15:13 | INFO | train_inner | epoch 011:   5114 / 6032 loss=4.628, nll_loss=2.908, ppl=7.5, wps=33393.3, ups=13.56, wpb=2461.8, bsz=128, num_updates=65300, lr=0.000173249, gnorm=1.066, loss_scale=8, train_wall=7, gb_free=19.2, wall=5452
2022-05-28 13:15:21 | INFO | train_inner | epoch 011:   5214 / 6032 loss=4.779, nll_loss=3.081, ppl=8.46, wps=35027.7, ups=13.36, wpb=2622.1, bsz=128, num_updates=65400, lr=0.000173117, gnorm=1.054, loss_scale=8, train_wall=7, gb_free=17.1, wall=5460
2022-05-28 13:15:28 | INFO | train_inner | epoch 011:   5314 / 6032 loss=4.824, nll_loss=3.132, ppl=8.77, wps=36271.7, ups=13.12, wpb=2765.7, bsz=128, num_updates=65500, lr=0.000172985, gnorm=1.037, loss_scale=8, train_wall=7, gb_free=13.9, wall=5468
2022-05-28 13:15:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 1.34 GiB free; 20.69 GiB reserved in total by PyTorch)
2022-05-28 13:15:33 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 129          |        cudaMalloc retries: 281       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18611 MB |   18674 MB |  400670 GB |  400651 GB |
|       from large pool |   18571 MB |   18634 MB |  391753 GB |  391735 GB |
|       from small pool |      40 MB |      73 MB |    8916 GB |    8916 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18611 MB |   18674 MB |  400670 GB |  400651 GB |
|       from large pool |   18571 MB |   18634 MB |  391753 GB |  391735 GB |
|       from small pool |      40 MB |      73 MB |    8916 GB |    8916 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21186 MB |   21380 MB |    1359 GB |    1338 GB |
|       from large pool |   21144 MB |   21178 MB |    1327 GB |    1307 GB |
|       from small pool |      42 MB |     202 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2574 MB |    3282 MB |  571933 GB |  571930 GB |
|       from large pool |    2572 MB |    3281 MB |  561407 GB |  561405 GB |
|       from small pool |       1 MB |      13 MB |   10525 GB |   10525 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  105387 K  |  105386 K  |
|       from large pool |     271    |     275    |   49851 K  |   49850 K  |
|       from small pool |     321    |     466    |   55536 K  |   55536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  105387 K  |  105386 K  |
|       from large pool |     271    |     275    |   49851 K  |   49850 K  |
|       from small pool |     321    |     466    |   55536 K  |   55536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      54    |     135    |   16942    |   16888    |
|       from large pool |      33    |      34    |     712    |     679    |
|       from small pool |      21    |     101    |   16230    |   16209    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      54    |   53739 K  |   53739 K  |
|       from large pool |      27    |      27    |   28404 K  |   28404 K  |
|       from small pool |       9    |      43    |   25335 K  |   25335 K  |
|===========================================================================|

2022-05-28 13:15:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:15:36 | INFO | train_inner | epoch 011:   5415 / 6032 loss=4.709, nll_loss=2.999, ppl=7.99, wps=33438.2, ups=13.11, wpb=2550.8, bsz=128, num_updates=65600, lr=0.000172853, gnorm=1.084, loss_scale=8, train_wall=7, gb_free=19.8, wall=5475
2022-05-28 13:15:44 | INFO | train_inner | epoch 011:   5515 / 6032 loss=4.912, nll_loss=3.234, ppl=9.41, wps=35634.5, ups=12.65, wpb=2817.8, bsz=128, num_updates=65700, lr=0.000172721, gnorm=1.058, loss_scale=8, train_wall=8, gb_free=20.1, wall=5483
2022-05-28 13:15:51 | INFO | train_inner | epoch 011:   5615 / 6032 loss=4.704, nll_loss=2.994, ppl=7.97, wps=34638.4, ups=13.38, wpb=2588.8, bsz=128, num_updates=65800, lr=0.00017259, gnorm=1.054, loss_scale=8, train_wall=7, gb_free=20.6, wall=5491
2022-05-28 13:15:59 | INFO | train_inner | epoch 011:   5715 / 6032 loss=4.758, nll_loss=3.057, ppl=8.32, wps=36515.2, ups=12.82, wpb=2847.6, bsz=128, num_updates=65900, lr=0.000172459, gnorm=1.023, loss_scale=8, train_wall=8, gb_free=18.5, wall=5498
2022-05-28 13:16:06 | INFO | train_inner | epoch 011:   5815 / 6032 loss=4.645, nll_loss=2.928, ppl=7.61, wps=33071.9, ups=13.71, wpb=2412.4, bsz=128, num_updates=66000, lr=0.000172328, gnorm=1.058, loss_scale=8, train_wall=7, gb_free=18.6, wall=5506
2022-05-28 13:16:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 543.69 MiB free; 21.50 GiB reserved in total by PyTorch)
2022-05-28 13:16:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 130          |        cudaMalloc retries: 283       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14035 MB |   18405 MB |  403347 GB |  403334 GB |
|       from large pool |   13996 MB |   18366 MB |  394369 GB |  394355 GB |
|       from small pool |      39 MB |      73 MB |    8978 GB |    8978 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14035 MB |   18405 MB |  403347 GB |  403334 GB |
|       from large pool |   13996 MB |   18366 MB |  394369 GB |  394355 GB |
|       from small pool |      39 MB |      73 MB |    8978 GB |    8978 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22016 MB |   22016 MB |    1363 GB |    1342 GB |
|       from large pool |   21974 MB |   21974 MB |    1331 GB |    1310 GB |
|       from small pool |      42 MB |     202 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3608 MB |    3610 MB |  576724 GB |  576720 GB |
|       from large pool |    3605 MB |    3607 MB |  566124 GB |  566121 GB |
|       from small pool |       2 MB |      14 MB |   10599 GB |   10599 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  106108 K  |  106107 K  |
|       from large pool |     271    |     275    |   50188 K  |   50188 K  |
|       from small pool |     321    |     466    |   55920 K  |   55919 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  106108 K  |  106107 K  |
|       from large pool |     271    |     275    |   50188 K  |   50188 K  |
|       from small pool |     321    |     466    |   55920 K  |   55919 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      54    |     134    |   17023    |   16969    |
|       from large pool |      33    |      33    |     713    |     680    |
|       from small pool |      21    |     101    |   16310    |   16289    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      52    |   54111 K  |   54111 K  |
|       from large pool |      22    |      23    |   28601 K  |   28601 K  |
|       from small pool |       9    |      40    |   25509 K  |   25509 K  |
|===========================================================================|

2022-05-28 13:16:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:16:14 | INFO | train_inner | epoch 011:   5916 / 6032 loss=4.647, nll_loss=2.931, ppl=7.63, wps=32529.7, ups=12.81, wpb=2539.9, bsz=128, num_updates=66100, lr=0.000172198, gnorm=1.061, loss_scale=16, train_wall=7, gb_free=20.2, wall=5513
2022-05-28 13:16:22 | INFO | train_inner | epoch 011:   6016 / 6032 loss=4.712, nll_loss=3.004, ppl=8.02, wps=33835.3, ups=13.15, wpb=2572.4, bsz=128, num_updates=66200, lr=0.000172068, gnorm=1.059, loss_scale=16, train_wall=7, gb_free=19.3, wall=5521
2022-05-28 13:16:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 13:16:42 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.306 | nll_loss 3.63 | ppl 12.38 | wps 109982 | wpb 2687.4 | bsz 128 | num_updates 66216 | best_loss 5.306
2022-05-28 13:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 66216 updates
2022-05-28 13:16:42 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint11.pt
2022-05-28 13:16:45 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint11.pt
2022-05-28 13:16:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint11.pt (epoch 11 @ 66216 updates, score 5.306) (writing took 9.8113725730218 seconds)
2022-05-28 13:16:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-05-28 13:16:52 | INFO | train | epoch 011 | loss 4.679 | nll_loss 2.964 | ppl 7.8 | wps 31816.6 | ups 11.9 | wpb 2673 | bsz 128 | num_updates 66216 | lr 0.000172047 | gnorm 1.034 | loss_scale 16 | train_wall 462 | gb_free 11.1 | wall 5551
2022-05-28 13:16:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 13:16:52 | INFO | fairseq.trainer | begin training epoch 12
2022-05-28 13:16:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 13:16:59 | INFO | train_inner | epoch 012:     84 / 6032 loss=4.596, nll_loss=2.866, ppl=7.29, wps=7343.6, ups=2.72, wpb=2704.3, bsz=128, num_updates=66300, lr=0.000171938, gnorm=1.029, loss_scale=16, train_wall=8, gb_free=20.4, wall=5558
2022-05-28 13:17:07 | INFO | train_inner | epoch 012:    184 / 6032 loss=4.539, nll_loss=2.8, ppl=6.96, wps=35978.9, ups=12.55, wpb=2867.3, bsz=128, num_updates=66400, lr=0.000171808, gnorm=0.979, loss_scale=16, train_wall=8, gb_free=20.9, wall=5566
2022-05-28 13:17:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.97 GiB already allocated; 1.55 GiB free; 20.48 GiB reserved in total by PyTorch)
2022-05-28 13:17:12 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 131          |        cudaMalloc retries: 289       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14034 MB |   18404 MB |  407328 GB |  407314 GB |
|       from large pool |   13995 MB |   18365 MB |  398266 GB |  398253 GB |
|       from small pool |      39 MB |      73 MB |    9061 GB |    9061 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14034 MB |   18404 MB |  407328 GB |  407314 GB |
|       from large pool |   13995 MB |   18365 MB |  398266 GB |  398253 GB |
|       from small pool |      39 MB |      73 MB |    9061 GB |    9061 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20970 MB |   20970 MB |    1409 GB |    1388 GB |
|       from large pool |   20922 MB |   20922 MB |    1376 GB |    1356 GB |
|       from small pool |      48 MB |     184 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2563 MB |    2635 MB |  581658 GB |  581655 GB |
|       from large pool |    2554 MB |    2626 MB |  570963 GB |  570960 GB |
|       from small pool |       8 MB |      18 MB |   10694 GB |   10694 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  107127 K  |  107126 K  |
|       from large pool |     271    |     275    |   50692 K  |   50692 K  |
|       from small pool |     321    |     466    |   56434 K  |   56434 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  107127 K  |  107126 K  |
|       from large pool |     271    |     275    |   50692 K  |   50692 K  |
|       from small pool |     321    |     466    |   56434 K  |   56434 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     120    |   17344    |   17292    |
|       from large pool |      28    |      28    |     743    |     715    |
|       from small pool |      24    |      92    |   16601    |   16577    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      67    |   54627 K  |   54627 K  |
|       from large pool |      18    |      19    |   28887 K  |   28887 K  |
|       from small pool |      13    |      51    |   25739 K  |   25739 K  |
|===========================================================================|

2022-05-28 13:17:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:17:14 | INFO | train_inner | epoch 012:    285 / 6032 loss=4.38, nll_loss=2.616, ppl=6.13, wps=32450.8, ups=13.15, wpb=2468.3, bsz=128, num_updates=66500, lr=0.000171679, gnorm=1.02, loss_scale=16, train_wall=7, gb_free=19.6, wall=5573
2022-05-28 13:17:22 | INFO | train_inner | epoch 012:    385 / 6032 loss=4.471, nll_loss=2.722, ppl=6.6, wps=35652.5, ups=13.1, wpb=2720.8, bsz=128, num_updates=66600, lr=0.00017155, gnorm=0.997, loss_scale=16, train_wall=7, gb_free=19, wall=5581
2022-05-28 13:17:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 21.67 GiB already allocated; 131.69 MiB free; 21.90 GiB reserved in total by PyTorch)
2022-05-28 13:17:22 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 132          |        cudaMalloc retries: 291       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17084 MB |   22187 MB |  408125 GB |  408109 GB |
|       from large pool |   17044 MB |   22147 MB |  399048 GB |  399031 GB |
|       from small pool |      39 MB |      73 MB |    9077 GB |    9077 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17084 MB |   22187 MB |  408125 GB |  408109 GB |
|       from large pool |   17044 MB |   22147 MB |  399048 GB |  399031 GB |
|       from small pool |      39 MB |      73 MB |    9077 GB |    9077 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22428 MB |   22428 MB |    1419 GB |    1397 GB |
|       from large pool |   22386 MB |   22386 MB |    1386 GB |    1365 GB |
|       from small pool |      42 MB |     202 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  243054 KB |    2703 MB |  582439 GB |  582439 GB |
|       from large pool |  242972 KB |    2702 MB |  571725 GB |  571724 GB |
|       from small pool |      82 KB |       3 MB |   10714 GB |   10714 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  107328 K  |  107328 K  |
|       from large pool |     271    |     275    |   50788 K  |   50788 K  |
|       from small pool |     321    |     466    |   56539 K  |   56539 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  107328 K  |  107328 K  |
|       from large pool |     271    |     275    |   50788 K  |   50788 K  |
|       from small pool |     321    |     466    |   56539 K  |   56539 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     129    |   17423    |   17374    |
|       from large pool |      28    |      28    |     745    |     717    |
|       from small pool |      21    |     101    |   16678    |   16657    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      32    |   54730 K  |   54730 K  |
|       from large pool |      20    |      21    |   28941 K  |   28941 K  |
|       from small pool |       9    |      18    |   25788 K  |   25788 K  |
|===========================================================================|

2022-05-28 13:17:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:17:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 3.80 GiB free; 18.23 GiB reserved in total by PyTorch)
2022-05-28 13:17:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 133          |        cudaMalloc retries: 292       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12954 MB |   16934 MB |  408459 GB |  408446 GB |
|       from large pool |   12915 MB |   16895 MB |  399375 GB |  399362 GB |
|       from small pool |      39 MB |      73 MB |    9083 GB |    9083 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12954 MB |   16934 MB |  408459 GB |  408446 GB |
|       from large pool |   12915 MB |   16895 MB |  399375 GB |  399362 GB |
|       from small pool |      39 MB |      73 MB |    9083 GB |    9083 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18664 MB |   21460 MB |    1423 GB |    1405 GB |
|       from large pool |   18624 MB |   21262 MB |    1390 GB |    1372 GB |
|       from small pool |      40 MB |     198 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1729 MB |    2588 MB |  582777 GB |  582775 GB |
|       from large pool |    1728 MB |    2587 MB |  572055 GB |  572053 GB |
|       from small pool |       0 MB |       3 MB |   10721 GB |   10721 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  107408 K  |  107408 K  |
|       from large pool |     271    |     275    |   50827 K  |   50827 K  |
|       from small pool |     321    |     466    |   56580 K  |   56580 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  107408 K  |  107408 K  |
|       from large pool |     271    |     275    |   50827 K  |   50827 K  |
|       from small pool |     321    |     466    |   56580 K  |   56580 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      47    |     127    |   17503    |   17456    |
|       from large pool |      27    |      28    |     746    |     719    |
|       from small pool |      20    |      99    |   16757    |   16737    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      30    |   54770 K  |   54770 K  |
|       from large pool |      18    |      18    |   28963 K  |   28963 K  |
|       from small pool |       7    |      18    |   25807 K  |   25807 K  |
|===========================================================================|

2022-05-28 13:17:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:17:30 | INFO | train_inner | epoch 012:    487 / 6032 loss=4.601, nll_loss=2.87, ppl=7.31, wps=33518.8, ups=12.24, wpb=2739.2, bsz=128, num_updates=66700, lr=0.000171421, gnorm=1.003, loss_scale=16, train_wall=7, gb_free=20.7, wall=5589
2022-05-28 13:17:38 | INFO | train_inner | epoch 012:    587 / 6032 loss=4.526, nll_loss=2.785, ppl=6.89, wps=34725.3, ups=13.11, wpb=2648.5, bsz=128, num_updates=66800, lr=0.000171293, gnorm=1.026, loss_scale=16, train_wall=7, gb_free=20.4, wall=5597
2022-05-28 13:17:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 469.69 MiB free; 21.57 GiB reserved in total by PyTorch)
2022-05-28 13:17:45 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 134          |        cudaMalloc retries: 293       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21956 MB |   22256 MB |  409842 GB |  409820 GB |
|       from large pool |   21913 MB |   22213 MB |  400727 GB |  400705 GB |
|       from small pool |      42 MB |      73 MB |    9115 GB |    9115 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21956 MB |   22256 MB |  409842 GB |  409820 GB |
|       from large pool |   21913 MB |   22213 MB |  400727 GB |  400705 GB |
|       from small pool |      42 MB |      73 MB |    9115 GB |    9115 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22090 MB |   22548 MB |    1431 GB |    1409 GB |
|       from large pool |   22046 MB |   22346 MB |    1398 GB |    1376 GB |
|       from small pool |      44 MB |     202 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  136615 KB |    5181 MB |  584154 GB |  584154 GB |
|       from large pool |  135198 KB |    5181 MB |  573395 GB |  573395 GB |
|       from small pool |    1417 KB |      12 MB |   10758 GB |   10758 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |  107771 K  |  107771 K  |
|       from large pool |     214    |     215    |   50997 K  |   50997 K  |
|       from small pool |     302    |     466    |   56773 K  |   56773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |  107771 K  |  107771 K  |
|       from large pool |     214    |     215    |   50997 K  |   50997 K  |
|       from small pool |     302    |     466    |   56773 K  |   56773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      54    |     134    |   17591    |   17537    |
|       from large pool |      32    |      33    |     753    |     721    |
|       from small pool |      22    |     101    |   16838    |   16816    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      43    |   54953 K  |   54953 K  |
|       from large pool |      21    |      24    |   29058 K  |   29058 K  |
|       from small pool |      12    |      19    |   25895 K  |   25895 K  |
|===========================================================================|

2022-05-28 13:17:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:17:46 | INFO | train_inner | epoch 012:    688 / 6032 loss=4.612, nll_loss=2.881, ppl=7.37, wps=34264, ups=12.74, wpb=2689.6, bsz=128, num_updates=66900, lr=0.000171165, gnorm=1.03, loss_scale=16, train_wall=7, gb_free=16.9, wall=5605
2022-05-28 13:17:53 | INFO | train_inner | epoch 012:    788 / 6032 loss=4.509, nll_loss=2.764, ppl=6.79, wps=34526, ups=13.38, wpb=2581.3, bsz=128, num_updates=67000, lr=0.000171037, gnorm=1.055, loss_scale=16, train_wall=7, gb_free=20.7, wall=5612
2022-05-28 13:18:00 | INFO | train_inner | epoch 012:    888 / 6032 loss=4.453, nll_loss=2.698, ppl=6.49, wps=32771.1, ups=13.77, wpb=2379.6, bsz=128, num_updates=67100, lr=0.00017091, gnorm=1.057, loss_scale=16, train_wall=7, gb_free=17.2, wall=5619
2022-05-28 13:18:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 3.56 GiB free; 18.47 GiB reserved in total by PyTorch)
2022-05-28 13:18:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 135          |        cudaMalloc retries: 294       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15208 MB |   15259 MB |  411540 GB |  411525 GB |
|       from large pool |   15168 MB |   15219 MB |  402388 GB |  402374 GB |
|       from small pool |      39 MB |      73 MB |    9151 GB |    9151 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15208 MB |   15259 MB |  411540 GB |  411525 GB |
|       from large pool |   15168 MB |   15219 MB |  402388 GB |  402374 GB |
|       from small pool |      39 MB |      73 MB |    9151 GB |    9151 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18912 MB |   22248 MB |    1431 GB |    1412 GB |
|       from large pool |   18872 MB |   22046 MB |    1398 GB |    1379 GB |
|       from small pool |      40 MB |     202 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3703 MB |    3703 MB |  585744 GB |  585741 GB |
|       from large pool |    3703 MB |    3703 MB |  574942 GB |  574939 GB |
|       from small pool |       0 MB |      19 MB |   10801 GB |   10801 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  108224 K  |  108224 K  |
|       from large pool |     271    |     275    |   51213 K  |   51213 K  |
|       from small pool |     321    |     466    |   57011 K  |   57010 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  108224 K  |  108224 K  |
|       from large pool |     271    |     275    |   51213 K  |   51213 K  |
|       from small pool |     321    |     466    |   57011 K  |   57010 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     133    |   17670    |   17619    |
|       from large pool |      31    |      32    |     753    |     722    |
|       from small pool |      20    |     101    |   16917    |   16897    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      70    |   55184 K  |   55184 K  |
|       from large pool |      22    |      22    |   29181 K  |   29181 K  |
|       from small pool |       8    |      50    |   26003 K  |   26003 K  |
|===========================================================================|

2022-05-28 13:18:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:18:09 | INFO | train_inner | epoch 012:    989 / 6032 loss=4.759, nll_loss=3.051, ppl=8.29, wps=35878.7, ups=12.17, wpb=2948, bsz=128, num_updates=67200, lr=0.000170783, gnorm=1.042, loss_scale=16, train_wall=8, gb_free=20.2, wall=5628
2022-05-28 13:18:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 23.70 GiB total capacity; 15.64 GiB already allocated; 3.08 GiB free; 18.95 GiB reserved in total by PyTorch)
2022-05-28 13:18:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 136          |        cudaMalloc retries: 296       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12350 MB |   16012 MB |  411687 GB |  411675 GB |
|       from large pool |   12310 MB |   15973 MB |  402533 GB |  402521 GB |
|       from small pool |      39 MB |      73 MB |    9153 GB |    9153 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12350 MB |   16012 MB |  411687 GB |  411675 GB |
|       from large pool |   12310 MB |   15973 MB |  402533 GB |  402521 GB |
|       from small pool |      39 MB |      73 MB |    9153 GB |    9153 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19404 MB |   19404 MB |    1435 GB |    1416 GB |
|       from large pool |   19362 MB |   19362 MB |    1401 GB |    1383 GB |
|       from small pool |      42 MB |     132 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3389 MB |    3391 MB |  585887 GB |  585883 GB |
|       from large pool |    3387 MB |    3388 MB |  575082 GB |  575078 GB |
|       from small pool |       2 MB |      17 MB |   10804 GB |   10804 GB |
|---------------------------------------------------------------------------|
| Allocations           |     589    |     594    |  108261 K  |  108260 K  |
|       from large pool |     271    |     275    |   51231 K  |   51230 K  |
|       from small pool |     318    |     466    |   57030 K  |   57029 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     589    |     594    |  108261 K  |  108260 K  |
|       from large pool |     271    |     275    |   51231 K  |   51230 K  |
|       from small pool |     318    |     466    |   57030 K  |   57029 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |      97    |   17717    |   17665    |
|       from large pool |      31    |      31    |     754    |     723    |
|       from small pool |      21    |      66    |   16963    |   16942    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      71    |   55203 K  |   55203 K  |
|       from large pool |      22    |      23    |   29191 K  |   29191 K  |
|       from small pool |       9    |      56    |   26011 K  |   26011 K  |
|===========================================================================|

2022-05-28 13:18:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:18:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 2.03 GiB free; 20.00 GiB reserved in total by PyTorch)
2022-05-28 13:18:15 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 137          |        cudaMalloc retries: 299       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13288 MB |   17219 MB |  412137 GB |  412124 GB |
|       from large pool |   13249 MB |   17179 MB |  402971 GB |  402958 GB |
|       from small pool |      39 MB |      73 MB |    9165 GB |    9165 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13288 MB |   17219 MB |  412137 GB |  412124 GB |
|       from large pool |   13249 MB |   17179 MB |  402971 GB |  402958 GB |
|       from small pool |      39 MB |      73 MB |    9165 GB |    9165 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20476 MB |   20476 MB |    1450 GB |    1430 GB |
|       from large pool |   20434 MB |   20434 MB |    1416 GB |    1396 GB |
|       from small pool |      42 MB |     196 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3255 MB |    3256 MB |  586308 GB |  586305 GB |
|       from large pool |    3252 MB |    3254 MB |  575489 GB |  575486 GB |
|       from small pool |       2 MB |      16 MB |   10818 GB |   10818 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  108379 K  |  108379 K  |
|       from large pool |     271    |     275    |   51284 K  |   51284 K  |
|       from small pool |     321    |     466    |   57094 K  |   57094 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  108379 K  |  108379 K  |
|       from large pool |     271    |     275    |   51284 K  |   51284 K  |
|       from small pool |     321    |     466    |   57094 K  |   57094 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     128    |   17878    |   17827    |
|       from large pool |      30    |      30    |     759    |     729    |
|       from small pool |      21    |      98    |   17119    |   17098    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      51    |   55263 K  |   55263 K  |
|       from large pool |      23    |      24    |   29221 K  |   29221 K  |
|       from small pool |      11    |      41    |   26041 K  |   26041 K  |
|===========================================================================|

2022-05-28 13:18:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:18:17 | INFO | train_inner | epoch 012:   1091 / 6032 loss=4.495, nll_loss=2.749, ppl=6.72, wps=29874.3, ups=12.24, wpb=2441.2, bsz=128, num_updates=67300, lr=0.000170656, gnorm=1.068, loss_scale=16, train_wall=8, gb_free=20.5, wall=5636
2022-05-28 13:18:24 | INFO | train_inner | epoch 012:   1191 / 6032 loss=4.392, nll_loss=2.632, ppl=6.2, wps=33650.5, ups=13.63, wpb=2468.6, bsz=128, num_updates=67400, lr=0.000170529, gnorm=1.048, loss_scale=16, train_wall=7, gb_free=20, wall=5643
2022-05-28 13:18:32 | INFO | train_inner | epoch 012:   1291 / 6032 loss=4.571, nll_loss=2.837, ppl=7.14, wps=34820.9, ups=12.85, wpb=2710.1, bsz=128, num_updates=67500, lr=0.000170403, gnorm=1.045, loss_scale=16, train_wall=8, gb_free=20.1, wall=5651
2022-05-28 13:18:40 | INFO | train_inner | epoch 012:   1391 / 6032 loss=4.744, nll_loss=3.034, ppl=8.19, wps=37145.4, ups=12.94, wpb=2870.6, bsz=128, num_updates=67600, lr=0.000170276, gnorm=1.036, loss_scale=16, train_wall=8, gb_free=18.7, wall=5659
2022-05-28 13:18:47 | INFO | train_inner | epoch 012:   1491 / 6032 loss=4.566, nll_loss=2.83, ppl=7.11, wps=34938.8, ups=13.09, wpb=2668.6, bsz=128, num_updates=67700, lr=0.000170151, gnorm=1.03, loss_scale=16, train_wall=7, gb_free=8.2, wall=5666
2022-05-28 13:18:55 | INFO | train_inner | epoch 012:   1591 / 6032 loss=4.548, nll_loss=2.812, ppl=7.02, wps=34219.3, ups=13.48, wpb=2537.8, bsz=128, num_updates=67800, lr=0.000170025, gnorm=1.054, loss_scale=16, train_wall=7, gb_free=19.8, wall=5674
2022-05-28 13:19:02 | INFO | train_inner | epoch 012:   1691 / 6032 loss=4.526, nll_loss=2.787, ppl=6.9, wps=31862.4, ups=13.22, wpb=2409.4, bsz=128, num_updates=67900, lr=0.0001699, gnorm=1.095, loss_scale=16, train_wall=7, gb_free=20.5, wall=5681
2022-05-28 13:19:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.46 GiB (GPU 0; 23.70 GiB total capacity; 15.28 GiB already allocated; 2.47 GiB free; 19.56 GiB reserved in total by PyTorch)
2022-05-28 13:19:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 138          |        cudaMalloc retries: 301       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12107 MB |   15648 MB |  416065 GB |  416053 GB |
|       from large pool |   12068 MB |   15608 MB |  406801 GB |  406789 GB |
|       from small pool |      39 MB |      73 MB |    9263 GB |    9263 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12107 MB |   15648 MB |  416065 GB |  416053 GB |
|       from large pool |   12068 MB |   15608 MB |  406801 GB |  406789 GB |
|       from small pool |      39 MB |      73 MB |    9263 GB |    9263 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20034 MB |   20034 MB |    1456 GB |    1437 GB |
|       from large pool |   19994 MB |   19994 MB |    1423 GB |    1403 GB |
|       from small pool |      40 MB |     202 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4384 MB |    4385 MB |  590453 GB |  590449 GB |
|       from large pool |    4383 MB |    4385 MB |  579519 GB |  579515 GB |
|       from small pool |       0 MB |      14 MB |   10933 GB |   10933 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  109450 K  |  109450 K  |
|       from large pool |     271    |     275    |   51779 K  |   51778 K  |
|       from small pool |     321    |     466    |   57671 K  |   57671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  109450 K  |  109450 K  |
|       from large pool |     271    |     275    |   51779 K  |   51778 K  |
|       from small pool |     321    |     466    |   57671 K  |   57671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     131    |   17960    |   17910    |
|       from large pool |      30    |      30    |     761    |     731    |
|       from small pool |      20    |     101    |   17199    |   17179    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      60    |   55809 K  |   55809 K  |
|       from large pool |      23    |      24    |   29502 K  |   29502 K  |
|       from small pool |       7    |      42    |   26307 K  |   26307 K  |
|===========================================================================|

2022-05-28 13:19:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:19:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.27 GiB (GPU 0; 23.70 GiB total capacity; 14.69 GiB already allocated; 2.66 GiB free; 19.37 GiB reserved in total by PyTorch)
2022-05-28 13:19:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 139          |        cudaMalloc retries: 302       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   11699 MB |   15044 MB |  416091 GB |  416080 GB |
|       from large pool |   11660 MB |   15005 MB |  406827 GB |  406816 GB |
|       from small pool |      38 MB |      73 MB |    9263 GB |    9263 GB |
|---------------------------------------------------------------------------|
| Active memory         |   11699 MB |   15044 MB |  416091 GB |  416080 GB |
|       from large pool |   11660 MB |   15005 MB |  406827 GB |  406816 GB |
|       from small pool |      38 MB |      73 MB |    9263 GB |    9263 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19838 MB |   19874 MB |    1459 GB |    1440 GB |
|       from large pool |   19798 MB |   19798 MB |    1426 GB |    1407 GB |
|       from small pool |      40 MB |      76 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4792 MB |    4793 MB |  590481 GB |  590477 GB |
|       from large pool |    4791 MB |    4792 MB |  579547 GB |  579543 GB |
|       from small pool |       1 MB |       4 MB |   10933 GB |   10933 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  109452 K  |  109452 K  |
|       from large pool |     271    |     275    |   51780 K  |   51780 K  |
|       from small pool |     321    |     466    |   57672 K  |   57672 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  109452 K  |  109452 K  |
|       from large pool |     271    |     275    |   51780 K  |   51780 K  |
|       from small pool |     321    |     466    |   57672 K  |   57672 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |      68    |   17979    |   17929    |
|       from large pool |      30    |      30    |     762    |     732    |
|       from small pool |      20    |      38    |   17217    |   17197    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      35    |   55810 K  |   55810 K  |
|       from large pool |      22    |      23    |   29502 K  |   29502 K  |
|       from small pool |       7    |      18    |   26307 K  |   26307 K  |
|===========================================================================|

2022-05-28 13:19:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:19:10 | INFO | train_inner | epoch 012:   1793 / 6032 loss=4.515, nll_loss=2.773, ppl=6.84, wps=32105.2, ups=12.54, wpb=2560.8, bsz=128, num_updates=68000, lr=0.000169775, gnorm=1.058, loss_scale=16, train_wall=7, gb_free=19.5, wall=5689
2022-05-28 13:19:18 | INFO | train_inner | epoch 012:   1893 / 6032 loss=4.644, nll_loss=2.918, ppl=7.56, wps=35595.5, ups=12.91, wpb=2757.6, bsz=128, num_updates=68100, lr=0.00016965, gnorm=1.07, loss_scale=16, train_wall=8, gb_free=20.4, wall=5697
2022-05-28 13:19:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-05-28 13:19:26 | INFO | train_inner | epoch 012:   1994 / 6032 loss=4.532, nll_loss=2.792, ppl=6.92, wps=33316.7, ups=12.39, wpb=2688.4, bsz=128, num_updates=68200, lr=0.000169526, gnorm=1.062, loss_scale=8, train_wall=8, gb_free=19, wall=5705
2022-05-28 13:19:33 | INFO | train_inner | epoch 012:   2094 / 6032 loss=4.569, nll_loss=2.834, ppl=7.13, wps=33818.7, ups=13.45, wpb=2514.3, bsz=128, num_updates=68300, lr=0.000169402, gnorm=1.09, loss_scale=8, train_wall=7, gb_free=19.8, wall=5713
2022-05-28 13:19:41 | INFO | train_inner | epoch 012:   2194 / 6032 loss=4.656, nll_loss=2.933, ppl=7.64, wps=36027.4, ups=13.03, wpb=2764.6, bsz=128, num_updates=68400, lr=0.000169278, gnorm=1.056, loss_scale=8, train_wall=7, gb_free=20.4, wall=5720
2022-05-28 13:19:49 | INFO | train_inner | epoch 012:   2294 / 6032 loss=4.73, nll_loss=3.02, ppl=8.11, wps=36987.5, ups=12.78, wpb=2894.2, bsz=128, num_updates=68500, lr=0.000169154, gnorm=1.038, loss_scale=8, train_wall=8, gb_free=18.4, wall=5728
2022-05-28 13:19:56 | INFO | train_inner | epoch 012:   2394 / 6032 loss=4.573, nll_loss=2.839, ppl=7.16, wps=35262.1, ups=13.19, wpb=2672.8, bsz=128, num_updates=68600, lr=0.000169031, gnorm=1.027, loss_scale=8, train_wall=7, gb_free=19.7, wall=5736
2022-05-28 13:20:04 | INFO | train_inner | epoch 012:   2494 / 6032 loss=4.589, nll_loss=2.858, ppl=7.25, wps=36530.9, ups=13.19, wpb=2769.8, bsz=128, num_updates=68700, lr=0.000168908, gnorm=1.039, loss_scale=8, train_wall=7, gb_free=20.1, wall=5743
2022-05-28 13:20:12 | INFO | train_inner | epoch 012:   2594 / 6032 loss=4.621, nll_loss=2.896, ppl=7.44, wps=35865.6, ups=13.17, wpb=2723.6, bsz=128, num_updates=68800, lr=0.000168785, gnorm=1.062, loss_scale=8, train_wall=7, gb_free=20.4, wall=5751
2022-05-28 13:20:19 | INFO | train_inner | epoch 012:   2694 / 6032 loss=4.565, nll_loss=2.831, ppl=7.11, wps=34836.3, ups=13.2, wpb=2638.8, bsz=128, num_updates=68900, lr=0.000168662, gnorm=1.055, loss_scale=8, train_wall=7, gb_free=20.3, wall=5758
2022-05-28 13:20:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.25 GiB already allocated; 1.68 GiB free; 20.35 GiB reserved in total by PyTorch)
2022-05-28 13:20:27 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 140          |        cudaMalloc retries: 306       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12704 MB |   16635 MB |  422237 GB |  422225 GB |
|       from large pool |   12665 MB |   16595 MB |  412837 GB |  412825 GB |
|       from small pool |      39 MB |      73 MB |    9399 GB |    9399 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12704 MB |   16635 MB |  422237 GB |  422225 GB |
|       from large pool |   12665 MB |   16595 MB |  412837 GB |  412825 GB |
|       from small pool |      39 MB |      73 MB |    9399 GB |    9399 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20840 MB |   20840 MB |    1477 GB |    1456 GB |
|       from large pool |   20800 MB |   20800 MB |    1443 GB |    1422 GB |
|       from small pool |      40 MB |     204 MB |      34 GB |      34 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4203 MB |    4204 MB |  596986 GB |  596982 GB |
|       from large pool |    4202 MB |    4204 MB |  585890 GB |  585886 GB |
|       from small pool |       0 MB |       4 MB |   11095 GB |   11095 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  111061 K  |  111061 K  |
|       from large pool |     271    |     275    |   52538 K  |   52537 K  |
|       from small pool |     321    |     466    |   58523 K  |   58523 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  111061 K  |  111061 K  |
|       from large pool |     271    |     275    |   52538 K  |   52537 K  |
|       from small pool |     321    |     466    |   58523 K  |   58523 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     132    |   18215    |   18165    |
|       from large pool |      30    |      30    |     767    |     737    |
|       from small pool |      20    |     102    |   17448    |   17428    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      35    |   56628 K  |   56628 K  |
|       from large pool |      18    |      19    |   29930 K  |   29930 K  |
|       from small pool |      11    |      20    |   26698 K  |   26698 K  |
|===========================================================================|

2022-05-28 13:20:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:20:27 | INFO | train_inner | epoch 012:   2795 / 6032 loss=4.506, nll_loss=2.764, ppl=6.79, wps=32877.5, ups=12.91, wpb=2546, bsz=128, num_updates=69000, lr=0.00016854, gnorm=1.063, loss_scale=8, train_wall=7, gb_free=19.6, wall=5766
2022-05-28 13:20:35 | INFO | train_inner | epoch 012:   2895 / 6032 loss=4.627, nll_loss=2.903, ppl=7.48, wps=35175, ups=13.19, wpb=2666.6, bsz=128, num_updates=69100, lr=0.000168418, gnorm=1.086, loss_scale=8, train_wall=7, gb_free=17.9, wall=5774
2022-05-28 13:20:42 | INFO | train_inner | epoch 012:   2995 / 6032 loss=4.554, nll_loss=2.819, ppl=7.05, wps=33098.3, ups=12.93, wpb=2559.2, bsz=128, num_updates=69200, lr=0.000168296, gnorm=1.098, loss_scale=8, train_wall=8, gb_free=17.1, wall=5781
2022-05-28 13:20:50 | INFO | train_inner | epoch 012:   3095 / 6032 loss=4.387, nll_loss=2.628, ppl=6.18, wps=32132.3, ups=13.8, wpb=2328.8, bsz=128, num_updates=69300, lr=0.000168175, gnorm=1.089, loss_scale=8, train_wall=7, gb_free=17.9, wall=5789
2022-05-28 13:20:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.62 GiB (GPU 0; 23.70 GiB total capacity; 15.66 GiB already allocated; 1.75 GiB free; 20.28 GiB reserved in total by PyTorch)
2022-05-28 13:20:55 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 141          |        cudaMalloc retries: 309       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12327 MB |   16038 MB |  424354 GB |  424342 GB |
|       from large pool |   12288 MB |   15999 MB |  414906 GB |  414894 GB |
|       from small pool |      39 MB |      73 MB |    9447 GB |    9447 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12327 MB |   16038 MB |  424354 GB |  424342 GB |
|       from large pool |   12288 MB |   15999 MB |  414906 GB |  414894 GB |
|       from small pool |      39 MB |      73 MB |    9447 GB |    9447 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20766 MB |   20766 MB |    1488 GB |    1467 GB |
|       from large pool |   20726 MB |   20726 MB |    1453 GB |    1433 GB |
|       from small pool |      40 MB |     198 MB |      34 GB |      34 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4726 MB |    4727 MB |  599228 GB |  599223 GB |
|       from large pool |    4725 MB |    4726 MB |  588076 GB |  588071 GB |
|       from small pool |       0 MB |       3 MB |   11152 GB |   11152 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  111633 K  |  111632 K  |
|       from large pool |     271    |     275    |   52807 K  |   52806 K  |
|       from small pool |     321    |     466    |   58826 K  |   58825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  111633 K  |  111632 K  |
|       from large pool |     271    |     275    |   52807 K  |   52806 K  |
|       from small pool |     321    |     466    |   58826 K  |   58825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     129    |   18377    |   18327    |
|       from large pool |      30    |      30    |     770    |     740    |
|       from small pool |      20    |      99    |   17607    |   17587    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      34    |   56919 K  |   56919 K  |
|       from large pool |      23    |      24    |   30083 K  |   30083 K  |
|       from small pool |       8    |      18    |   26835 K  |   26835 K  |
|===========================================================================|

2022-05-28 13:20:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:20:57 | INFO | train_inner | epoch 012:   3196 / 6032 loss=4.682, nll_loss=2.964, ppl=7.8, wps=35401.3, ups=12.64, wpb=2801.7, bsz=128, num_updates=69400, lr=0.000168054, gnorm=1.065, loss_scale=8, train_wall=7, gb_free=18.2, wall=5797
2022-05-28 13:21:05 | INFO | train_inner | epoch 012:   3296 / 6032 loss=4.678, nll_loss=2.96, ppl=7.78, wps=37231.3, ups=13, wpb=2864.2, bsz=128, num_updates=69500, lr=0.000167933, gnorm=1.025, loss_scale=8, train_wall=8, gb_free=16.5, wall=5804
2022-05-28 13:21:13 | INFO | train_inner | epoch 012:   3396 / 6032 loss=4.537, nll_loss=2.799, ppl=6.96, wps=33985.1, ups=13.53, wpb=2512.6, bsz=128, num_updates=69600, lr=0.000167812, gnorm=1.07, loss_scale=8, train_wall=7, gb_free=18.4, wall=5812
2022-05-28 13:21:20 | INFO | train_inner | epoch 012:   3496 / 6032 loss=4.542, nll_loss=2.806, ppl=6.99, wps=34592.1, ups=13.43, wpb=2576.6, bsz=128, num_updates=69700, lr=0.000167692, gnorm=1.073, loss_scale=8, train_wall=7, gb_free=19.7, wall=5819
2022-05-28 13:21:28 | INFO | train_inner | epoch 012:   3596 / 6032 loss=4.668, nll_loss=2.95, ppl=7.73, wps=36398.7, ups=12.74, wpb=2857, bsz=128, num_updates=69800, lr=0.000167572, gnorm=1.027, loss_scale=8, train_wall=8, gb_free=19.9, wall=5827
2022-05-28 13:21:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 2.25 GiB free; 19.78 GiB reserved in total by PyTorch)
2022-05-28 13:21:29 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 142          |        cudaMalloc retries: 311       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13631 MB |   17562 MB |  426978 GB |  426965 GB |
|       from large pool |   13592 MB |   17522 MB |  417472 GB |  417459 GB |
|       from small pool |      39 MB |      73 MB |    9505 GB |    9505 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13631 MB |   17562 MB |  426978 GB |  426965 GB |
|       from large pool |   13592 MB |   17522 MB |  417472 GB |  417459 GB |
|       from small pool |      39 MB |      73 MB |    9505 GB |    9505 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20254 MB |   20254 MB |    1494 GB |    1474 GB |
|       from large pool |   20214 MB |   20214 MB |    1460 GB |    1440 GB |
|       from small pool |      40 MB |     202 MB |      34 GB |      34 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2690 MB |    2691 MB |  602044 GB |  602042 GB |
|       from large pool |    2689 MB |    2691 MB |  590823 GB |  590820 GB |
|       from small pool |       0 MB |       3 MB |   11221 GB |   11221 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  112332 K  |  112331 K  |
|       from large pool |     271    |     275    |   53137 K  |   53136 K  |
|       from small pool |     321    |     466    |   59195 K  |   59194 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  112332 K  |  112331 K  |
|       from large pool |     271    |     275    |   53137 K  |   53136 K  |
|       from small pool |     321    |     466    |   59195 K  |   59194 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     131    |   18460    |   18410    |
|       from large pool |      30    |      30    |     772    |     742    |
|       from small pool |      20    |     101    |   17688    |   17668    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      36    |      38    |   57274 K  |   57274 K  |
|       from large pool |      26    |      27    |   30270 K  |   30270 K  |
|       from small pool |      10    |      18    |   27004 K  |   27004 K  |
|===========================================================================|

2022-05-28 13:21:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:21:36 | INFO | train_inner | epoch 012:   3697 / 6032 loss=4.579, nll_loss=2.848, ppl=7.2, wps=33242.5, ups=12.91, wpb=2574.2, bsz=128, num_updates=69900, lr=0.000167452, gnorm=1.068, loss_scale=8, train_wall=7, gb_free=18.8, wall=5835
2022-05-28 13:21:43 | INFO | train_inner | epoch 012:   3797 / 6032 loss=4.832, nll_loss=3.137, ppl=8.8, wps=35480, ups=13.18, wpb=2692.2, bsz=128, num_updates=70000, lr=0.000167332, gnorm=1.071, loss_scale=8, train_wall=7, gb_free=11.5, wall=5842
2022-05-28 13:21:51 | INFO | train_inner | epoch 012:   3897 / 6032 loss=4.756, nll_loss=3.05, ppl=8.28, wps=37786.9, ups=12.91, wpb=2927.4, bsz=128, num_updates=70100, lr=0.000167213, gnorm=1.048, loss_scale=8, train_wall=8, gb_free=18.1, wall=5850
2022-05-28 13:21:58 | INFO | train_inner | epoch 012:   3997 / 6032 loss=4.497, nll_loss=2.755, ppl=6.75, wps=34380.4, ups=13.39, wpb=2568.1, bsz=128, num_updates=70200, lr=0.000167093, gnorm=1.085, loss_scale=8, train_wall=7, gb_free=20, wall=5858
2022-05-28 13:22:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.70 GiB total capacity; 15.42 GiB already allocated; 1.91 GiB free; 20.12 GiB reserved in total by PyTorch)
2022-05-28 13:22:05 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 143          |        cudaMalloc retries: 314       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12222 MB |   15787 MB |  429683 GB |  429671 GB |
|       from large pool |   12183 MB |   15748 MB |  420112 GB |  420100 GB |
|       from small pool |      39 MB |      73 MB |    9571 GB |    9571 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12222 MB |   15787 MB |  429683 GB |  429671 GB |
|       from large pool |   12183 MB |   15748 MB |  420112 GB |  420100 GB |
|       from small pool |      39 MB |      73 MB |    9571 GB |    9571 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20600 MB |   20660 MB |    1505 GB |    1485 GB |
|       from large pool |   20556 MB |   20556 MB |    1470 GB |    1450 GB |
|       from small pool |      44 MB |     202 MB |      34 GB |      34 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4811 MB |    4812 MB |  604883 GB |  604878 GB |
|       from large pool |    4806 MB |    4807 MB |  593584 GB |  593580 GB |
|       from small pool |       4 MB |      16 MB |   11298 GB |   11298 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  113059 K  |  113058 K  |
|       from large pool |     271    |     275    |   53474 K  |   53474 K  |
|       from small pool |     321    |     466    |   59584 K  |   59584 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  113059 K  |  113058 K  |
|       from large pool |     271    |     275    |   53474 K  |   53474 K  |
|       from small pool |     321    |     466    |   59584 K  |   59584 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     131    |   18625    |   18573    |
|       from large pool |      30    |      30    |     775    |     745    |
|       from small pool |      22    |     101    |   17850    |   17828    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      66    |   57644 K  |   57644 K  |
|       from large pool |      23    |      24    |   30460 K  |   30460 K  |
|       from small pool |      10    |      47    |   27184 K  |   27184 K  |
|===========================================================================|

2022-05-28 13:22:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:22:06 | INFO | train_inner | epoch 012:   4098 / 6032 loss=4.478, nll_loss=2.734, ppl=6.65, wps=30449.2, ups=12.89, wpb=2362.7, bsz=128, num_updates=70300, lr=0.000166975, gnorm=1.109, loss_scale=8, train_wall=7, gb_free=20.3, wall=5865
2022-05-28 13:22:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 1.39 GiB free; 20.64 GiB reserved in total by PyTorch)
2022-05-28 13:22:08 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 144          |        cudaMalloc retries: 315       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13822 MB |   17924 MB |  429916 GB |  429903 GB |
|       from large pool |   13782 MB |   17884 MB |  420340 GB |  420326 GB |
|       from small pool |      39 MB |      73 MB |    9576 GB |    9576 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13822 MB |   17924 MB |  429916 GB |  429903 GB |
|       from large pool |   13782 MB |   17884 MB |  420340 GB |  420326 GB |
|       from small pool |      39 MB |      73 MB |    9576 GB |    9576 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21132 MB |   21276 MB |    1509 GB |    1488 GB |
|       from large pool |   21092 MB |   21092 MB |    1474 GB |    1453 GB |
|       from small pool |      40 MB |     184 MB |      35 GB |      34 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3207 MB |    3207 MB |  605121 GB |  605118 GB |
|       from large pool |    3207 MB |    3207 MB |  593816 GB |  593813 GB |
|       from small pool |       0 MB |      33 MB |   11304 GB |   11304 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  113120 K  |  113120 K  |
|       from large pool |     271    |     275    |   53503 K  |   53503 K  |
|       from small pool |     321    |     466    |   59617 K  |   59616 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  113120 K  |  113120 K  |
|       from large pool |     271    |     275    |   53503 K  |   53503 K  |
|       from small pool |     321    |     466    |   59617 K  |   59616 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     122    |   18696    |   18646    |
|       from large pool |      30    |      30    |     776    |     746    |
|       from small pool |      20    |      92    |   17920    |   17900    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      78    |   57675 K  |   57675 K  |
|       from large pool |      22    |      22    |   30476 K  |   30476 K  |
|       from small pool |       8    |      63    |   27198 K  |   27198 K  |
|===========================================================================|

2022-05-28 13:22:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:22:14 | INFO | train_inner | epoch 012:   4199 / 6032 loss=4.641, nll_loss=2.92, ppl=7.57, wps=35634.7, ups=12.73, wpb=2799.9, bsz=128, num_updates=70400, lr=0.000166856, gnorm=1.047, loss_scale=8, train_wall=8, gb_free=20.6, wall=5873
2022-05-28 13:22:22 | INFO | train_inner | epoch 012:   4299 / 6032 loss=4.679, nll_loss=2.961, ppl=7.79, wps=36407.8, ups=13.14, wpb=2771.2, bsz=128, num_updates=70500, lr=0.000166738, gnorm=1.064, loss_scale=8, train_wall=7, gb_free=18.6, wall=5881
2022-05-28 13:22:29 | INFO | train_inner | epoch 012:   4399 / 6032 loss=4.637, nll_loss=2.915, ppl=7.54, wps=35138.1, ups=13.04, wpb=2695.2, bsz=128, num_updates=70600, lr=0.000166619, gnorm=1.075, loss_scale=8, train_wall=7, gb_free=19.3, wall=5888
2022-05-28 13:22:37 | INFO | train_inner | epoch 012:   4499 / 6032 loss=4.643, nll_loss=2.923, ppl=7.59, wps=34620, ups=12.97, wpb=2668.5, bsz=128, num_updates=70700, lr=0.000166502, gnorm=1.067, loss_scale=8, train_wall=8, gb_free=20.7, wall=5896
2022-05-28 13:22:45 | INFO | train_inner | epoch 012:   4599 / 6032 loss=4.66, nll_loss=2.94, ppl=7.68, wps=32696.5, ups=13.2, wpb=2477.5, bsz=128, num_updates=70800, lr=0.000166384, gnorm=1.097, loss_scale=8, train_wall=7, gb_free=20.5, wall=5904
2022-05-28 13:22:52 | INFO | train_inner | epoch 012:   4699 / 6032 loss=4.731, nll_loss=3.023, ppl=8.13, wps=36276.3, ups=13.14, wpb=2761.1, bsz=128, num_updates=70900, lr=0.000166267, gnorm=1.061, loss_scale=8, train_wall=7, gb_free=20.8, wall=5911
2022-05-28 13:23:00 | INFO | train_inner | epoch 012:   4799 / 6032 loss=4.728, nll_loss=3.019, ppl=8.11, wps=36450.6, ups=12.98, wpb=2809.2, bsz=128, num_updates=71000, lr=0.000166149, gnorm=1.072, loss_scale=8, train_wall=8, gb_free=20.4, wall=5919
2022-05-28 13:23:08 | INFO | train_inner | epoch 012:   4899 / 6032 loss=4.654, nll_loss=2.934, ppl=7.64, wps=36253.8, ups=13.01, wpb=2787, bsz=128, num_updates=71100, lr=0.000166033, gnorm=1.067, loss_scale=8, train_wall=7, gb_free=19.9, wall=5927
2022-05-28 13:23:15 | INFO | train_inner | epoch 012:   4999 / 6032 loss=4.704, nll_loss=2.994, ppl=7.96, wps=35112.9, ups=13.17, wpb=2665.5, bsz=128, num_updates=71200, lr=0.000165916, gnorm=1.06, loss_scale=8, train_wall=7, gb_free=18.8, wall=5934
2022-05-28 13:23:23 | INFO | train_inner | epoch 012:   5099 / 6032 loss=4.627, nll_loss=2.904, ppl=7.49, wps=34842.9, ups=13.36, wpb=2608.5, bsz=128, num_updates=71300, lr=0.0001658, gnorm=1.074, loss_scale=8, train_wall=7, gb_free=20.4, wall=5942
2022-05-28 13:23:30 | INFO | train_inner | epoch 012:   5199 / 6032 loss=4.59, nll_loss=2.862, ppl=7.27, wps=34119.7, ups=13.41, wpb=2544.6, bsz=126.8, num_updates=71400, lr=0.000165683, gnorm=1.146, loss_scale=8, train_wall=7, gb_free=19.5, wall=5949
2022-05-28 13:23:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 1.56 GiB free; 20.47 GiB reserved in total by PyTorch)
2022-05-28 13:23:34 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 145          |        cudaMalloc retries: 318       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13955 MB |   18277 MB |  436555 GB |  436542 GB |
|       from large pool |   13916 MB |   18238 MB |  426831 GB |  426817 GB |
|       from small pool |      39 MB |      73 MB |    9724 GB |    9724 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13955 MB |   18277 MB |  436555 GB |  436542 GB |
|       from large pool |   13916 MB |   18238 MB |  426831 GB |  426817 GB |
|       from small pool |      39 MB |      73 MB |    9724 GB |    9724 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20966 MB |   20966 MB |    1519 GB |    1499 GB |
|       from large pool |   20922 MB |   20922 MB |    1484 GB |    1463 GB |
|       from small pool |      44 MB |     202 MB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2688 MB |    2688 MB |  612160 GB |  612158 GB |
|       from large pool |    2683 MB |    2683 MB |  600680 GB |  600678 GB |
|       from small pool |       4 MB |       8 MB |   11480 GB |   11480 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  114885 K  |  114884 K  |
|       from large pool |     271    |     275    |   54336 K  |   54336 K  |
|       from small pool |     321    |     466    |   60549 K  |   60548 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  114885 K  |  114884 K  |
|       from large pool |     271    |     275    |   54336 K  |   54336 K  |
|       from small pool |     321    |     466    |   60549 K  |   60548 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      52    |     131    |   18857    |   18805    |
|       from large pool |      30    |      30    |     779    |     749    |
|       from small pool |      22    |     101    |   18078    |   18056    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      34    |      35    |   58572 K  |   58571 K  |
|       from large pool |      24    |      24    |   30947 K  |   30947 K  |
|       from small pool |      10    |      21    |   27624 K  |   27624 K  |
|===========================================================================|

2022-05-28 13:23:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:23:38 | INFO | train_inner | epoch 012:   5300 / 6032 loss=4.646, nll_loss=2.927, ppl=7.61, wps=32618.3, ups=12.38, wpb=2634.8, bsz=128, num_updates=71500, lr=0.000165567, gnorm=1.088, loss_scale=8, train_wall=8, gb_free=15.9, wall=5957
2022-05-28 13:23:46 | INFO | train_inner | epoch 012:   5400 / 6032 loss=4.713, nll_loss=3.002, ppl=8.01, wps=37445.4, ups=12.81, wpb=2923.6, bsz=128, num_updates=71600, lr=0.000165452, gnorm=1.017, loss_scale=8, train_wall=8, gb_free=18.5, wall=5965
2022-05-28 13:23:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 18.18 GiB already allocated; 2.44 GiB free; 19.59 GiB reserved in total by PyTorch)
2022-05-28 13:23:50 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 146          |        cudaMalloc retries: 321       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   18613 MB |   18676 MB |  437781 GB |  437763 GB |
|       from large pool |   18573 MB |   18636 MB |  428032 GB |  428014 GB |
|       from small pool |      40 MB |      73 MB |    9748 GB |    9748 GB |
|---------------------------------------------------------------------------|
| Active memory         |   18613 MB |   18676 MB |  437781 GB |  437763 GB |
|       from large pool |   18573 MB |   18636 MB |  428032 GB |  428014 GB |
|       from small pool |      40 MB |      73 MB |    9748 GB |    9748 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20064 MB |   20064 MB |    1531 GB |    1511 GB |
|       from large pool |   20022 MB |   20022 MB |    1495 GB |    1476 GB |
|       from small pool |      42 MB |     202 MB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1450 MB |    3141 MB |  613468 GB |  613467 GB |
|       from large pool |    1448 MB |    3140 MB |  601958 GB |  601957 GB |
|       from small pool |       1 MB |      17 MB |   11509 GB |   11509 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  115188 K  |  115187 K  |
|       from large pool |     271    |     275    |   54481 K  |   54480 K  |
|       from small pool |     321    |     466    |   60707 K  |   60706 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  115188 K  |  115187 K  |
|       from large pool |     271    |     275    |   54481 K  |   54480 K  |
|       from small pool |     321    |     466    |   60707 K  |   60706 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     131    |   19016    |   18966    |
|       from large pool |      29    |      30    |     782    |     753    |
|       from small pool |      21    |     101    |   18234    |   18213    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      64    |   58725 K  |   58725 K  |
|       from large pool |      22    |      22    |   31028 K  |   31028 K  |
|       from small pool |       8    |      50    |   27696 K  |   27696 K  |
|===========================================================================|

2022-05-28 13:23:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:23:54 | INFO | train_inner | epoch 012:   5501 / 6032 loss=4.678, nll_loss=2.961, ppl=7.79, wps=33197.3, ups=12.32, wpb=2693.7, bsz=128, num_updates=71700, lr=0.000165336, gnorm=1.069, loss_scale=8, train_wall=8, gb_free=19.9, wall=5973
2022-05-28 13:24:02 | INFO | train_inner | epoch 012:   5601 / 6032 loss=4.72, nll_loss=3.011, ppl=8.06, wps=35683, ups=13.04, wpb=2735.8, bsz=128, num_updates=71800, lr=0.000165221, gnorm=1.064, loss_scale=8, train_wall=7, gb_free=21, wall=5981
2022-05-28 13:24:10 | INFO | train_inner | epoch 012:   5701 / 6032 loss=4.644, nll_loss=2.924, ppl=7.59, wps=34544.3, ups=12.9, wpb=2677.4, bsz=128, num_updates=71900, lr=0.000165106, gnorm=1.069, loss_scale=8, train_wall=8, gb_free=20.2, wall=5989
2022-05-28 13:24:17 | INFO | train_inner | epoch 012:   5801 / 6032 loss=4.633, nll_loss=2.912, ppl=7.53, wps=35874.6, ups=12.97, wpb=2766.6, bsz=128, num_updates=72000, lr=0.000164992, gnorm=1.033, loss_scale=8, train_wall=8, gb_free=20.3, wall=5996
2022-05-28 13:24:25 | INFO | train_inner | epoch 012:   5901 / 6032 loss=4.762, nll_loss=3.058, ppl=8.33, wps=34635.3, ups=12.85, wpb=2695.1, bsz=128, num_updates=72100, lr=0.000164877, gnorm=1.085, loss_scale=8, train_wall=8, gb_free=20.3, wall=6004
2022-05-28 13:24:33 | INFO | train_inner | epoch 012:   6001 / 6032 loss=4.831, nll_loss=3.138, ppl=8.8, wps=36203.8, ups=12.79, wpb=2829.7, bsz=128, num_updates=72200, lr=0.000164763, gnorm=1.084, loss_scale=8, train_wall=8, gb_free=19.3, wall=6012
2022-05-28 13:24:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 13:24:54 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.317 | nll_loss 3.637 | ppl 12.44 | wps 106645 | wpb 2687.4 | bsz 128 | num_updates 72231 | best_loss 5.306
2022-05-28 13:24:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 72231 updates
2022-05-28 13:24:54 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint12.pt
2022-05-28 13:24:58 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint12.pt
2022-05-28 13:25:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint12.pt (epoch 12 @ 72231 updates, score 5.317) (writing took 6.650468243286014 seconds)
2022-05-28 13:25:01 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-05-28 13:25:01 | INFO | train | epoch 012 | loss 4.611 | nll_loss 2.883 | ppl 7.38 | wps 32790.9 | ups 12.29 | wpb 2667.6 | bsz 128 | num_updates 72231 | lr 0.000164728 | gnorm 1.058 | loss_scale 8 | train_wall 448 | gb_free 20.9 | wall 6040
2022-05-28 13:25:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 13:25:01 | INFO | fairseq.trainer | begin training epoch 13
2022-05-28 13:25:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 13:25:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 23.70 GiB total capacity; 15.64 GiB already allocated; 3.55 GiB free; 18.48 GiB reserved in total by PyTorch)
2022-05-28 13:25:06 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 147          |        cudaMalloc retries: 324       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12349 MB |   16011 MB |  442941 GB |  442929 GB |
|       from large pool |   12310 MB |   15972 MB |  433084 GB |  433072 GB |
|       from small pool |      39 MB |      73 MB |    9857 GB |    9857 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12349 MB |   16011 MB |  442941 GB |  442929 GB |
|       from large pool |   12310 MB |   15972 MB |  433084 GB |  433072 GB |
|       from small pool |      39 MB |      73 MB |    9857 GB |    9857 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18922 MB |   19076 MB |    1547 GB |    1529 GB |
|       from large pool |   18880 MB |   18880 MB |    1511 GB |    1493 GB |
|       from small pool |      42 MB |     196 MB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6572 MB |    6572 MB |  619087 GB |  619081 GB |
|       from large pool |    6569 MB |    6569 MB |  607453 GB |  607447 GB |
|       from small pool |       2 MB |      13 MB |   11634 GB |   11634 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  116514 K  |  116513 K  |
|       from large pool |     271    |     275    |   55132 K  |   55131 K  |
|       from small pool |     321    |     466    |   61382 K  |   61381 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  116514 K  |  116513 K  |
|       from large pool |     271    |     275    |   55132 K  |   55131 K  |
|       from small pool |     321    |     466    |   61382 K  |   61381 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      42    |     119    |   19217    |   19175    |
|       from large pool |      21    |      21    |     786    |     765    |
|       from small pool |      21    |      98    |   18431    |   18410    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      58    |   59382 K  |   59382 K  |
|       from large pool |      15    |      15    |   31402 K  |   31402 K  |
|       from small pool |       9    |      43    |   27979 K  |   27979 K  |
|===========================================================================|

2022-05-28 13:25:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:25:07 | INFO | train_inner | epoch 013:     70 / 6032 loss=4.425, nll_loss=2.671, ppl=6.37, wps=7458.9, ups=2.96, wpb=2520.6, bsz=128, num_updates=72300, lr=0.000164649, gnorm=1.019, loss_scale=8, train_wall=8, gb_free=18.2, wall=6046
2022-05-28 13:25:15 | INFO | train_inner | epoch 013:    170 / 6032 loss=4.589, nll_loss=2.855, ppl=7.24, wps=34936.5, ups=12.16, wpb=2873.3, bsz=128, num_updates=72400, lr=0.000164535, gnorm=1.022, loss_scale=8, train_wall=8, gb_free=18.1, wall=6054
2022-05-28 13:25:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 16.54 GiB already allocated; 171.69 MiB free; 21.86 GiB reserved in total by PyTorch)
2022-05-28 13:25:17 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 148          |        cudaMalloc retries: 325       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12953 MB |   16932 MB |  443839 GB |  443827 GB |
|       from large pool |   12914 MB |   16893 MB |  433962 GB |  433950 GB |
|       from small pool |      39 MB |      73 MB |    9877 GB |    9877 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12953 MB |   16932 MB |  443839 GB |  443827 GB |
|       from large pool |   12914 MB |   16893 MB |  433962 GB |  433950 GB |
|       from small pool |      39 MB |      73 MB |    9877 GB |    9877 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22388 MB |   22550 MB |    1551 GB |    1529 GB |
|       from large pool |   22348 MB |   22348 MB |    1514 GB |    1493 GB |
|       from small pool |      40 MB |     202 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1978 MB |    5455 MB |  620590 GB |  620588 GB |
|       from large pool |    1977 MB |    5454 MB |  608933 GB |  608931 GB |
|       from small pool |       0 MB |      14 MB |   11657 GB |   11657 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  116727 K  |  116727 K  |
|       from large pool |     271    |     275    |   55231 K  |   55231 K  |
|       from small pool |     321    |     466    |   61496 K  |   61495 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  116727 K  |  116727 K  |
|       from large pool |     271    |     275    |   55231 K  |   55231 K  |
|       from small pool |     321    |     466    |   61496 K  |   61495 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      42    |     123    |   19298    |   19256    |
|       from large pool |      22    |      22    |     787    |     765    |
|       from small pool |      20    |     101    |   18511    |   18491    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      57    |   59490 K  |   59490 K  |
|       from large pool |      15    |      16    |   31458 K  |   31458 K  |
|       from small pool |       9    |      44    |   28032 K  |   28032 K  |
|===========================================================================|

2022-05-28 13:25:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:25:23 | INFO | train_inner | epoch 013:    271 / 6032 loss=4.604, nll_loss=2.869, ppl=7.31, wps=35385.7, ups=12.04, wpb=2939.7, bsz=128, num_updates=72500, lr=0.000164422, gnorm=1.018, loss_scale=8, train_wall=8, gb_free=18.8, wall=6062
2022-05-28 13:25:31 | INFO | train_inner | epoch 013:    371 / 6032 loss=4.477, nll_loss=2.724, ppl=6.61, wps=34422.4, ups=12.88, wpb=2672.8, bsz=128, num_updates=72600, lr=0.000164308, gnorm=1.046, loss_scale=8, train_wall=8, gb_free=19.5, wall=6070
2022-05-28 13:25:39 | INFO | train_inner | epoch 013:    471 / 6032 loss=4.676, nll_loss=2.954, ppl=7.75, wps=34353.9, ups=11.92, wpb=2882.3, bsz=128, num_updates=72700, lr=0.000164195, gnorm=1.05, loss_scale=8, train_wall=8, gb_free=20.5, wall=6078
2022-05-28 13:25:47 | INFO | train_inner | epoch 013:    571 / 6032 loss=4.479, nll_loss=2.728, ppl=6.63, wps=34989.1, ups=12.62, wpb=2771.5, bsz=128, num_updates=72800, lr=0.000164083, gnorm=1.044, loss_scale=8, train_wall=8, gb_free=18.8, wall=6086
2022-05-28 13:25:55 | INFO | train_inner | epoch 013:    671 / 6032 loss=4.312, nll_loss=2.537, ppl=5.81, wps=33177.7, ups=13.18, wpb=2517.9, bsz=128, num_updates=72900, lr=0.00016397, gnorm=1.059, loss_scale=8, train_wall=7, gb_free=20.8, wall=6094
2022-05-28 13:26:03 | INFO | train_inner | epoch 013:    771 / 6032 loss=4.612, nll_loss=2.878, ppl=7.35, wps=36823.2, ups=12.6, wpb=2923.4, bsz=128, num_updates=73000, lr=0.000163858, gnorm=1.04, loss_scale=8, train_wall=8, gb_free=18.3, wall=6102
2022-05-28 13:26:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.24 GiB already allocated; 3.78 GiB free; 18.25 GiB reserved in total by PyTorch)
2022-05-28 13:26:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 149          |        cudaMalloc retries: 328       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12701 MB |   16631 MB |  447770 GB |  447758 GB |
|       from large pool |   12661 MB |   16592 MB |  437807 GB |  437795 GB |
|       from small pool |      39 MB |      73 MB |    9962 GB |    9962 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12701 MB |   16631 MB |  447770 GB |  447758 GB |
|       from large pool |   12661 MB |   16592 MB |  437807 GB |  437795 GB |
|       from small pool |      39 MB |      73 MB |    9962 GB |    9962 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18692 MB |   22006 MB |    1568 GB |    1550 GB |
|       from large pool |   18652 MB |   21804 MB |    1532 GB |    1513 GB |
|       from small pool |      40 MB |     202 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2058 MB |    3337 MB |  627264 GB |  627262 GB |
|       from large pool |    2058 MB |    3335 MB |  615505 GB |  615503 GB |
|       from small pool |       0 MB |      14 MB |   11758 GB |   11758 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  117744 K  |  117744 K  |
|       from large pool |     271    |     275    |   55710 K  |   55710 K  |
|       from small pool |     321    |     466    |   62033 K  |   62033 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  117744 K  |  117744 K  |
|       from large pool |     271    |     275    |   55710 K  |   55710 K  |
|       from small pool |     321    |     466    |   62033 K  |   62033 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      42    |     124    |   19463    |   19421    |
|       from large pool |      22    |      23    |     792    |     770    |
|       from small pool |      20    |     101    |   18671    |   18651    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      71    |   60002 K  |   60002 K  |
|       from large pool |      15    |      20    |   31724 K  |   31724 K  |
|       from small pool |       8    |      51    |   28277 K  |   28277 K  |
|===========================================================================|

2022-05-28 13:26:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:26:11 | INFO | train_inner | epoch 013:    872 / 6032 loss=4.473, nll_loss=2.724, ppl=6.61, wps=32675, ups=12.41, wpb=2632.4, bsz=128, num_updates=73100, lr=0.000163745, gnorm=1.051, loss_scale=8, train_wall=8, gb_free=19.5, wall=6110
2022-05-28 13:26:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 23.70 GiB total capacity; 16.68 GiB already allocated; 2.63 GiB free; 19.40 GiB reserved in total by PyTorch)
2022-05-28 13:26:15 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 150          |        cudaMalloc retries: 329       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   17084 MB |   17143 MB |  448197 GB |  448180 GB |
|       from large pool |   17044 MB |   17103 MB |  438225 GB |  438209 GB |
|       from small pool |      39 MB |      73 MB |    9971 GB |    9971 GB |
|---------------------------------------------------------------------------|
| Active memory         |   17084 MB |   17143 MB |  448197 GB |  448180 GB |
|       from large pool |   17044 MB |   17103 MB |  438225 GB |  438209 GB |
|       from small pool |      39 MB |      73 MB |    9971 GB |    9971 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19870 MB |   20026 MB |    1573 GB |    1554 GB |
|       from large pool |   19824 MB |   19824 MB |    1537 GB |    1517 GB |
|       from small pool |      46 MB |     202 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2785 MB |    5426 MB |  627982 GB |  627979 GB |
|       from large pool |    2779 MB |    5420 MB |  616213 GB |  616210 GB |
|       from small pool |       6 MB |      18 MB |   11769 GB |   11769 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  117856 K  |  117855 K  |
|       from large pool |     271    |     275    |   55763 K  |   55763 K  |
|       from small pool |     321    |     466    |   62092 K  |   62091 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  117856 K  |  117855 K  |
|       from large pool |     271    |     275    |   55763 K  |   55763 K  |
|       from small pool |     321    |     466    |   62092 K  |   62091 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     123    |   19545    |   19500    |
|       from large pool |      22    |      22    |     793    |     771    |
|       from small pool |      23    |     101    |   18752    |   18729    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      67    |   60058 K  |   60058 K  |
|       from large pool |      15    |      15    |   31754 K  |   31754 K  |
|       from small pool |      12    |      54    |   28304 K  |   28304 K  |
|===========================================================================|

2022-05-28 13:26:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:26:19 | INFO | train_inner | epoch 013:    973 / 6032 loss=4.406, nll_loss=2.645, ppl=6.26, wps=32560.5, ups=12.59, wpb=2586.7, bsz=128, num_updates=73200, lr=0.000163634, gnorm=1.075, loss_scale=8, train_wall=8, gb_free=20.3, wall=6118
2022-05-28 13:26:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 23.70 GiB total capacity; 13.14 GiB already allocated; 2.63 GiB free; 19.40 GiB reserved in total by PyTorch)
2022-05-28 13:26:23 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 151          |        cudaMalloc retries: 330       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10425 MB |   13452 MB |  448802 GB |  448792 GB |
|       from large pool |   10386 MB |   13413 MB |  438817 GB |  438807 GB |
|       from small pool |      38 MB |      73 MB |    9985 GB |    9985 GB |
|---------------------------------------------------------------------------|
| Active memory         |   10425 MB |   13452 MB |  448802 GB |  448792 GB |
|       from large pool |   10386 MB |   13413 MB |  438817 GB |  438807 GB |
|       from small pool |      38 MB |      73 MB |    9985 GB |    9985 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19868 MB |   20026 MB |    1573 GB |    1554 GB |
|       from large pool |   19824 MB |   19824 MB |    1537 GB |    1517 GB |
|       from small pool |      44 MB |     202 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4022 MB |    6415 MB |  628989 GB |  628985 GB |
|       from large pool |    4017 MB |    6410 MB |  617204 GB |  617200 GB |
|       from small pool |       5 MB |      44 MB |   11785 GB |   11785 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  118008 K  |  118008 K  |
|       from large pool |     271    |     275    |   55834 K  |   55834 K  |
|       from small pool |     321    |     466    |   62174 K  |   62173 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  118008 K  |  118008 K  |
|       from large pool |     271    |     275    |   55834 K  |   55834 K  |
|       from small pool |     321    |     466    |   62174 K  |   62173 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      44    |     123    |   19623    |   19579    |
|       from large pool |      22    |      22    |     793    |     771    |
|       from small pool |      22    |     101    |   18830    |   18808    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      64    |   60135 K  |   60135 K  |
|       from large pool |      15    |      16    |   31793 K  |   31793 K  |
|       from small pool |      11    |      56    |   28342 K  |   28342 K  |
|===========================================================================|

2022-05-28 13:26:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:26:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 3.46 GiB free; 18.57 GiB reserved in total by PyTorch)
2022-05-28 13:26:25 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 152          |        cudaMalloc retries: 331       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15207 MB |   15259 MB |  449012 GB |  448997 GB |
|       from large pool |   15167 MB |   15219 MB |  439021 GB |  439006 GB |
|       from small pool |      39 MB |      73 MB |    9990 GB |    9990 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15207 MB |   15259 MB |  449012 GB |  448997 GB |
|       from large pool |   15167 MB |   15219 MB |  439021 GB |  439006 GB |
|       from small pool |      39 MB |      73 MB |    9990 GB |    9990 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19012 MB |   19172 MB |    1578 GB |    1559 GB |
|       from large pool |   18970 MB |   18970 MB |    1541 GB |    1523 GB |
|       from small pool |      42 MB |     202 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3804 MB |    3804 MB |  629332 GB |  629328 GB |
|       from large pool |    3802 MB |    3802 MB |  617540 GB |  617537 GB |
|       from small pool |       2 MB |      21 MB |   11791 GB |   11791 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  118057 K  |  118056 K  |
|       from large pool |     271    |     275    |   55856 K  |   55856 K  |
|       from small pool |     321    |     466    |   62200 K  |   62200 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  118057 K  |  118056 K  |
|       from large pool |     271    |     275    |   55856 K  |   55856 K  |
|       from small pool |     321    |     466    |   62200 K  |   62200 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      43    |     123    |   19703    |   19660    |
|       from large pool |      22    |      22    |     794    |     772    |
|       from small pool |      21    |     101    |   18909    |   18888    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      64    |   60160 K  |   60160 K  |
|       from large pool |      13    |      13    |   31805 K  |   31805 K  |
|       from small pool |      11    |      55    |   28354 K  |   28354 K  |
|===========================================================================|

2022-05-28 13:26:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:26:27 | INFO | train_inner | epoch 013:   1075 / 6032 loss=4.551, nll_loss=2.811, ppl=7.02, wps=31537.6, ups=12.26, wpb=2572.8, bsz=128, num_updates=73300, lr=0.000163522, gnorm=1.063, loss_scale=8, train_wall=8, gb_free=21, wall=6126
2022-05-28 13:26:35 | INFO | train_inner | epoch 013:   1175 / 6032 loss=4.674, nll_loss=2.95, ppl=7.73, wps=34716.9, ups=11.95, wpb=2905.2, bsz=128, num_updates=73400, lr=0.000163411, gnorm=1.058, loss_scale=8, train_wall=8, gb_free=18.9, wall=6134
2022-05-28 13:26:44 | INFO | train_inner | epoch 013:   1275 / 6032 loss=4.559, nll_loss=2.82, ppl=7.06, wps=32194.2, ups=11.9, wpb=2704.9, bsz=128, num_updates=73500, lr=0.000163299, gnorm=1.08, loss_scale=8, train_wall=8, gb_free=16.9, wall=6143
2022-05-28 13:26:51 | INFO | train_inner | epoch 013:   1375 / 6032 loss=4.373, nll_loss=2.606, ppl=6.09, wps=31950.5, ups=13.07, wpb=2444.7, bsz=128, num_updates=73600, lr=0.000163188, gnorm=1.078, loss_scale=8, train_wall=7, gb_free=20.6, wall=6150
2022-05-28 13:26:59 | INFO | train_inner | epoch 013:   1475 / 6032 loss=4.354, nll_loss=2.586, ppl=6.01, wps=30232.2, ups=12.91, wpb=2342, bsz=128, num_updates=73700, lr=0.000163078, gnorm=1.115, loss_scale=8, train_wall=8, gb_free=19.9, wall=6158
2022-05-28 13:27:07 | INFO | train_inner | epoch 013:   1575 / 6032 loss=4.464, nll_loss=2.711, ppl=6.55, wps=33058.6, ups=13.29, wpb=2487.8, bsz=128, num_updates=73800, lr=0.000162967, gnorm=1.081, loss_scale=8, train_wall=7, gb_free=20.5, wall=6166
2022-05-28 13:27:14 | INFO | train_inner | epoch 013:   1675 / 6032 loss=4.403, nll_loss=2.642, ppl=6.24, wps=32846.9, ups=13.19, wpb=2490.9, bsz=128, num_updates=73900, lr=0.000162857, gnorm=1.081, loss_scale=8, train_wall=7, gb_free=19.8, wall=6173
2022-05-28 13:27:22 | INFO | train_inner | epoch 013:   1775 / 6032 loss=4.569, nll_loss=2.834, ppl=7.13, wps=35572.8, ups=12.66, wpb=2808.9, bsz=128, num_updates=74000, lr=0.000162747, gnorm=1.071, loss_scale=8, train_wall=8, gb_free=18.1, wall=6181
2022-05-28 13:27:30 | INFO | train_inner | epoch 013:   1875 / 6032 loss=4.537, nll_loss=2.795, ppl=6.94, wps=32585, ups=12.21, wpb=2667.8, bsz=128, num_updates=74100, lr=0.000162637, gnorm=1.083, loss_scale=8, train_wall=8, gb_free=19.2, wall=6189
2022-05-28 13:27:38 | INFO | train_inner | epoch 013:   1975 / 6032 loss=4.487, nll_loss=2.739, ppl=6.67, wps=32816.5, ups=12.98, wpb=2529, bsz=128, num_updates=74200, lr=0.000162527, gnorm=1.115, loss_scale=8, train_wall=8, gb_free=20.3, wall=6197
2022-05-28 13:27:46 | INFO | train_inner | epoch 013:   2075 / 6032 loss=4.589, nll_loss=2.855, ppl=7.24, wps=34993.2, ups=12.48, wpb=2803.2, bsz=128, num_updates=74300, lr=0.000162418, gnorm=1.07, loss_scale=8, train_wall=8, gb_free=19.1, wall=6205
2022-05-28 13:27:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 17.85 GiB already allocated; 3.41 GiB free; 18.62 GiB reserved in total by PyTorch)
2022-05-28 13:27:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 153          |        cudaMalloc retries: 338       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13956 MB |   18278 MB |  455559 GB |  455545 GB |
|       from large pool |   13917 MB |   18239 MB |  445421 GB |  445408 GB |
|       from small pool |      39 MB |      73 MB |   10137 GB |   10137 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13956 MB |   18278 MB |  455559 GB |  455545 GB |
|       from large pool |   13917 MB |   18239 MB |  445421 GB |  445408 GB |
|       from small pool |      39 MB |      73 MB |   10137 GB |   10137 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19066 MB |   21054 MB |    1632 GB |    1614 GB |
|       from large pool |   19020 MB |   20852 MB |    1595 GB |    1576 GB |
|       from small pool |      46 MB |     202 MB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     787 MB |    3471 MB |  636332 GB |  636331 GB |
|       from large pool |     780 MB |    3464 MB |  624366 GB |  624365 GB |
|       from small pool |       6 MB |      18 MB |   11966 GB |   11966 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  119801 K  |  119801 K  |
|       from large pool |     271    |     275    |   56677 K  |   56677 K  |
|       from small pool |     321    |     466    |   63124 K  |   63124 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  119801 K  |  119801 K  |
|       from large pool |     271    |     275    |   56677 K  |   56677 K  |
|       from small pool |     321    |     466    |   63124 K  |   63124 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     130    |   20122    |   20071    |
|       from large pool |      28    |      29    |     825    |     797    |
|       from small pool |      23    |     101    |   19297    |   19274    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      57    |   61045 K  |   61045 K  |
|       from large pool |      21    |      21    |   32268 K  |   32268 K  |
|       from small pool |      12    |      44    |   28777 K  |   28777 K  |
|===========================================================================|

2022-05-28 13:27:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:27:55 | INFO | train_inner | epoch 013:   2176 / 6032 loss=4.673, nll_loss=2.951, ppl=7.73, wps=33852.4, ups=11.76, wpb=2879, bsz=128, num_updates=74400, lr=0.000162309, gnorm=1.08, loss_scale=8, train_wall=8, gb_free=5.8, wall=6214
2022-05-28 13:28:02 | INFO | train_inner | epoch 013:   2276 / 6032 loss=4.288, nll_loss=2.511, ppl=5.7, wps=32501.1, ups=13.44, wpb=2418.8, bsz=128, num_updates=74500, lr=0.0001622, gnorm=1.083, loss_scale=8, train_wall=7, gb_free=20.6, wall=6221
2022-05-28 13:28:10 | INFO | train_inner | epoch 013:   2376 / 6032 loss=4.693, nll_loss=2.975, ppl=7.86, wps=33036.7, ups=12.53, wpb=2635.8, bsz=128, num_updates=74600, lr=0.000162091, gnorm=1.128, loss_scale=8, train_wall=8, gb_free=20.7, wall=6229
2022-05-28 13:28:18 | INFO | train_inner | epoch 013:   2476 / 6032 loss=4.29, nll_loss=2.515, ppl=5.71, wps=28679.4, ups=13.01, wpb=2204.6, bsz=128, num_updates=74700, lr=0.000161982, gnorm=1.109, loss_scale=8, train_wall=7, gb_free=20.3, wall=6237
2022-05-28 13:28:25 | INFO | train_inner | epoch 013:   2576 / 6032 loss=4.45, nll_loss=2.697, ppl=6.48, wps=31636.2, ups=13.34, wpb=2371.5, bsz=128, num_updates=74800, lr=0.000161874, gnorm=1.142, loss_scale=8, train_wall=7, gb_free=15.5, wall=6244
2022-05-28 13:28:33 | INFO | train_inner | epoch 013:   2676 / 6032 loss=4.506, nll_loss=2.761, ppl=6.78, wps=33536.7, ups=12.8, wpb=2620.4, bsz=128, num_updates=74900, lr=0.000161766, gnorm=1.095, loss_scale=8, train_wall=8, gb_free=20.9, wall=6252
2022-05-28 13:28:41 | INFO | train_inner | epoch 013:   2776 / 6032 loss=4.455, nll_loss=2.703, ppl=6.51, wps=33627.6, ups=12.65, wpb=2658.9, bsz=128, num_updates=75000, lr=0.000161658, gnorm=1.062, loss_scale=8, train_wall=8, gb_free=20.3, wall=6260
2022-05-28 13:28:49 | INFO | train_inner | epoch 013:   2876 / 6032 loss=4.491, nll_loss=2.744, ppl=6.7, wps=35053.2, ups=13.04, wpb=2688.7, bsz=128, num_updates=75100, lr=0.00016155, gnorm=1.064, loss_scale=8, train_wall=7, gb_free=19.8, wall=6268
2022-05-28 13:28:56 | INFO | train_inner | epoch 013:   2976 / 6032 loss=4.589, nll_loss=2.855, ppl=7.23, wps=34168.4, ups=12.57, wpb=2717.7, bsz=128, num_updates=75200, lr=0.000161443, gnorm=1.081, loss_scale=8, train_wall=8, gb_free=18.2, wall=6276
2022-05-28 13:29:04 | INFO | train_inner | epoch 013:   3076 / 6032 loss=4.484, nll_loss=2.737, ppl=6.67, wps=32120.8, ups=12.87, wpb=2496.3, bsz=128, num_updates=75300, lr=0.000161336, gnorm=1.096, loss_scale=8, train_wall=8, gb_free=18.3, wall=6283
2022-05-28 13:29:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 23.70 GiB total capacity; 17.50 GiB already allocated; 3.63 GiB free; 18.40 GiB reserved in total by PyTorch)
2022-05-28 13:29:10 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 154          |        cudaMalloc retries: 339       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13822 MB |   17925 MB |  460939 GB |  460925 GB |
|       from large pool |   13783 MB |   17885 MB |  450668 GB |  450654 GB |
|       from small pool |      39 MB |      73 MB |   10271 GB |   10271 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13822 MB |   17925 MB |  460939 GB |  460925 GB |
|       from large pool |   13783 MB |   17885 MB |  450668 GB |  450654 GB |
|       from small pool |      39 MB |      73 MB |   10271 GB |   10271 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   18846 MB |   19004 MB |    1636 GB |    1618 GB |
|       from large pool |   18800 MB |   18800 MB |    1599 GB |    1580 GB |
|       from small pool |      46 MB |     204 MB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |     921 MB |    3406 MB |  641804 GB |  641803 GB |
|       from large pool |     914 MB |    3399 MB |  629681 GB |  629680 GB |
|       from small pool |       6 MB |      12 MB |   12123 GB |   12123 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  121315 K  |  121314 K  |
|       from large pool |     271    |     275    |   57378 K  |   57378 K  |
|       from small pool |     321    |     466    |   63936 K  |   63935 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  121315 K  |  121314 K  |
|       from large pool |     271    |     275    |   57378 K  |   57378 K  |
|       from small pool |     321    |     466    |   63936 K  |   63935 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |     130    |   20202    |   20151    |
|       from large pool |      28    |      28    |     826    |     798    |
|       from small pool |      23    |     102    |   19376    |   19353    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      31    |      40    |   61812 K  |   61812 K  |
|       from large pool |      19    |      19    |   32664 K  |   32664 K  |
|       from small pool |      12    |      27    |   29148 K  |   29148 K  |
|===========================================================================|

2022-05-28 13:29:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:29:12 | INFO | train_inner | epoch 013:   3177 / 6032 loss=4.328, nll_loss=2.559, ppl=5.89, wps=31914.5, ups=13.11, wpb=2435, bsz=128, num_updates=75400, lr=0.000161229, gnorm=1.107, loss_scale=8, train_wall=7, gb_free=19.5, wall=6291
2022-05-28 13:29:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 23.70 GiB total capacity; 17.98 GiB already allocated; 3.32 GiB free; 18.71 GiB reserved in total by PyTorch)
2022-05-28 13:29:13 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 155          |        cudaMalloc retries: 341       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   14036 MB |   18407 MB |  461195 GB |  461181 GB |
|       from large pool |   13997 MB |   18367 MB |  450918 GB |  450904 GB |
|       from small pool |      39 MB |      73 MB |   10276 GB |   10276 GB |
|---------------------------------------------------------------------------|
| Active memory         |   14036 MB |   18407 MB |  461195 GB |  461181 GB |
|       from large pool |   13997 MB |   18367 MB |  450918 GB |  450904 GB |
|       from small pool |      39 MB |      73 MB |   10276 GB |   10276 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   19162 MB |   19230 MB |    1645 GB |    1626 GB |
|       from large pool |   19120 MB |   19120 MB |    1607 GB |    1589 GB |
|       from small pool |      42 MB |     160 MB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  771136 KB |    3378 MB |  642050 GB |  642049 GB |
|       from large pool |  768392 KB |    3375 MB |  629919 GB |  629919 GB |
|       from small pool |    2744 KB |      10 MB |   12130 GB |   12130 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  121389 K  |  121388 K  |
|       from large pool |     271    |     275    |   57414 K  |   57414 K  |
|       from small pool |     321    |     466    |   63974 K  |   63974 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  121389 K  |  121388 K  |
|       from large pool |     271    |     275    |   57414 K  |   57414 K  |
|       from small pool |     321    |     466    |   63974 K  |   63974 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     108    |   20261    |   20212    |
|       from large pool |      28    |      28    |     828    |     800    |
|       from small pool |      21    |      80    |   19433    |   19412    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      38    |   61850 K  |   61850 K  |
|       from large pool |      17    |      18    |   32684 K  |   32684 K  |
|       from small pool |       8    |      24    |   29165 K  |   29165 K  |
|===========================================================================|

2022-05-28 13:29:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:29:20 | INFO | train_inner | epoch 013:   3278 / 6032 loss=4.648, nll_loss=2.923, ppl=7.59, wps=34647.3, ups=12.27, wpb=2824.8, bsz=126.8, num_updates=75500, lr=0.000161122, gnorm=1.156, loss_scale=8, train_wall=8, gb_free=17.1, wall=6299
2022-05-28 13:29:27 | INFO | train_inner | epoch 013:   3378 / 6032 loss=4.377, nll_loss=2.614, ppl=6.12, wps=32955.5, ups=13.38, wpb=2462.5, bsz=128, num_updates=75600, lr=0.000161015, gnorm=1.101, loss_scale=8, train_wall=7, gb_free=19.1, wall=6307
2022-05-28 13:29:36 | INFO | train_inner | epoch 013:   3478 / 6032 loss=4.576, nll_loss=2.843, ppl=7.18, wps=34996.3, ups=12.34, wpb=2836.1, bsz=128, num_updates=75700, lr=0.000160909, gnorm=1.063, loss_scale=8, train_wall=8, gb_free=19, wall=6315
2022-05-28 13:29:44 | INFO | train_inner | epoch 013:   3578 / 6032 loss=4.643, nll_loss=2.919, ppl=7.56, wps=35582.7, ups=12.39, wpb=2872.2, bsz=128, num_updates=75800, lr=0.000160803, gnorm=1.049, loss_scale=8, train_wall=8, gb_free=20.5, wall=6323
2022-05-28 13:29:52 | INFO | train_inner | epoch 013:   3678 / 6032 loss=4.744, nll_loss=3.036, ppl=8.2, wps=35817.8, ups=12.1, wpb=2959.1, bsz=128, num_updates=75900, lr=0.000160697, gnorm=1.065, loss_scale=8, train_wall=8, gb_free=19.5, wall=6331
2022-05-28 13:30:00 | INFO | train_inner | epoch 013:   3778 / 6032 loss=4.725, nll_loss=3.012, ppl=8.07, wps=36272.7, ups=12.57, wpb=2885.8, bsz=128, num_updates=76000, lr=0.000160591, gnorm=1.082, loss_scale=8, train_wall=8, gb_free=19.4, wall=6339
2022-05-28 13:30:08 | INFO | train_inner | epoch 013:   3878 / 6032 loss=4.595, nll_loss=2.864, ppl=7.28, wps=33836.4, ups=12.67, wpb=2670.7, bsz=128, num_updates=76100, lr=0.000160485, gnorm=1.112, loss_scale=8, train_wall=8, gb_free=20, wall=6347
2022-05-28 13:30:15 | INFO | train_inner | epoch 013:   3978 / 6032 loss=4.548, nll_loss=2.81, ppl=7.01, wps=33681.9, ups=13.39, wpb=2514.8, bsz=128, num_updates=76200, lr=0.00016038, gnorm=1.101, loss_scale=8, train_wall=7, gb_free=20.1, wall=6354
2022-05-28 13:30:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 17.15 GiB already allocated; 1.22 GiB free; 20.81 GiB reserved in total by PyTorch)
2022-05-28 13:30:20 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 156          |        cudaMalloc retries: 344       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13632 MB |   17563 MB |  466344 GB |  466331 GB |
|       from large pool |   13593 MB |   17523 MB |  455957 GB |  455944 GB |
|       from small pool |      39 MB |      73 MB |   10386 GB |   10386 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13632 MB |   17563 MB |  466344 GB |  466331 GB |
|       from large pool |   13593 MB |   17523 MB |  455957 GB |  455944 GB |
|       from small pool |      39 MB |      73 MB |   10386 GB |   10386 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21312 MB |   21312 MB |    1663 GB |    1642 GB |
|       from large pool |   21268 MB |   21268 MB |    1624 GB |    1603 GB |
|       from small pool |      44 MB |     202 MB |      38 GB |      38 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    3747 MB |    3748 MB |  647350 GB |  647346 GB |
|       from large pool |    3742 MB |    3744 MB |  635090 GB |  635086 GB |
|       from small pool |       4 MB |      17 MB |   12260 GB |   12260 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  122702 K  |  122702 K  |
|       from large pool |     271    |     275    |   58038 K  |   58037 K  |
|       from small pool |     321    |     466    |   64664 K  |   64664 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  122702 K  |  122702 K  |
|       from large pool |     271    |     275    |   58038 K  |   58037 K  |
|       from small pool |     321    |     466    |   64664 K  |   64664 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     129    |   20427    |   20377    |
|       from large pool |      28    |      28    |     833    |     805    |
|       from small pool |      22    |     101    |   19594    |   19572    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      37    |      60    |   62516 K  |   62516 K  |
|       from large pool |      22    |      23    |   33033 K  |   33033 K  |
|       from small pool |      15    |      45    |   29482 K  |   29482 K  |
|===========================================================================|

2022-05-28 13:30:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:30:23 | INFO | train_inner | epoch 013:   4079 / 6032 loss=4.699, nll_loss=2.984, ppl=7.91, wps=36018.2, ups=12.42, wpb=2899.2, bsz=128, num_updates=76300, lr=0.000160275, gnorm=1.065, loss_scale=8, train_wall=8, gb_free=20.3, wall=6362
2022-05-28 13:30:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 1.22 GiB free; 20.81 GiB reserved in total by PyTorch)
2022-05-28 13:30:30 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 157          |        cudaMalloc retries: 345       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13289 MB |   17219 MB |  467111 GB |  467098 GB |
|       from large pool |   13249 MB |   17180 MB |  456710 GB |  456697 GB |
|       from small pool |      39 MB |      73 MB |   10401 GB |   10401 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13289 MB |   17219 MB |  467111 GB |  467098 GB |
|       from large pool |   13249 MB |   17180 MB |  456710 GB |  456697 GB |
|       from small pool |      39 MB |      73 MB |   10401 GB |   10401 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   21310 MB |   21466 MB |    1667 GB |    1646 GB |
|       from large pool |   21268 MB |   21268 MB |    1628 GB |    1607 GB |
|       from small pool |      42 MB |     198 MB |      38 GB |      38 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4088 MB |    4090 MB |  648166 GB |  648162 GB |
|       from large pool |    4086 MB |    4087 MB |  635888 GB |  635884 GB |
|       from small pool |       2 MB |      13 MB |   12277 GB |   12277 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  122884 K  |  122883 K  |
|       from large pool |     271    |     275    |   58124 K  |   58124 K  |
|       from small pool |     321    |     466    |   64759 K  |   64758 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  122884 K  |  122883 K  |
|       from large pool |     271    |     275    |   58124 K  |   58124 K  |
|       from small pool |     321    |     466    |   64759 K  |   64758 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     127    |   20505    |   20456    |
|       from large pool |      28    |      28    |     834    |     806    |
|       from small pool |      21    |      99    |   19671    |   19650    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      57    |   62608 K  |   62607 K  |
|       from large pool |      19    |      20    |   33081 K  |   33081 K  |
|       from small pool |      10    |      40    |   29526 K  |   29526 K  |
|===========================================================================|

2022-05-28 13:30:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:30:31 | INFO | train_inner | epoch 013:   4180 / 6032 loss=4.68, nll_loss=2.963, ppl=7.8, wps=37122.2, ups=12.34, wpb=3008.2, bsz=128, num_updates=76400, lr=0.00016017, gnorm=1.07, loss_scale=8, train_wall=8, gb_free=17.8, wall=6371
2022-05-28 13:30:39 | INFO | train_inner | epoch 013:   4280 / 6032 loss=4.45, nll_loss=2.698, ppl=6.49, wps=33577.5, ups=13.62, wpb=2465, bsz=128, num_updates=76500, lr=0.000160065, gnorm=1.097, loss_scale=8, train_wall=7, gb_free=20.6, wall=6378
2022-05-28 13:30:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.70 GiB total capacity; 12.94 GiB already allocated; 5.09 GiB free; 16.94 GiB reserved in total by PyTorch)
2022-05-28 13:30:43 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 158          |        cudaMalloc retries: 346       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13255 MB |   13286 MB |  468080 GB |  468067 GB |
|       from large pool |   13214 MB |   13246 MB |  457655 GB |  457642 GB |
|       from small pool |      40 MB |      73 MB |   10425 GB |   10425 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13255 MB |   13286 MB |  468080 GB |  468067 GB |
|       from large pool |   13214 MB |   13246 MB |  457655 GB |  457642 GB |
|       from small pool |      40 MB |      73 MB |   10425 GB |   10425 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   17350 MB |   17538 MB |    1667 GB |    1650 GB |
|       from large pool |   17302 MB |   17336 MB |    1628 GB |    1611 GB |
|       from small pool |      48 MB |     202 MB |      38 GB |      38 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4094 MB |    4112 MB |  649145 GB |  649141 GB |
|       from large pool |    4087 MB |    4105 MB |  636839 GB |  636835 GB |
|       from small pool |       7 MB |      17 MB |   12305 GB |   12305 GB |
|---------------------------------------------------------------------------|
| Allocations           |     590    |     593    |  123155 K  |  123154 K  |
|       from large pool |     273    |     275    |   58251 K  |   58251 K  |
|       from small pool |     317    |     466    |   64904 K  |   64903 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     590    |     593    |  123155 K  |  123154 K  |
|       from large pool |     273    |     275    |   58251 K  |   58251 K  |
|       from small pool |     317    |     466    |   64904 K  |   64903 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     128    |   20585    |   20535    |
|       from large pool |      26    |      27    |     834    |     808    |
|       from small pool |      24    |     101    |   19751    |   19727    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      51    |   62745 K  |   62745 K  |
|       from large pool |      16    |      19    |   33152 K  |   33152 K  |
|       from small pool |      14    |      35    |   29592 K  |   29592 K  |
|===========================================================================|

2022-05-28 13:30:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:30:47 | INFO | train_inner | epoch 013:   4381 / 6032 loss=4.683, nll_loss=2.966, ppl=7.81, wps=34947.6, ups=12.84, wpb=2721.7, bsz=128, num_updates=76600, lr=0.000159961, gnorm=1.105, loss_scale=8, train_wall=7, gb_free=18.1, wall=6386
2022-05-28 13:30:54 | INFO | train_inner | epoch 013:   4481 / 6032 loss=4.439, nll_loss=2.686, ppl=6.44, wps=33652.8, ups=13.28, wpb=2533.4, bsz=128, num_updates=76700, lr=0.000159857, gnorm=1.095, loss_scale=8, train_wall=7, gb_free=19.8, wall=6393
2022-05-28 13:30:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 441.69 MiB free; 21.60 GiB reserved in total by PyTorch)
2022-05-28 13:30:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 159          |        cudaMalloc retries: 348       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21958 MB |   22258 MB |  469149 GB |  469128 GB |
|       from large pool |   21915 MB |   22215 MB |  458701 GB |  458680 GB |
|       from small pool |      42 MB |      73 MB |   10448 GB |   10447 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21958 MB |   22258 MB |  469149 GB |  469128 GB |
|       from large pool |   21915 MB |   22215 MB |  458701 GB |  458680 GB |
|       from small pool |      42 MB |      73 MB |   10448 GB |   10447 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22118 MB |   22418 MB |    1672 GB |    1650 GB |
|       from large pool |   22074 MB |   22374 MB |    1633 GB |    1611 GB |
|       from small pool |      44 MB |     202 MB |      38 GB |      38 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  163625 KB |    4380 MB |  650234 GB |  650234 GB |
|       from large pool |  162208 KB |    4379 MB |  637901 GB |  637901 GB |
|       from small pool |    1417 KB |      14 MB |   12332 GB |   12332 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |  123423 K  |  123423 K  |
|       from large pool |     214    |     215    |   58378 K  |   58378 K  |
|       from small pool |     302    |     466    |   65045 K  |   65045 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |  123423 K  |  123423 K  |
|       from large pool |     214    |     215    |   58378 K  |   58378 K  |
|       from small pool |     302    |     466    |   65045 K  |   65045 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      63    |     142    |   20678    |   20615    |
|       from large pool |      41    |      42    |     850    |     809    |
|       from small pool |      22    |     101    |   19828    |   19806    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      59    |   62881 K  |   62881 K  |
|       from large pool |      25    |      25    |   33223 K  |   33223 K  |
|       from small pool |      15    |      46    |   29658 K  |   29658 K  |
|===========================================================================|

2022-05-28 13:30:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:31:02 | INFO | train_inner | epoch 013:   4582 / 6032 loss=4.703, nll_loss=2.988, ppl=7.93, wps=36164.2, ups=12.51, wpb=2890.4, bsz=128, num_updates=76800, lr=0.000159752, gnorm=1.075, loss_scale=8, train_wall=8, gb_free=20.4, wall=6401
2022-05-28 13:31:10 | INFO | train_inner | epoch 013:   4682 / 6032 loss=4.49, nll_loss=2.745, ppl=6.7, wps=33902.9, ups=13.35, wpb=2539.4, bsz=128, num_updates=76900, lr=0.000159649, gnorm=1.108, loss_scale=8, train_wall=7, gb_free=20.2, wall=6409
2022-05-28 13:31:17 | INFO | train_inner | epoch 013:   4782 / 6032 loss=4.706, nll_loss=2.994, ppl=7.96, wps=36480.1, ups=12.99, wpb=2808.6, bsz=128, num_updates=77000, lr=0.000159545, gnorm=1.083, loss_scale=8, train_wall=8, gb_free=19, wall=6416
2022-05-28 13:31:25 | INFO | train_inner | epoch 013:   4882 / 6032 loss=4.634, nll_loss=2.913, ppl=7.53, wps=36919.3, ups=12.69, wpb=2910.4, bsz=128, num_updates=77100, lr=0.000159441, gnorm=1.047, loss_scale=8, train_wall=8, gb_free=18.1, wall=6424
2022-05-28 13:31:33 | INFO | train_inner | epoch 013:   4982 / 6032 loss=4.523, nll_loss=2.784, ppl=6.89, wps=34990.8, ups=13.22, wpb=2646, bsz=128, num_updates=77200, lr=0.000159338, gnorm=1.08, loss_scale=8, train_wall=7, gb_free=20.2, wall=6432
2022-05-28 13:31:40 | INFO | train_inner | epoch 013:   5082 / 6032 loss=4.584, nll_loss=2.851, ppl=7.22, wps=32171, ups=12.95, wpb=2485.1, bsz=128, num_updates=77300, lr=0.000159235, gnorm=1.124, loss_scale=8, train_wall=8, gb_free=20.3, wall=6440
2022-05-28 13:31:48 | INFO | train_inner | epoch 013:   5182 / 6032 loss=4.716, nll_loss=3.004, ppl=8.02, wps=36684, ups=12.84, wpb=2856, bsz=128, num_updates=77400, lr=0.000159132, gnorm=1.09, loss_scale=8, train_wall=8, gb_free=20, wall=6447
2022-05-28 13:31:56 | INFO | train_inner | epoch 013:   5282 / 6032 loss=4.548, nll_loss=2.812, ppl=7.02, wps=34192.6, ups=13.41, wpb=2549.2, bsz=128, num_updates=77500, lr=0.000159029, gnorm=1.106, loss_scale=8, train_wall=7, gb_free=19.3, wall=6455
2022-05-28 13:32:04 | INFO | train_inner | epoch 013:   5382 / 6032 loss=4.626, nll_loss=2.901, ppl=7.47, wps=33872.8, ups=12.44, wpb=2722.7, bsz=128, num_updates=77600, lr=0.000158927, gnorm=1.103, loss_scale=8, train_wall=8, gb_free=19.4, wall=6463
2022-05-28 13:32:11 | INFO | train_inner | epoch 013:   5482 / 6032 loss=4.447, nll_loss=2.696, ppl=6.48, wps=31325, ups=13.5, wpb=2319.9, bsz=128, num_updates=77700, lr=0.000158825, gnorm=1.127, loss_scale=8, train_wall=7, gb_free=20.5, wall=6470
2022-05-28 13:32:19 | INFO | train_inner | epoch 013:   5582 / 6032 loss=4.553, nll_loss=2.818, ppl=7.05, wps=34101.9, ups=13.04, wpb=2615.9, bsz=128, num_updates=77800, lr=0.000158722, gnorm=1.101, loss_scale=8, train_wall=7, gb_free=20.7, wall=6478
2022-05-28 13:32:27 | INFO | train_inner | epoch 013:   5682 / 6032 loss=4.59, nll_loss=2.86, ppl=7.26, wps=34325.2, ups=12.83, wpb=2675.7, bsz=128, num_updates=77900, lr=0.00015862, gnorm=1.113, loss_scale=8, train_wall=8, gb_free=20.7, wall=6486
2022-05-28 13:32:35 | INFO | train_inner | epoch 013:   5782 / 6032 loss=4.707, nll_loss=2.996, ppl=7.98, wps=36432.8, ups=12.53, wpb=2906.8, bsz=128, num_updates=78000, lr=0.000158519, gnorm=1.057, loss_scale=8, train_wall=8, gb_free=20.9, wall=6494
2022-05-28 13:32:42 | INFO | train_inner | epoch 013:   5882 / 6032 loss=4.613, nll_loss=2.889, ppl=7.41, wps=35016.3, ups=12.97, wpb=2700.2, bsz=128, num_updates=78100, lr=0.000158417, gnorm=1.084, loss_scale=8, train_wall=7, gb_free=20.6, wall=6501
2022-05-28 13:32:51 | INFO | train_inner | epoch 013:   5982 / 6032 loss=4.61, nll_loss=2.883, ppl=7.38, wps=33335.4, ups=12.09, wpb=2757, bsz=128, num_updates=78200, lr=0.000158316, gnorm=1.064, loss_scale=8, train_wall=8, gb_free=18.6, wall=6510
2022-05-28 13:32:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-05-28 13:33:14 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.324 | nll_loss 3.642 | ppl 12.48 | wps 105887 | wpb 2687.4 | bsz 128 | num_updates 78250 | best_loss 5.306
2022-05-28 13:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 78250 updates
2022-05-28 13:33:14 | INFO | fairseq.trainer | Saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint13.pt
2022-05-28 13:33:17 | INFO | fairseq.trainer | Finished saving checkpoint to /user-data/5_25/checkpoints/5_25_tokenized/transformer/checkpoint13.pt
2022-05-28 13:33:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/5_25_tokenized/transformer/checkpoint13.pt (epoch 13 @ 78250 updates, score 5.324) (writing took 6.788893310818821 seconds)
2022-05-28 13:33:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-05-28 13:33:21 | INFO | train | epoch 013 | loss 4.554 | nll_loss 2.816 | ppl 7.04 | wps 32196.1 | ups 12.04 | wpb 2673.2 | bsz 128 | num_updates 78250 | lr 0.000158265 | gnorm 1.082 | loss_scale 8 | train_wall 459 | gb_free 19.5 | wall 6540
2022-05-28 13:33:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6032
2022-05-28 13:33:21 | INFO | fairseq.trainer | begin training epoch 14
2022-05-28 13:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-05-28 13:33:25 | INFO | train_inner | epoch 014:     50 / 6032 loss=4.544, nll_loss=2.805, ppl=6.99, wps=7986.4, ups=2.92, wpb=2731.8, bsz=128, num_updates=78300, lr=0.000158215, gnorm=1.08, loss_scale=8, train_wall=8, gb_free=18.4, wall=6544
2022-05-28 13:33:32 | INFO | train_inner | epoch 014:    150 / 6032 loss=4.366, nll_loss=2.597, ppl=6.05, wps=32927.3, ups=12.97, wpb=2539, bsz=128, num_updates=78400, lr=0.000158114, gnorm=1.037, loss_scale=8, train_wall=8, gb_free=18.5, wall=6552
2022-05-28 13:33:41 | INFO | train_inner | epoch 014:    250 / 6032 loss=4.578, nll_loss=2.84, ppl=7.16, wps=36644.2, ups=12.39, wpb=2957.7, bsz=128, num_updates=78500, lr=0.000158013, gnorm=1.042, loss_scale=8, train_wall=8, gb_free=21, wall=6560
2022-05-28 13:33:49 | INFO | train_inner | epoch 014:    350 / 6032 loss=4.501, nll_loss=2.751, ppl=6.73, wps=32317.5, ups=12.22, wpb=2645, bsz=128, num_updates=78600, lr=0.000157913, gnorm=1.085, loss_scale=8, train_wall=8, gb_free=20.7, wall=6568
2022-05-28 13:33:57 | INFO | train_inner | epoch 014:    450 / 6032 loss=4.628, nll_loss=2.898, ppl=7.45, wps=36275.7, ups=12.18, wpb=2978.8, bsz=126.8, num_updates=78700, lr=0.000157812, gnorm=1.08, loss_scale=8, train_wall=8, gb_free=18.8, wall=6576
2022-05-28 13:34:05 | INFO | train_inner | epoch 014:    550 / 6032 loss=4.47, nll_loss=2.715, ppl=6.57, wps=34333.1, ups=12.53, wpb=2739.6, bsz=128, num_updates=78800, lr=0.000157712, gnorm=1.063, loss_scale=8, train_wall=8, gb_free=17.9, wall=6584
2022-05-28 13:34:13 | INFO | train_inner | epoch 014:    650 / 6032 loss=4.332, nll_loss=2.559, ppl=5.89, wps=32405.8, ups=12.85, wpb=2521.5, bsz=128, num_updates=78900, lr=0.000157612, gnorm=1.088, loss_scale=8, train_wall=8, gb_free=20, wall=6592
2022-05-28 13:34:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.46 GiB (GPU 0; 23.70 GiB total capacity; 14.85 GiB already allocated; 1.90 GiB free; 20.13 GiB reserved in total by PyTorch)
2022-05-28 13:34:17 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 160          |        cudaMalloc retries: 351       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15209 MB |   15260 MB |  483702 GB |  483687 GB |
|       from large pool |   15169 MB |   15220 MB |  472933 GB |  472918 GB |
|       from small pool |      39 MB |      73 MB |   10769 GB |   10769 GB |
|---------------------------------------------------------------------------|
| Active memory         |   15209 MB |   15260 MB |  483702 GB |  483687 GB |
|       from large pool |   15169 MB |   15220 MB |  472933 GB |  472918 GB |
|       from small pool |      39 MB |      73 MB |   10769 GB |   10769 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20614 MB |   20774 MB |    1688 GB |    1668 GB |
|       from large pool |   20572 MB |   20572 MB |    1649 GB |    1629 GB |
|       from small pool |      42 MB |     202 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    5404 MB |    5404 MB |  667903 GB |  667898 GB |
|       from large pool |    5402 MB |    5402 MB |  655194 GB |  655189 GB |
|       from small pool |       2 MB |      23 MB |   12709 GB |   12709 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  127226 K  |  127225 K  |
|       from large pool |     271    |     275    |   60191 K  |   60191 K  |
|       from small pool |     321    |     466    |   67034 K  |   67034 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  127226 K  |  127225 K  |
|       from large pool |     271    |     275    |   60191 K  |   60191 K  |
|       from small pool |     321    |     466    |   67034 K  |   67034 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |     126    |   20902    |   20856    |
|       from large pool |      25    |      25    |     854    |     829    |
|       from small pool |      21    |     101    |   20048    |   20027    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      56    |   64809 K  |   64809 K  |
|       from large pool |      16    |      16    |   34258 K  |   34258 K  |
|       from small pool |       7    |      46    |   30551 K  |   30551 K  |
|===========================================================================|

2022-05-28 13:34:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:34:21 | INFO | train_inner | epoch 014:    751 / 6032 loss=4.33, nll_loss=2.555, ppl=5.88, wps=32203.2, ups=12.63, wpb=2550.2, bsz=128, num_updates=79000, lr=0.000157512, gnorm=1.078, loss_scale=8, train_wall=8, gb_free=20, wall=6600
2022-05-28 13:34:28 | INFO | train_inner | epoch 014:    851 / 6032 loss=4.332, nll_loss=2.557, ppl=5.89, wps=33677, ups=12.94, wpb=2601.6, bsz=128, num_updates=79100, lr=0.000157413, gnorm=1.072, loss_scale=8, train_wall=8, gb_free=20.1, wall=6607
2022-05-28 13:34:36 | INFO | train_inner | epoch 014:    951 / 6032 loss=4.624, nll_loss=2.892, ppl=7.42, wps=37884.6, ups=12.44, wpb=3046.1, bsz=128, num_updates=79200, lr=0.000157313, gnorm=1.045, loss_scale=8, train_wall=8, gb_free=20.6, wall=6615
2022-05-28 13:34:44 | INFO | train_inner | epoch 014:   1051 / 6032 loss=4.445, nll_loss=2.688, ppl=6.44, wps=33255.5, ups=12.48, wpb=2665.3, bsz=128, num_updates=79300, lr=0.000157214, gnorm=1.082, loss_scale=8, train_wall=8, gb_free=20.2, wall=6624
2022-05-28 13:34:52 | INFO | train_inner | epoch 014:   1151 / 6032 loss=4.375, nll_loss=2.607, ppl=6.09, wps=32490.8, ups=12.77, wpb=2545, bsz=128, num_updates=79400, lr=0.000157115, gnorm=1.087, loss_scale=8, train_wall=8, gb_free=19.1, wall=6631
2022-05-28 13:34:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.89 GiB (GPU 0; 23.70 GiB total capacity; 12.65 GiB already allocated; 1.90 GiB free; 20.13 GiB reserved in total by PyTorch)
2022-05-28 13:34:55 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 161          |        cudaMalloc retries: 352       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   12953 MB |   12997 MB |  486489 GB |  486477 GB |
|       from large pool |   12914 MB |   12957 MB |  475656 GB |  475644 GB |
|       from small pool |      39 MB |      73 MB |   10832 GB |   10832 GB |
|---------------------------------------------------------------------------|
| Active memory         |   12953 MB |   12997 MB |  486489 GB |  486477 GB |
|       from large pool |   12914 MB |   12957 MB |  475656 GB |  475644 GB |
|       from small pool |      39 MB |      73 MB |   10832 GB |   10832 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20612 MB |   20774 MB |    1688 GB |    1668 GB |
|       from large pool |   20572 MB |   20572 MB |    1649 GB |    1629 GB |
|       from small pool |      40 MB |     202 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    7658 MB |    7658 MB |  672809 GB |  672802 GB |
|       from large pool |    7657 MB |    7657 MB |  660025 GB |  660017 GB |
|       from small pool |       0 MB |       4 MB |   12784 GB |   12784 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  127964 K  |  127964 K  |
|       from large pool |     271    |     275    |   60539 K  |   60538 K  |
|       from small pool |     321    |     466    |   67425 K  |   67425 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  127964 K  |  127964 K  |
|       from large pool |     271    |     275    |   60539 K  |   60538 K  |
|       from small pool |     321    |     466    |   67425 K  |   67425 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     126    |   20982    |   20937    |
|       from large pool |      25    |      25    |     854    |     829    |
|       from small pool |      20    |     101    |   20128    |   20108    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      27    |      36    |   65185 K  |   65185 K  |
|       from large pool |      19    |      19    |   34454 K  |   34454 K  |
|       from small pool |       8    |      20    |   30730 K  |   30730 K  |
|===========================================================================|

2022-05-28 13:34:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:34:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.22 GiB (GPU 0; 23.70 GiB total capacity; 13.63 GiB already allocated; 1.90 GiB free; 20.13 GiB reserved in total by PyTorch)
2022-05-28 13:34:56 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 162          |        cudaMalloc retries: 353       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13957 MB |   14005 MB |  486602 GB |  486588 GB |
|       from large pool |   13918 MB |   13966 MB |  475766 GB |  475752 GB |
|       from small pool |      39 MB |      73 MB |   10836 GB |   10836 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13957 MB |   14005 MB |  486602 GB |  486588 GB |
|       from large pool |   13918 MB |   13966 MB |  475766 GB |  475752 GB |
|       from small pool |      39 MB |      73 MB |   10836 GB |   10836 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   20612 MB |   20774 MB |    1689 GB |    1669 GB |
|       from large pool |   20572 MB |   20572 MB |    1649 GB |    1629 GB |
|       from small pool |      40 MB |     202 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6654 MB |    6654 MB |  673013 GB |  673006 GB |
|       from large pool |    6653 MB |    6653 MB |  660224 GB |  660218 GB |
|       from small pool |       0 MB |      10 MB |   12788 GB |   12788 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  127995 K  |  127994 K  |
|       from large pool |     271    |     275    |   60552 K  |   60552 K  |
|       from small pool |     321    |     466    |   67442 K  |   67442 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  127995 K  |  127994 K  |
|       from large pool |     271    |     275    |   60552 K  |   60552 K  |
|       from small pool |     321    |     466    |   67442 K  |   67442 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      45    |     126    |   21063    |   21018    |
|       from large pool |      25    |      25    |     854    |     829    |
|       from small pool |      20    |     101    |   20209    |   20189    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      43    |   65201 K  |   65201 K  |
|       from large pool |      14    |      14    |   34462 K  |   34462 K  |
|       from small pool |       6    |      30    |   30739 K  |   30739 K  |
|===========================================================================|

2022-05-28 13:34:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:35:00 | INFO | train_inner | epoch 014:   1253 / 6032 loss=4.29, nll_loss=2.51, ppl=5.7, wps=30667.6, ups=12.61, wpb=2432.2, bsz=128, num_updates=79500, lr=0.000157016, gnorm=1.096, loss_scale=8, train_wall=7, gb_free=20.7, wall=6639
2022-05-28 13:35:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 343.69 MiB free; 21.70 GiB reserved in total by PyTorch)
2022-05-28 13:35:07 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 163          |        cudaMalloc retries: 355       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   21956 MB |   22256 MB |  487375 GB |  487354 GB |
|       from large pool |   21913 MB |   22213 MB |  476518 GB |  476496 GB |
|       from small pool |      42 MB |      73 MB |   10857 GB |   10857 GB |
|---------------------------------------------------------------------------|
| Active memory         |   21956 MB |   22256 MB |  487375 GB |  487354 GB |
|       from large pool |   21913 MB |   22213 MB |  476518 GB |  476496 GB |
|       from small pool |      42 MB |      73 MB |   10857 GB |   10857 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22216 MB |   22516 MB |    1691 GB |    1669 GB |
|       from large pool |   22172 MB |   22472 MB |    1651 GB |    1629 GB |
|       from small pool |      44 MB |     198 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  265937 KB |    8103 MB |  674381 GB |  674381 GB |
|       from large pool |  264520 KB |    8101 MB |  661568 GB |  661568 GB |
|       from small pool |    1417 KB |       5 MB |   12812 GB |   12812 GB |
|---------------------------------------------------------------------------|
| Allocations           |     516    |     526    |  128209 K  |  128209 K  |
|       from large pool |     214    |     215    |   60649 K  |   60649 K  |
|       from small pool |     302    |     466    |   67560 K  |   67559 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     516    |     526    |  128209 K  |  128209 K  |
|       from large pool |     214    |     215    |   60649 K  |   60649 K  |
|       from small pool |     302    |     466    |   67560 K  |   67559 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      50    |     127    |   21146    |   21096    |
|       from large pool |      28    |      29    |     858    |     830    |
|       from small pool |      22    |      99    |   20288    |   20266    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      35    |   65311 K  |   65311 K  |
|       from large pool |      19    |      20    |   34517 K  |   34517 K  |
|       from small pool |      11    |      20    |   30793 K  |   30793 K  |
|===========================================================================|

2022-05-28 13:35:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:35:08 | INFO | train_inner | epoch 014:   1354 / 6032 loss=4.358, nll_loss=2.588, ppl=6.01, wps=31283.1, ups=12.67, wpb=2469.4, bsz=128, num_updates=79600, lr=0.000156918, gnorm=1.118, loss_scale=8, train_wall=7, gb_free=20.2, wall=6647
2022-05-28 13:35:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 23.70 GiB total capacity; 16.82 GiB already allocated; 345.69 MiB free; 21.69 GiB reserved in total by PyTorch)
2022-05-28 13:35:09 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 164          |        cudaMalloc retries: 356       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13288 MB |   17218 MB |  487472 GB |  487459 GB |
|       from large pool |   13248 MB |   17179 MB |  476613 GB |  476600 GB |
|       from small pool |      39 MB |      73 MB |   10859 GB |   10859 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13288 MB |   17218 MB |  487472 GB |  487459 GB |
|       from large pool |   13248 MB |   17179 MB |  476613 GB |  476600 GB |
|       from small pool |      39 MB |      73 MB |   10859 GB |   10859 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   22214 MB |   22370 MB |    1691 GB |    1669 GB |
|       from large pool |   22172 MB |   22172 MB |    1651 GB |    1629 GB |
|       from small pool |      42 MB |     198 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1469 MB |    4995 MB |  674511 GB |  674510 GB |
|       from large pool |    1467 MB |    4992 MB |  661696 GB |  661695 GB |
|       from small pool |       2 MB |      25 MB |   12814 GB |   12814 GB |
|---------------------------------------------------------------------------|
| Allocations           |     592    |     597    |  128232 K  |  128231 K  |
|       from large pool |     271    |     275    |   60660 K  |   60660 K  |
|       from small pool |     321    |     466    |   67571 K  |   67571 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     592    |     597    |  128232 K  |  128231 K  |
|       from large pool |     271    |     275    |   60660 K  |   60660 K  |
|       from small pool |     321    |     466    |   67571 K  |   67571 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      49    |     127    |   21223    |   21174    |
|       from large pool |      28    |      28    |     858    |     830    |
|       from small pool |      21    |      99    |   20365    |   20344    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      29    |      57    |   65322 K  |   65322 K  |
|       from large pool |      21    |      22    |   34524 K  |   34524 K  |
|       from small pool |       8    |      48    |   30798 K  |   30798 K  |
|===========================================================================|

2022-05-28 13:35:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
2022-05-28 13:35:16 | INFO | train_inner | epoch 014:   1455 / 6032 loss=4.357, nll_loss=2.589, ppl=6.01, wps=32968.5, ups=12.62, wpb=2613, bsz=128, num_updates=79700, lr=0.000156819, gnorm=1.12, loss_scale=8, train_wall=8, gb_free=20.5, wall=6655
2022-05-28 13:35:24 | INFO | train_inner | epoch 014:   1555 / 6032 loss=4.505, nll_loss=2.757, ppl=6.76, wps=35407.6, ups=12.73, wpb=2780.4, bsz=128, num_updates=79800, lr=0.000156721, gnorm=1.093, loss_scale=8, train_wall=8, gb_free=18.9, wall=6663
2022-05-28 13:35:32 | INFO | train_inner | epoch 014:   1655 / 6032 loss=4.614, nll_loss=2.881, ppl=7.37, wps=35644.9, ups=12.51, wpb=2849.7, bsz=128, num_updates=79900, lr=0.000156623, gnorm=1.077, loss_scale=8, train_wall=8, gb_free=19.5, wall=6671
2022-05-28 13:35:40 | INFO | train_inner | epoch 014:   1755 / 6032 loss=4.499, nll_loss=2.75, ppl=6.72, wps=34207.4, ups=12.51, wpb=2733.9, bsz=128, num_updates=80000, lr=0.000156525, gnorm=1.104, loss_scale=8, train_wall=8, gb_free=21, wall=6679
2022-05-28 13:35:48 | INFO | train_inner | epoch 014:   1855 / 6032 loss=4.557, nll_loss=2.818, ppl=7.05, wps=31804.5, ups=12.38, wpb=2568.9, bsz=128, num_updates=80100, lr=0.000156427, gnorm=1.13, loss_scale=8, train_wall=8, gb_free=19.4, wall=6687
2022-05-28 13:35:56 | INFO | train_inner | epoch 014:   1955 / 6032 loss=4.48, nll_loss=2.729, ppl=6.63, wps=32703.1, ups=12.35, wpb=2648.5, bsz=128, num_updates=80200, lr=0.000156329, gnorm=1.124, loss_scale=8, train_wall=8, gb_free=19.6, wall=6695
2022-05-28 13:36:04 | INFO | train_inner | epoch 014:   2055 / 6032 loss=4.378, nll_loss=2.612, ppl=6.11, wps=32415.6, ups=13.22, wpb=2452.9, bsz=128, num_updates=80300, lr=0.000156232, gnorm=1.109, loss_scale=8, train_wall=7, gb_free=19.2, wall=6703
2022-05-28 13:36:12 | INFO | train_inner | epoch 014:   2155 / 6032 loss=4.513, nll_loss=2.766, ppl=6.8, wps=33578.4, ups=12.3, wpb=2730.9, bsz=128, num_updates=80400, lr=0.000156135, gnorm=1.108, loss_scale=8, train_wall=8, gb_free=18.5, wall=6711
2022-05-28 13:36:20 | INFO | train_inner | epoch 014:   2255 / 6032 loss=4.799, nll_loss=3.096, ppl=8.55, wps=37436.6, ups=12.12, wpb=3089.4, bsz=128, num_updates=80500, lr=0.000156038, gnorm=1.077, loss_scale=8, train_wall=8, gb_free=20.4, wall=6719
2022-05-28 13:36:28 | INFO | train_inner | epoch 014:   2355 / 6032 loss=4.44, nll_loss=2.685, ppl=6.43, wps=32968.5, ups=12.94, wpb=2548, bsz=128, num_updates=80600, lr=0.000155941, gnorm=1.118, loss_scale=8, train_wall=8, gb_free=20.1, wall=6727
2022-05-28 13:36:36 | INFO | train_inner | epoch 014:   2455 / 6032 loss=4.542, nll_loss=2.8, ppl=6.96, wps=34153.3, ups=12.31, wpb=2774, bsz=128, num_updates=80700, lr=0.000155844, gnorm=1.091, loss_scale=8, train_wall=8, gb_free=18.7, wall=6735
2022-05-28 13:36:44 | INFO | train_inner | epoch 014:   2555 / 6032 loss=4.411, nll_loss=2.65, ppl=6.28, wps=33431.5, ups=12.89, wpb=2592.8, bsz=128, num_updates=80800, lr=0.000155748, gnorm=1.094, loss_scale=8, train_wall=8, gb_free=20.7, wall=6743
2022-05-28 13:36:52 | INFO | train_inner | epoch 014:   2655 / 6032 loss=4.58, nll_loss=2.844, ppl=7.18, wps=34537.2, ups=12.33, wpb=2801.1, bsz=128, num_updates=80900, lr=0.000155652, gnorm=1.118, loss_scale=8, train_wall=8, gb_free=18.9, wall=6751
2022-05-28 13:36:59 | INFO | train_inner | epoch 014:   2755 / 6032 loss=4.43, nll_loss=2.674, ppl=6.38, wps=33259, ups=12.93, wpb=2572, bsz=128, num_updates=81000, lr=0.000155556, gnorm=1.127, loss_scale=8, train_wall=8, gb_free=19.6, wall=6759
2022-05-28 13:37:07 | INFO | train_inner | epoch 014:   2855 / 6032 loss=4.532, nll_loss=2.79, ppl=6.92, wps=35118.7, ups=12.69, wpb=2767.6, bsz=128, num_updates=81100, lr=0.00015546, gnorm=1.099, loss_scale=8, train_wall=8, gb_free=19.6, wall=6766
2022-05-28 13:37:15 | INFO | train_inner | epoch 014:   2955 / 6032 loss=4.497, nll_loss=2.749, ppl=6.72, wps=33602.5, ups=12.26, wpb=2741, bsz=128, num_updates=81200, lr=0.000155364, gnorm=1.089, loss_scale=8, train_wall=8, gb_free=19.9, wall=6775
2022-05-28 13:37:23 | INFO | train_inner | epoch 014:   3055 / 6032 loss=4.482, nll_loss=2.733, ppl=6.65, wps=33272.7, ups=12.95, wpb=2568.4, bsz=128, num_updates=81300, lr=0.000155268, gnorm=1.099, loss_scale=8, train_wall=8, gb_free=19.3, wall=6782
2022-05-28 13:37:31 | INFO | train_inner | epoch 014:   3155 / 6032 loss=4.668, nll_loss=2.945, ppl=7.7, wps=33337.2, ups=12.18, wpb=2736.2, bsz=128, num_updates=81400, lr=0.000155173, gnorm=1.103, loss_scale=8, train_wall=8, gb_free=19, wall=6790
2022-05-28 13:37:39 | INFO | train_inner | epoch 014:   3255 / 6032 loss=4.476, nll_loss=2.725, ppl=6.61, wps=32828.5, ups=12.71, wpb=2582.7, bsz=128, num_updates=81500, lr=0.000155078, gnorm=1.13, loss_scale=8, train_wall=8, gb_free=19.5, wall=6798
